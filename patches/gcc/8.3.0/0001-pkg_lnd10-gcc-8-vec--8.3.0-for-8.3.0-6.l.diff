diff --git a/src/gcc/common/config/loongarch/loongarch-common.c b/src/gcc/common/config/loongarch/loongarch-common.c
index ce58d622a..bdc5b939c 100644
--- a/src/gcc/common/config/loongarch/loongarch-common.c
+++ b/src/gcc/common/config/loongarch/loongarch-common.c
@@ -1,5 +1,5 @@
 /* Common hooks for LoongArch.
-   Copyright (C) 1989-2018 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
 
 This file is part of GCC.
 
diff --git a/src/gcc/config/loongarch/constraints.md b/src/gcc/config/loongarch/constraints.md
index 55fff4a5d..82c0ccf37 100644
--- a/src/gcc/config/loongarch/constraints.md
+++ b/src/gcc/config/loongarch/constraints.md
@@ -1,5 +1,5 @@
 ;; Constraint definitions for LoongArch.
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Co. Ltd.
 ;;
 ;; This file is part of GCC.
@@ -74,17 +74,11 @@
 ;; "W" <-----unused
 ;; "X" Matches anything. (Global non-architectural)
 ;; "Y" -
-;;    "Yb"
-;;	 Bitmask for QImode: 0xff.
 ;;    "Yd"
 ;;       A constant @code{move_operand} that can be safely loaded using
 ;;	  @code{la}.
 ;;    "YG"
 ;;	 A vector zero.
-;;    "Yh"
-;;       Bitmask for HImode: 0xffff.
-;;    "Yw"
-;;       Bitmask for SImode: 0xffffffff.
 ;;    "Yx"
 ;;    "YC"
 ;;       A replicated vector const in which the replicated value has a single
@@ -131,8 +125,7 @@
   "A memory operand whose address is formed by a base register and (optionally scaled)
    index register."
   (and (match_code "mem")
-       (not (match_test "loongarch_14bit_shifted_offset_address_p (XEXP (op, 0), mode)"))
-       (not (match_test "loongarch_12bit_offset_address_p (XEXP (op, 0), mode)"))))
+       (match_test "loongarch_base_index_address_p (XEXP (op, 0), mode)")))
 
 (define_constraint "l"
 "A signed 16-bit constant."
@@ -160,7 +153,7 @@
        (match_test "LU32I_OPERAND (ival)")))
 
 (define_constraint "v"
-  "A unsigned 64-bit constant and low 52-bit is zero (for logic instructions)."
+  "A signed 64-bit constant and low 52-bit is zero (for logic instructions)."
   (and (match_code "const_int")
        (match_test "LU52I_OPERAND (ival)")))
 
@@ -197,15 +190,6 @@
   "@internal"
   (match_operand 0 "const_arith_operand"))
 
-(define_memory_constraint "W"
-  "@internal
-   A memory address based on a member of @code{BASE_REG_CLASS}.  This is
-   true for allreferences."
-  (and (match_code "mem")
-       (match_operand 0 "memory_operand")
-	    (and (not (match_operand 0 "stack_operand"))
-		 (not (match_test "CONSTANT_P (XEXP (op, 0))")))))
-
 (define_constraint "Yd"
   "@internal
    A constant @code{move_operand} that can be safely loaded using
@@ -229,27 +213,19 @@
   An address that is held in a general-purpose register.
   The offset is zero"
   (and (match_code "mem")
-       (match_test "GET_CODE (XEXP (op,0)) == REG")))
+       (match_test "REG_P (XEXP (op, 0))")))
 
 (define_memory_constraint "R"
   "An address that can be used in a non-macro load or store."
   (and (match_code "mem")
        (match_test "loongarch_address_insns (XEXP (op, 0), mode, false) == 1")))
 
-(define_constraint "Yb"
-   "@internal"
-   (match_operand 0 "qi_mask_operand"))
-
 (define_constraint "YG"
   "@internal
    A vector zero."
   (and (match_code "const_vector")
        (match_test "op == CONST0_RTX (mode)")))
 
-(define_constraint "Yh"
-   "@internal"
-    (match_operand 0 "hi_mask_operand"))
-
 (define_constraint "YI"
   "@internal
    A replicated vector const in which the replicated value is in the range
@@ -257,10 +233,6 @@
   (and (match_code "const_vector")
        (match_test "loongarch_const_vector_same_int_p (op, mode, -512, 511)")))
 
-(define_constraint "Yw"
-   "@internal"
-    (match_operand 0 "si_mask_operand"))
-
 (define_constraint "YC"
   "@internal
    A replicated vector const in which the replicated value has a single
diff --git a/src/gcc/config/loongarch/generic.md b/src/gcc/config/loongarch/generic.md
index 1b3942ad3..0f6eb3f42 100644
--- a/src/gcc/config/loongarch/generic.md
+++ b/src/gcc/config/loongarch/generic.md
@@ -1,5 +1,5 @@
 ;; Generic DFA-based pipeline description for LoongArch targets
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Co. Ltd.
 ;; Based on MIPS target for GNU compiler.
 
@@ -19,17 +19,6 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
-
-;; Pipeline descriptions.
-;;
-;; generic.md provides a fallback for processors without a specific
-;; pipeline description.  It is derived from the old define_function_unit
-;; version and uses the "alu" and "imuldiv" units declared below.
-;;
-;; Some of the processor-specific files are also derived from old
-;; define_function_unit descriptions and simply override the parts of
-;; generic.md that don't apply.  The other processor-specific files
-;; are self-contained.
 (define_automaton "alu,imuldiv")
 
 (define_cpu_unit "alu" "alu")
@@ -41,9 +30,6 @@
   (eq_attr "type" "ghost")
   "nothing")
 
-;; This file is derived from the old define_function_unit description.
-;; Each reservation can be overridden on a processor-by-processor basis.
-
 (define_insn_reservation "generic_alu" 1
   (eq_attr "type" "unknown,prefetch,prefetchx,condmove,const,arith,
 		   shift,slt,clz,trap,multi,nop,logical,signext,move")
diff --git a/src/gcc/config/loongarch/genopts/genstr.sh b/src/gcc/config/loongarch/genopts/genstr.sh
index 972ef125f..e895f7ec8 100755
--- a/src/gcc/config/loongarch/genopts/genstr.sh
+++ b/src/gcc/config/loongarch/genopts/genstr.sh
@@ -2,7 +2,7 @@
 # A simple script that generates loongarch-str.h and loongarch.opt
 # from genopt/loongarch-optstr.
 #
-# Copyright (C) 2021-2022 Free Software Foundation, Inc.
+# Copyright (C) 2020-2022 Free Software Foundation, Inc.
 #
 # This file is part of GCC.
 #
@@ -28,7 +28,7 @@ gen_defines() {
 /* Generated automatically by "genstr" from "loongarch-strings".
    Please do not edit this file directly.
 
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/genopts/loongarch-strings b/src/gcc/config/loongarch/genopts/loongarch-strings
index 1364da8d6..22aa6e7cb 100644
--- a/src/gcc/config/loongarch/genopts/loongarch-strings
+++ b/src/gcc/config/loongarch/genopts/loongarch-strings
@@ -1,6 +1,6 @@
 # Defines the key strings for LoongArch compiler options.
 #
-# Copyright (C) 2021-2022 Free Software Foundation, Inc.
+# Copyright (C) 2020-2022 Free Software Foundation, Inc.
 #
 # This file is part of GCC.
 #
diff --git a/src/gcc/config/loongarch/genopts/loongarch.opt.in b/src/gcc/config/loongarch/genopts/loongarch.opt.in
index b61eadc13..c3538eb27 100644
--- a/src/gcc/config/loongarch/genopts/loongarch.opt.in
+++ b/src/gcc/config/loongarch/genopts/loongarch.opt.in
@@ -1,7 +1,7 @@
 ; Generated by "genstr" from the template "loongarch.opt.in"
 ; and definitions from "loongarch-strings".
 ;
-; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;
 ; This file is part of GCC.
 ;
@@ -147,6 +147,10 @@ mvecarg
 Target Report Var(TARGET_VECARG) Init(1)
 Target pass vect arg uses vector register.
 
+mmemvec-cost=
+Target RejectNegative Joined UInteger Var(loongarch_vector_access_cost) IntegerRange(1, 5)
+mmemvec-cost=COST      Set the cost of vector memory access instructions.
+
 mcheck-zero-division
 Target Mask(CHECK_ZERO_DIV)
 Trap on integer divide by zero.
diff --git a/src/gcc/config/loongarch/gnu-user.h b/src/gcc/config/loongarch/gnu-user.h
index 7cfa90136..357027776 100644
--- a/src/gcc/config/loongarch/gnu-user.h
+++ b/src/gcc/config/loongarch/gnu-user.h
@@ -78,7 +78,3 @@ along with GCC; see the file COPYING3.  If not see
        builtin_define ("_GNU_SOURCE"); \
     } \
   while (0)
-
-
-#undef ASM_DECLARE_OBJECT_NAME
-#define ASM_DECLARE_OBJECT_NAME loongarch_declare_object_name
diff --git a/src/gcc/config/loongarch/la464.md b/src/gcc/config/loongarch/la464.md
index aab0f6ebe..7ac51f190 100644
--- a/src/gcc/config/loongarch/la464.md
+++ b/src/gcc/config/loongarch/la464.md
@@ -1,6 +1,6 @@
 ;; Pipeline model for LoongArch LA464 cores.
 
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Co. Ltd.
 
 ;; This file is part of GCC.
@@ -43,88 +43,88 @@
 ;; Describe instruction reservations.
 
 (define_insn_reservation "la464_arith" 1
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "arith,clz,const,logical,
 			move,nop,shift,signext,slt"))
   "la464_alu1 | la464_alu2")
 
 (define_insn_reservation "la464_branch" 1
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "branch,jump,call,condmove,trap"))
   "la464_alu1 | la464_alu2")
 
 (define_insn_reservation "la464_imul" 7
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "imul"))
   "la464_alu1 | la464_alu2")
 
 (define_insn_reservation "la464_idiv_si" 12
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (and (eq_attr "type" "idiv")
 	    (eq_attr "mode" "SI")))
   "la464_alu1 | la464_alu2")
 
 (define_insn_reservation "la464_idiv_di" 25
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (and (eq_attr "type" "idiv")
 	    (eq_attr "mode" "DI")))
   "la464_alu1 | la464_alu2")
 
 (define_insn_reservation "la464_load" 4
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "load"))
   "la464_mem1 | la464_mem2")
 
 (define_insn_reservation "la464_gpr_fp" 16
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "mftg,mgtf"))
   "la464_mem1")
 
 (define_insn_reservation "la464_fpload" 4
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "fpload"))
   "la464_mem1 | la464_mem2")
 
 (define_insn_reservation "la464_prefetch" 0
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "prefetch,prefetchx"))
   "la464_mem1 | la464_mem2")
 
 (define_insn_reservation "la464_store" 0
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "store,fpstore,fpidxstore"))
   "la464_mem1 | la464_mem2")
 
 (define_insn_reservation "la464_fadd" 4
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "fadd,fmul,fmadd"))
   "la464_falu1 | la464_falu2")
 
 (define_insn_reservation "la464_fcmp" 2
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "fabs,fcmp,fmove,fneg"))
   "la464_falu1 | la464_falu2")
 
 (define_insn_reservation "la464_fcvt" 4
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "fcvt"))
   "la464_falu1 | la464_falu2")
 
 (define_insn_reservation "la464_fdiv_sf" 12
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (and (eq_attr "type" "fdiv,frdiv,fsqrt,frsqrt")
 	    (eq_attr "mode" "SF")))
   "la464_falu1 | la464_falu2")
 
 (define_insn_reservation "la464_fdiv_df" 19
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (and (eq_attr "type" "fdiv,frdiv,fsqrt,frsqrt")
 	    (eq_attr "mode" "DF")))
   "la464_falu1 | la464_falu2")
 
 ;; Force single-dispatch for unknown or multi.
 (define_insn_reservation "la464_unknown" 1
-  (and (match_test "TARGET_ARCH_LA464")
+  (and (match_test "TARGET_TUNE_LA464")
        (eq_attr "type" "unknown,multi,atomic,syncloop"))
   "la464_alu1 + la464_alu2 + la464_falu1
    + la464_falu2 + la464_mem1 + la464_mem2")
diff --git a/src/gcc/config/loongarch/larchintrin.h b/src/gcc/config/loongarch/larchintrin.h
index d8d6ccb1b..8e26ed6f0 100644
--- a/src/gcc/config/loongarch/larchintrin.h
+++ b/src/gcc/config/loongarch/larchintrin.h
@@ -1,5 +1,5 @@
 /* Intrinsics for LoongArch BASE operations.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
@@ -45,64 +45,61 @@ typedef struct rdtime
 #ifdef __loongarch64
 extern __inline __drdtime_t
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_rdtime_d (void)
+__rdtime_d (void)
 {
-  __drdtime_t drdtime;
+  __drdtime_t __drdtime;
   __asm__ volatile (
     "rdtime.d\t%[val],%[tid]\n\t"
-    : [val]"=&r"(drdtime.dvalue),[tid]"=&r"(drdtime.dtimeid)
+    : [val]"=&r"(__drdtime.dvalue),[tid]"=&r"(__drdtime.dtimeid)
     :);
-  return drdtime;
+  return __drdtime;
 }
-#define __rdtime_d __builtin_loongarch_rdtime_d
 #endif
 
 extern __inline __rdtime_t
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_rdtimeh_w (void)
+__rdtimeh_w (void)
 {
-  __rdtime_t rdtime;
+  __rdtime_t __rdtime;
   __asm__ volatile (
     "rdtimeh.w\t%[val],%[tid]\n\t"
-    : [val]"=&r"(rdtime.value),[tid]"=&r"(rdtime.timeid)
+    : [val]"=&r"(__rdtime.value),[tid]"=&r"(__rdtime.timeid)
     :);
-  return rdtime;
+  return __rdtime;
 }
-#define __rdtimel_w __builtin_loongarch_rdtimel_w
 
 extern __inline __rdtime_t
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_rdtimel_w (void)
+__rdtimel_w (void)
 {
-  __rdtime_t rdtime;
+  __rdtime_t __rdtime;
   __asm__ volatile (
     "rdtimel.w\t%[val],%[tid]\n\t"
-    : [val]"=&r"(rdtime.value),[tid]"=&r"(rdtime.timeid)
+    : [val]"=&r"(__rdtime.value),[tid]"=&r"(__rdtime.timeid)
     :);
-  return rdtime;
+  return __rdtime;
 }
-#define __rdtimeh_w __builtin_loongarch_rdtimeh_w
 
 /* Assembly instruction format:	rj, fcsr.  */
 /* Data types in instruction templates:  USI, UQI.  */
 #define __movfcsr2gr(/*ui5*/ _1) __builtin_loongarch_movfcsr2gr ((_1));
 
-/* Assembly instruction format:	0, fcsr, rj.  */
+/* Assembly instruction format:	fcsr, rj.  */
 /* Data types in instruction templates:  VOID, UQI, USI.  */
 #define __movgr2fcsr(/*ui5*/ _1, _2) \
-  __builtin_loongarch_movgr2fcsr ((unsigned short) _1, (unsigned int) _2);
+  __builtin_loongarch_movgr2fcsr ((_1), (unsigned int) _2);
 
 #if defined __loongarch64
 /* Assembly instruction format:	ui5, rj, si12.  */
 /* Data types in instruction templates:  VOID, USI, UDI, SI.  */
-#define __dcacop(/*ui5*/ _1, /*unsigned long int*/ _2, /*si12*/ _3) \
-  ((void) __builtin_loongarch_dcacop ((_1), (unsigned long int) (_2), (_3)))
+#define __cacop_d(/*ui5*/ _1, /*unsigned long int*/ _2, /*si12*/ _3) \
+  ((void) __builtin_loongarch_cacop_d ((_1), (unsigned long int) (_2), (_3)))
 #else
-#error "Don't support this ABI."
+#error "Unsupported ABI."
 #endif
 
-/* Assembly instruction format:	rd, rj */
-/* Data types in instruction templates:  USI, USI */
+/* Assembly instruction format:	rd, rj.  */
+/* Data types in instruction templates:  USI, USI.  */
 extern __inline unsigned int
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
 __cpucfg (unsigned int _1)
@@ -111,7 +108,7 @@ __cpucfg (unsigned int _1)
 }
 
 #ifdef __loongarch64
-/* Assembly instruction format:	rd, rj.  */
+/* Assembly instruction format:	rj, rk.  */
 /* Data types in instruction templates:  DI, DI.  */
 extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
@@ -120,7 +117,7 @@ __asrtle_d (long int _1, long int _2)
   __builtin_loongarch_asrtle_d ((long int) _1, (long int) _2);
 }
 
-/* Assembly instruction format:	rd, rj.  */
+/* Assembly instruction format:	rj, rk.  */
 /* Data types in instruction templates:  DI, DI.  */
 extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
@@ -133,19 +130,19 @@ __asrtgt_d (long int _1, long int _2)
 #if defined __loongarch64
 /* Assembly instruction format:	rd, rj, ui5.  */
 /* Data types in instruction templates:  DI, DI, UQI.  */
-#define __dlddir(/*long int*/ _1, /*ui5*/ _2) \
-  ((long int) __builtin_loongarch_dlddir ((long int) (_1), (_2)))
+#define __lddir_d(/*long int*/ _1, /*ui5*/ _2) \
+  ((long int) __builtin_loongarch_lddir_d ((long int) (_1), (_2)))
 #else
-#error "Don't support this ABI."
+#error "Unsupported ABI."
 #endif
 
 #if defined __loongarch64
 /* Assembly instruction format:	rj, ui5.  */
 /* Data types in instruction templates:  VOID, DI, UQI.  */
-#define __dldpte(/*long int*/ _1, /*ui5*/ _2) \
-  ((void) __builtin_loongarch_dldpte ((long int) (_1), (_2)))
+#define __ldpte_d(/*long int*/ _1, /*ui5*/ _2) \
+  ((void) __builtin_loongarch_ldpte_d ((long int) (_1), (_2)))
 #else
-#error "Don't support this ABI."
+#error "Unsupported ABI."
 #endif
 
 /* Assembly instruction format:	rd, rj, rk.  */
@@ -226,36 +223,37 @@ __crcc_w_d_w (long int _1, int _2)
 
 /* Assembly instruction format:	rd, ui14.  */
 /* Data types in instruction templates:  USI, USI.  */
-#define __csrrd(/*ui14*/ _1) ((unsigned int) __builtin_loongarch_csrrd ((_1)))
+#define __csrrd_w(/*ui14*/ _1) \
+  ((unsigned int) __builtin_loongarch_csrrd_w ((_1)))
 
 /* Assembly instruction format:	rd, ui14.  */
 /* Data types in instruction templates:  USI, USI, USI.  */
-#define __csrwr(/*unsigned int*/ _1, /*ui14*/ _2) \
-  ((unsigned int) __builtin_loongarch_csrwr ((unsigned int) (_1), (_2)))
+#define __csrwr_w(/*unsigned int*/ _1, /*ui14*/ _2) \
+  ((unsigned int) __builtin_loongarch_csrwr_w ((unsigned int) (_1), (_2)))
 
 /* Assembly instruction format:	rd, rj, ui14.  */
 /* Data types in instruction templates:  USI, USI, USI, USI.  */
-#define __csrxchg(/*unsigned int*/ _1, /*unsigned int*/ _2, /*ui14*/ _3) \
-  ((unsigned int) __builtin_loongarch_csrxchg ((unsigned int) (_1), \
+#define __csrxchg_w(/*unsigned int*/ _1, /*unsigned int*/ _2, /*ui14*/ _3) \
+  ((unsigned int) __builtin_loongarch_csrxchg_w ((unsigned int) (_1), \
 					       (unsigned int) (_2), (_3)))
 
 #ifdef __loongarch64
 /* Assembly instruction format:	rd, ui14.  */
 /* Data types in instruction templates:  UDI, USI.  */
-#define __dcsrrd(/*ui14*/ _1) \
-  ((unsigned long int) __builtin_loongarch_dcsrrd ((_1)))
+#define __csrrd_d(/*ui14*/ _1) \
+  ((unsigned long int) __builtin_loongarch_csrrd_d ((_1)))
 
 /* Assembly instruction format:	rd, ui14.  */
 /* Data types in instruction templates:  UDI, UDI, USI.  */
-#define __dcsrwr(/*unsigned long int*/ _1, /*ui14*/ _2) \
-  ((unsigned long int) __builtin_loongarch_dcsrwr ((unsigned long int) (_1), \
+#define __csrwr_d(/*unsigned long int*/ _1, /*ui14*/ _2) \
+  ((unsigned long int) __builtin_loongarch_csrwr_d ((unsigned long int) (_1), \
 						   (_2)))
 
 /* Assembly instruction format:	rd, rj, ui14.  */
 /* Data types in instruction templates:  UDI, UDI, UDI, USI.  */
-#define __dcsrxchg(/*unsigned long int*/ _1, /*unsigned long int*/ _2, \
+#define __csrxchg_d(/*unsigned long int*/ _1, /*unsigned long int*/ _2, \
 		   /*ui14*/ _3) \
-  ((unsigned long int) __builtin_loongarch_dcsrxchg ( \
+  ((unsigned long int) __builtin_loongarch_csrxchg_d ( \
     (unsigned long int) (_1), (unsigned long int) (_2), (_3)))
 #endif
 
@@ -303,8 +301,7 @@ extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
 __iocsrwr_b (unsigned char _1, unsigned int _2)
 {
-  return (void) __builtin_loongarch_iocsrwr_b ((unsigned char) _1,
-					       (unsigned int) _2);
+  __builtin_loongarch_iocsrwr_b ((unsigned char) _1, (unsigned int) _2);
 }
 
 /* Assembly instruction format:	rd, rj.  */
@@ -313,8 +310,7 @@ extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
 __iocsrwr_h (unsigned short _1, unsigned int _2)
 {
-  return (void) __builtin_loongarch_iocsrwr_h ((unsigned short) _1,
-					       (unsigned int) _2);
+  __builtin_loongarch_iocsrwr_h ((unsigned short) _1, (unsigned int) _2);
 }
 
 /* Assembly instruction format:	rd, rj.  */
@@ -323,8 +319,7 @@ extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
 __iocsrwr_w (unsigned int _1, unsigned int _2)
 {
-  return (void) __builtin_loongarch_iocsrwr_w ((unsigned int) _1,
-					       (unsigned int) _2);
+  __builtin_loongarch_iocsrwr_w ((unsigned int) _1, (unsigned int) _2);
 }
 
 #ifdef __loongarch64
@@ -334,78 +329,25 @@ extern __inline void
 __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
 __iocsrwr_d (unsigned long int _1, unsigned int _2)
 {
-  return (void) __builtin_loongarch_iocsrwr_d ((unsigned long int) _1,
-					       (unsigned int) _2);
+  __builtin_loongarch_iocsrwr_d ((unsigned long int) _1, (unsigned int) _2);
 }
 #endif
 
 /* Assembly instruction format:	ui15.  */
-/* Data types in instruction templates:  UQI.  */
+/* Data types in instruction templates:  USI.  */
 #define __dbar(/*ui15*/ _1) __builtin_loongarch_dbar ((_1))
 
 /* Assembly instruction format:	ui15.  */
-/* Data types in instruction templates:  UQI.  */
+/* Data types in instruction templates:  USI.  */
 #define __ibar(/*ui15*/ _1) __builtin_loongarch_ibar ((_1))
 
-#define __builtin_loongarch_syscall(a) \
-  { \
-    __asm__ volatile ("syscall %0\n\t" ::"I"(a)); \
-  }
-#define __syscall __builtin_loongarch_syscall
-
-#define __builtin_loongarch_break(a) \
-  { \
-    __asm__ volatile ("break %0\n\t" ::"I"(a)); \
-  }
-#define __break __builtin_loongarch_break
-
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbsrch (void)
-{
-  __asm__ volatile ("tlbsrch\n\t");
-}
-#define __tlbsrch __builtin_loongarch_tlbsrch
-
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbrd (void)
-{
-  __asm__ volatile ("tlbrd\n\t");
-}
-#define __tlbrd __builtin_loongarch_tlbrd
-
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbwr (void)
-{
-  __asm__ volatile ("tlbwr\n\t");
-}
-#define __tlbwr __builtin_loongarch_tlbwr
-
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbfill (void)
-{
-  __asm__ volatile ("tlbfill\n\t");
-}
-#define __tlbfill __builtin_loongarch_tlbfill
-
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbclr (void)
-{
-  __asm__ volatile ("tlbclr\n\t");
-}
-#define __tlbclr __builtin_loongarch_tlbclr
+/* Assembly instruction format:	ui15.  */
+/* Data types in instruction templates:  USI.  */
+#define __syscall(/*ui15*/ _1) __builtin_loongarch_syscall ((_1))
 
-extern __inline void
-__attribute__ ((__gnu_inline__, __always_inline__, __artificial__))
-__builtin_loongarch_tlbflush (void)
-{
-  __asm__ volatile ("tlbflush\n\t");
-}
-#define __tlbflush __builtin_loongarch_tlbflush
+/* Assembly instruction format:	ui15.  */
+/* Data types in instruction templates:  USI.  */
+#define __break(/*ui15*/ _1) __builtin_loongarch_break ((_1))
 
 #ifdef __cplusplus
 }
diff --git a/src/gcc/config/loongarch/lasx.md b/src/gcc/config/loongarch/lasx.md
index 649dc0b59..06dace26f 100644
--- a/src/gcc/config/loongarch/lasx.md
+++ b/src/gcc/config/loongarch/lasx.md
@@ -230,7 +230,7 @@
 ;; Only used for immediate set shuffle elements instruction.
 (define_mode_iterator LASX_WHB_W [V8SI V16HI V32QI V8SF])
 
-;; The atribute gives the integer vector mode with same size in Loongson ASX.
+;; The attribute gives the integer vector mode with same size in Loongson ASX.
  (define_mode_attr VIMODE256
    [(V4DF "V4DI")
     (V8SF "V8SI")
@@ -524,7 +524,6 @@
   operands[2] = loongarch_lsx_vec_parallel_const_half (V8SFmode, false/*high_p*/);
 })
 
-
 (define_expand "vec_unpacks_hi_<mode>"
   [(match_operand:<VDMODE256> 0 "register_operand")
    (match_operand:ILASX_WHB 1 "register_operand")]
@@ -653,28 +652,49 @@
    (set_attr "mode" "V4DI")])
 
 ;; xshuf.w
-(define_insn "lasx_xvperm_w"
-  [(set (match_operand:V8SI 0 "register_operand" "=f")
-         (unspec:V8SI
-	   [(match_operand:V8SI 1 "register_operand" "f")
-	    (match_operand:V8SI 2 "register_operand" "f")]
-	   UNSPEC_LASX_XVPERM_W))]
+(define_insn "lasx_xvperm_<lasxfmt_f_wd>"
+  [(set (match_operand:LASX_W 0 "register_operand" "=f")
+         (unspec:LASX_W
+           [(match_operand:LASX_W 1 "nonimmediate_operand" "f")
+            (match_operand:V8SI 2 "register_operand" "f")]
+           UNSPEC_LASX_XVPERM_W))]
   "ISA_HAS_LASX"
   "xvperm.w\t%u0,%u1,%u2"
   [(set_attr "type" "simd_splat")
-   (set_attr "mode" "V8SI")])
+   (set_attr "mode" "<MODE>")])
 
 ;; xvpermi.d
-(define_insn "lasx_xvpermi_d"
-  [(set (match_operand:V4DI 0 "register_operand" "=f")
-         (unspec:V4DI
-	   [(match_operand:V4DI 1 "register_operand" "f")
-	    (match_operand     2 "const_uimm8_operand")]
-	   UNSPEC_LASX_XVPERMI_D))]
+(define_insn "lasx_xvpermi_d_<LASX:mode>"
+  [(set (match_operand:LASX 0 "register_operand" "=f")
+         (unspec:LASX
+           [(match_operand:LASX 1 "register_operand" "f")
+            (match_operand:SI     2 "const_uimm8_operand")]
+           UNSPEC_LASX_XVPERMI_D))]
   "ISA_HAS_LASX"
   "xvpermi.d\t%u0,%u1,%2"
   [(set_attr "type" "simd_splat")
-   (set_attr "mode" "V4DI")])
+   (set_attr "mode" "<MODE>")])
+
+(define_insn "lasx_xvpermi_d_<mode>_1"
+  [(set (match_operand:LASX_D 0 "register_operand" "=f")
+    (vec_select:LASX_D
+      (match_operand:LASX_D 1 "register_operand" "f")
+      (parallel [(match_operand 2 "const_0_to_3_operand")
+             (match_operand 3 "const_0_to_3_operand")
+             (match_operand 4 "const_0_to_3_operand")
+             (match_operand 5 "const_0_to_3_operand")])))]
+  "ISA_HAS_LASX"
+{
+  int mask = 0;
+  mask |= INTVAL (operands[2]) << 0;
+  mask |= INTVAL (operands[3]) << 2;
+  mask |= INTVAL (operands[4]) << 4;
+  mask |= INTVAL (operands[5]) << 6;
+  operands[2] = GEN_INT (mask);
+  return "xvpermi.d\t%u0,%u1,%2";
+}
+  [(set_attr "type" "simd_splat")
+   (set_attr "mode" "<MODE>")])
 
 ;; xvpermi.q
 (define_insn "lasx_xvpermi_q_<LASX:mode>"
@@ -702,81 +722,26 @@
 
 (define_expand "vec_extract<mode><unitmode>"
   [(match_operand:<UNITMODE> 0 "register_operand")
-   (match_operand:ILASX 1 "register_operand")
+   (match_operand:LASX 1 "register_operand")
    (match_operand 2 "const_<indeximm256>_operand")]
   "ISA_HAS_LASX"
 {
-  if (<UNITMODE>mode == SImode || <UNITMODE>mode == DImode)
-    {
-      emit_insn(gen_lasx_xvpickve2gr_<lasxfmt_wd> (operands[0], operands[1], operands[2]));
-    }
-  else
-    {
-      HOST_WIDE_INT size_0 = GET_MODE_SIZE (GET_MODE (operands[0]));
-      HOST_WIDE_INT size_1 = GET_MODE_SIZE (GET_MODE (operands[1]));
-      HOST_WIDE_INT val = INTVAL (operands[2]);
-
-      /* High part */
-      if (val >= size_1/size_0/2 )
-      {
-        rtx dest1 = gen_reg_rtx (GET_MODE (operands[1]));
-        rtx pos = GEN_INT( val - size_1/size_0/2);
-        emit_insn (gen_lasx_xvpermi_q_<mode256_i> (dest1, dest1, operands[1], GEN_INT(1)));
-        rtx dest2 = gen_reg_rtx (SImode);
-        emit_insn (gen_lsx_vpickve2gr_<lasxfmt> (dest2,
-                                            gen_lowpart(<VHMODE256>mode, dest1),
-					    pos));
-        emit_move_insn (operands[0],
-		     gen_lowpart (<UNITMODE>mode, dest2));
-      }
-      else
-      {
-        rtx dest1 = gen_reg_rtx (SImode);
-        emit_insn (gen_lsx_vpickve2gr_<lasxfmt> (dest1,
-                                            gen_lowpart(<VHMODE256>mode, operands[1]),
-					    operands[2]));
-        emit_move_insn (operands[0],
-		     gen_lowpart (<UNITMODE>mode, dest1));
-      }
-    }
+  loongarch_expand_vector_extract (operands[0], operands[1],
+      INTVAL (operands[2]));
   DONE;
 })
 
-(define_expand "vec_extract<mode><unitmode>"
-  [(match_operand:<UNITMODE> 0 "register_operand")
-   (match_operand:FLASX 1 "register_operand")
-   (match_operand 2 "const_<indeximm256>_operand")]
+(define_expand "vec_perm<mode>"
+ [(match_operand:LASX 0 "register_operand")
+  (match_operand:LASX 1 "register_operand")
+  (match_operand:LASX 2 "register_operand")
+  (match_operand:<VIMODE256> 3 "register_operand")]
   "ISA_HAS_LASX"
 {
-  rtx temp;
-  HOST_WIDE_INT val = INTVAL (operands[2]);
-
-  if (val == 0)
-    temp = operands[1];
-  else
-    {
-      temp = gen_reg_rtx (<MODE>mode);
-      emit_insn (gen_lasx_xvpickve_<lasxfmt_f> (temp, operands[1], operands[2]));
-    }
-  emit_insn (gen_lasx_vec_extract_<lasxfmt_f> (operands[0], temp));
-  DONE;
+   loongarch_expand_vec_perm_1(operands);
+   DONE;
 })
 
-(define_insn_and_split "lasx_vec_extract_<lasxfmt_f>"
-  [(set (match_operand:<UNITMODE> 0 "register_operand" "=f")
-	(vec_select:<UNITMODE>
-	  (match_operand:FLASX 1 "register_operand" "f")
-	  (parallel [(const_int 0)])))]
-  "ISA_HAS_LASX"
-  "#"
-  "&& reload_completed"
-  [(set (match_dup 0) (match_dup 1))]
-{
-   operands[1] = gen_rtx_REG (<UNITMODE>mode, REGNO (operands[1]));
-}
-  [(set_attr "move_type" "fmove")
-   (set_attr "mode" "<UNITMODE>")])
-
 ;; FIXME: 256??
 (define_expand "vcondu<LASX:mode><ILASX:mode>"
   [(match_operand:LASX 0 "register_operand")
@@ -869,7 +834,7 @@
   [(set (match_operand:LASX 0 "nonimmediate_operand")
 	(match_operand:LASX 1 "move_operand"))]
   "reload_completed && ISA_HAS_LASX
-   && loongarch_split_move_insn_p (operands[0], operands[1], insn)"
+   && loongarch_split_move_insn_p (operands[0], operands[1])"
   [(const_int 0)]
 {
   loongarch_split_move_insn (operands[0], operands[1], curr_insn);
@@ -1308,13 +1273,13 @@
   [(set_attr "type" "simd_bit")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "lasx_xvbitsel_<lasxfmt>"
-  [(set (match_operand:ILASX 0 "register_operand" "=f")
-	(ior:ILASX (and:ILASX (not:ILASX
-			      (match_operand:ILASX 3 "register_operand" "f"))
-			    (match_operand:ILASX 1 "register_operand" "f"))
-		  (and:ILASX (match_dup 3)
-			    (match_operand:ILASX 2 "register_operand" "f"))))]
+(define_insn "lasx_xvbitsel_<lasxfmt_f>"
+  [(set (match_operand:LASX 0 "register_operand" "=f")
+        (ior:LASX (and:LASX (not:LASX
+                              (match_operand:LASX 3 "register_operand" "0"))
+                            (match_operand:LASX 1 "register_operand" "f"))
+                  (and:LASX (match_dup 3)
+                            (match_operand:LASX 2 "register_operand" "f"))))]
   "ISA_HAS_LASX"
   "xvbitsel.v\t%u0,%u1,%u2,%u3"
   [(set_attr "type" "simd_bitmov")
@@ -1364,11 +1329,11 @@
   [(set_attr "type" "simd_int_arith")
    (set_attr "mode" "<MODE>")])
 
-(define_expand "vec_cmp<mode><mode>"
-  [(set (match_operand:ILASX 0 "register_operand")
-	(match_operator:ILASX 1 ""
-	  [(match_operand:ILASX 2 "register_operand")
-	   (match_operand:ILASX 3 "register_operand")]))]
+(define_expand "vec_cmp<mode><mode256_i>"
+  [(set (match_operand:<VIMODE256> 0 "register_operand")
+	(match_operator 1 ""
+	  [(match_operand:LASX 2 "register_operand")
+	   (match_operand:LASX 3 "register_operand")]))]
   "ISA_HAS_LASX"
 {
   bool ok = loongarch_expand_int_vec_cmp (operands);
@@ -1376,11 +1341,11 @@
   DONE;
 })
 
-(define_expand "vec_cmp<mode><mode>"
-  [(set (match_operand:FLASX 0 "register_operand")
-	(match_operator:FLASX 1 ""
-	  [(match_operand:FLASX 2 "register_operand")
-	   (match_operand:FLASX 3 "register_operand")]))]
+(define_expand "vec_cmpu<ILASX:mode><mode256_i>"
+  [(set (match_operand:<VIMODE256> 0 "register_operand")
+	(match_operator 1 ""
+	  [(match_operand:ILASX 2 "register_operand")
+	   (match_operand:ILASX 3 "register_operand")]))]
   "ISA_HAS_LASX"
 {
   bool ok = loongarch_expand_fp_vec_cmp (operands);
@@ -2670,9 +2635,6 @@
  [(set_attr "type" "simd_branch")
   (set_attr "mode" "<MODE>")])
 
-
-
-
 ;; loongson-asx.
 (define_insn "lasx_vext2xv_h<u>_b<u>"
  [(set (match_operand:V16HI 0 "register_operand" "=f")
@@ -4723,10 +4685,10 @@
    (set_attr "mode" "<MODE>")
    (set_attr "length" "4")])
 
-(define_insn "lasx_xvinsve0_<lasxfmt>"
-  [(set (match_operand:ILASX_DW 0 "register_operand" "=f")
-	(unspec:ILASX_DW [(match_operand:ILASX_DW 1 "register_operand" "0")
-		     (match_operand:ILASX_DW 2 "register_operand" "f")
+(define_insn "lasx_xvinsve0_<lasxfmt_f>"
+  [(set (match_operand:LASX_WD 0 "register_operand" "=f")
+	(unspec:LASX_WD [(match_operand:LASX_WD 1 "register_operand" "0")
+		     (match_operand:LASX_WD 2 "register_operand" "f")
 		     (match_operand 3 "const_<indeximm256>_operand" "")]
 		    UNSPEC_LASX_XVINSVE0))]
   "ISA_HAS_LASX"
@@ -4744,6 +4706,16 @@
   [(set_attr "type" "simd_shf")
    (set_attr "mode" "<MODE>")])
 
+(define_insn "lasx_xvpickve_<lasxfmt_f>_scalar"
+  [(set (match_operand:<UNITMODE> 0 "register_operand" "=f")
+    (vec_select:<UNITMODE>
+      (match_operand:FLASX 1 "register_operand" "f")
+      (parallel [(match_operand 2 "const_<indeximm256>_operand" "")])))]
+  "ISA_HAS_LASX"
+  "xvpickve.<lasxfmt>\t%u0,%u1,%2"
+  [(set_attr "type" "simd_shf")
+   (set_attr "mode" "<MODE>")])
+
 (define_insn "lasx_xvssrlrn_<hlasxfmt>_<lasxfmt>"
   [(set (match_operand:<VHSMODE256> 0 "register_operand" "=f")
 	(unspec:<VHSMODE256> [(match_operand:ILASX_DWH 1 "register_operand" "f")
@@ -4818,3 +4790,107 @@
   [(set_attr "type" "simd_store")
    (set_attr "mode" "DI")])
 
+;; Vector reduction operation
+(define_expand "reduc_plus_scal_v4di"
+  [(match_operand:DI 0 "register_operand")
+   (match_operand:V4DI 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (V4DImode);
+  rtx tmp1 = gen_reg_rtx (V4DImode);
+  rtx vec_res = gen_reg_rtx (V4DImode);
+  emit_insn (gen_lasx_xvhaddw_q_d (tmp, operands[1], operands[1]));
+  emit_insn (gen_lasx_xvpermi_d_v4di (tmp1, tmp, GEN_INT (2)));
+  emit_insn (gen_addv4di3 (vec_res, tmp, tmp1));
+  emit_insn (gen_vec_extractv4didi (operands[0], vec_res, const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_plus_scal_v8si"
+  [(match_operand:SI 0 "register_operand")
+   (match_operand:V8SI 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (V4DImode);
+  rtx tmp1 = gen_reg_rtx (V4DImode);
+  rtx vec_res = gen_reg_rtx (V4DImode);
+  emit_insn (gen_lasx_xvhaddw_d_w (tmp, operands[1], operands[1]));
+  emit_insn (gen_lasx_xvhaddw_q_d (tmp1, tmp, tmp));
+  emit_insn (gen_lasx_xvpermi_d_v4di (tmp, tmp1, GEN_INT (2)));
+  emit_insn (gen_addv4di3 (vec_res, tmp, tmp1));
+  emit_insn (gen_vec_extractv8sisi (operands[0], gen_lowpart(V8SImode,vec_res), const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_plus_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:FLASX 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_add<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_<optab>_scal_<mode>"
+  [(any_bitwise:<UNITMODE>
+     (match_operand:<UNITMODE> 0 "register_operand")
+     (match_operand:ILASX 1 "register_operand"))]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_<optab><mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_smax_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:LASX 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_smax<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_smin_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:LASX 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_smin<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_umax_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:ILASX 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_umax<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_umin_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:ILASX 1 "register_operand")]
+  "ISA_HAS_LASX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_umin<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
diff --git a/src/gcc/config/loongarch/loongarch-builtins.c b/src/gcc/config/loongarch/loongarch-builtins.c
index 4f580fb7e..177ef8db9 100644
--- a/src/gcc/config/loongarch/loongarch-builtins.c
+++ b/src/gcc/config/loongarch/loongarch-builtins.c
@@ -1,5 +1,5 @@
 /* Subroutines used for expanding LoongArch builtins.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Co. Ltd.
 
 This file is part of GCC.
@@ -137,9 +137,6 @@ struct loongarch_builtin_description
      for more information.  */
   enum insn_code icode;
 
-  /* The floating-point comparison code to use with ICODE, if any.  */
-  enum loongarch_fp_condition cond;
-
   /* The name of the built-in function.  */
   const char *name;
 
@@ -172,10 +169,10 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
 
    AVAIL is the name of the availability predicate, without the leading
    loongarch_builtin_avail_.  */
-#define LARCH_BUILTIN(INSN, COND, NAME, BUILTIN_TYPE, FUNCTION_TYPE, AVAIL) \
+#define LARCH_BUILTIN(INSN, NAME, BUILTIN_TYPE, FUNCTION_TYPE, AVAIL) \
   { \
-    CODE_FOR_loongarch_##INSN, LARCH_FP_COND_##COND, \
-      "__builtin_loongarch_" NAME, BUILTIN_TYPE, FUNCTION_TYPE, \
+    CODE_FOR_loongarch_##INSN, "__builtin_loongarch_" NAME, \
+      BUILTIN_TYPE, FUNCTION_TYPE, \
       loongarch_builtin_avail_##AVAIL \
   }
 
@@ -183,21 +180,21 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    mapped to instruction CODE_FOR_loongarch_<INSN>,  FUNCTION_TYPE and AVAIL
    are as for LARCH_BUILTIN.  */
 #define DIRECT_BUILTIN(INSN, FUNCTION_TYPE, AVAIL) \
-  LARCH_BUILTIN (INSN, f, #INSN, LARCH_BUILTIN_DIRECT, FUNCTION_TYPE, AVAIL)
+  LARCH_BUILTIN (INSN, #INSN, LARCH_BUILTIN_DIRECT, FUNCTION_TYPE, AVAIL)
 
 /* Define __builtin_loongarch_<INSN>, which is a LARCH_BUILTIN_DIRECT_NO_TARGET
    function mapped to instruction CODE_FOR_loongarch_<INSN>,  FUNCTION_TYPE
    and AVAIL are as for LARCH_BUILTIN.  */
 #define DIRECT_NO_TARGET_BUILTIN(INSN, FUNCTION_TYPE, AVAIL) \
-  LARCH_BUILTIN (INSN, f, #INSN, LARCH_BUILTIN_DIRECT_NO_TARGET, \
+  LARCH_BUILTIN (INSN, #INSN, LARCH_BUILTIN_DIRECT_NO_TARGET, \
 		 FUNCTION_TYPE, AVAIL)
 
 /* Define an LSX LARCH_BUILTIN_DIRECT function __builtin_lsx_<INSN>
    for instruction CODE_FOR_lsx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LSX_BUILTIN(INSN, FUNCTION_TYPE)				\
-    { CODE_FOR_lsx_ ## INSN, LARCH_FP_COND_f,				\
-    "__builtin_lsx_" #INSN,  LARCH_BUILTIN_DIRECT,			\
+    { CODE_FOR_lsx_ ## INSN,						\
+    "__builtin_lsx_" #INSN, LARCH_BUILTIN_DIRECT,			\
     FUNCTION_TYPE, loongarch_builtin_avail_lsx }
 
 
@@ -205,7 +202,7 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    for instruction CODE_FOR_lsx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LSX_BUILTIN_TEST_BRANCH(INSN, FUNCTION_TYPE)			\
-    { CODE_FOR_lsx_ ## INSN, LARCH_FP_COND_f,				\
+    { CODE_FOR_lsx_ ## INSN,		     				\
     "__builtin_lsx_" #INSN, LARCH_BUILTIN_LSX_TEST_BRANCH,		\
     FUNCTION_TYPE, loongarch_builtin_avail_lsx }
 
@@ -213,7 +210,7 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    for instruction CODE_FOR_lsx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LSX_NO_TARGET_BUILTIN(INSN, FUNCTION_TYPE)			\
-    { CODE_FOR_lsx_ ## INSN, LARCH_FP_COND_f,				\
+    { CODE_FOR_lsx_ ## INSN,						\
     "__builtin_lsx_" #INSN,  LARCH_BUILTIN_DIRECT_NO_TARGET,		\
     FUNCTION_TYPE, loongarch_builtin_avail_lsx }
 
@@ -221,7 +218,7 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    for instruction CODE_FOR_lasx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LASX_BUILTIN(INSN, FUNCTION_TYPE)				\
-    { CODE_FOR_lasx_ ## INSN, LARCH_FP_COND_f,				\
+    { CODE_FOR_lasx_ ## INSN,						\
     "__builtin_lasx_" #INSN,  LARCH_BUILTIN_LASX,			\
     FUNCTION_TYPE, loongarch_builtin_avail_lasx }
 
@@ -229,7 +226,7 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    for instruction CODE_FOR_lasx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LASX_NO_TARGET_BUILTIN(INSN, FUNCTION_TYPE)			\
-    { CODE_FOR_lasx_ ## INSN, LARCH_FP_COND_f,				\
+    { CODE_FOR_lasx_ ## INSN,						\
     "__builtin_lasx_" #INSN,  LARCH_BUILTIN_DIRECT_NO_TARGET,		\
     FUNCTION_TYPE, loongarch_builtin_avail_lasx }
 
@@ -237,65 +234,10 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
    for instruction CODE_FOR_lasx_<INSN>.  FUNCTION_TYPE is a builtin_description
    field.  */
 #define LASX_BUILTIN_TEST_BRANCH(INSN, FUNCTION_TYPE)			\
-    { CODE_FOR_lasx_ ## INSN, LARCH_FP_COND_f,				\
+    { CODE_FOR_lasx_ ## INSN,						\
     "__builtin_lasx_" #INSN, LARCH_BUILTIN_LASX_TEST_BRANCH,		\
     FUNCTION_TYPE, loongarch_builtin_avail_lasx }
 
-/* LoongArch BASE instructions define CODE_FOR_loongarch_xxx */
-#define	CODE_FOR_loongarch_fmax_sf	CODE_FOR_smaxsf3
-#define	CODE_FOR_loongarch_fmax_df	CODE_FOR_smaxdf3
-#define	CODE_FOR_loongarch_fmin_sf	CODE_FOR_sminsf3
-#define	CODE_FOR_loongarch_fmin_df	CODE_FOR_smindf3
-#define	CODE_FOR_loongarch_fmaxa_sf	CODE_FOR_smaxasf3
-#define	CODE_FOR_loongarch_fmaxa_df	CODE_FOR_smaxadf3
-#define	CODE_FOR_loongarch_fmina_sf	CODE_FOR_sminasf3
-#define	CODE_FOR_loongarch_fmina_df	CODE_FOR_sminadf3
-#define	CODE_FOR_loongarch_fclass_s	CODE_FOR_fclass_s
-#define	CODE_FOR_loongarch_fclass_d	CODE_FOR_fclass_d
-#define CODE_FOR_loongarch_frint_s   CODE_FOR_frint_s
-#define CODE_FOR_loongarch_frint_d   CODE_FOR_frint_d
-#define	CODE_FOR_loongarch_bytepick_w	CODE_FOR_bytepick_w
-#define	CODE_FOR_loongarch_bytepick_d	CODE_FOR_bytepick_d
-#define	CODE_FOR_loongarch_bitrev_4b	CODE_FOR_bitrev_4b
-#define	CODE_FOR_loongarch_bitrev_8b	CODE_FOR_bitrev_8b
-
-/* LoongArch support crc */
-#define	CODE_FOR_loongarch_crc_w_b_w	CODE_FOR_crc_w_b_w
-#define	CODE_FOR_loongarch_crc_w_h_w	CODE_FOR_crc_w_h_w
-#define	CODE_FOR_loongarch_crc_w_w_w	CODE_FOR_crc_w_w_w
-#define	CODE_FOR_loongarch_crc_w_d_w	CODE_FOR_crc_w_d_w
-#define	CODE_FOR_loongarch_crcc_w_b_w	CODE_FOR_crcc_w_b_w
-#define	CODE_FOR_loongarch_crcc_w_h_w	CODE_FOR_crcc_w_h_w
-#define	CODE_FOR_loongarch_crcc_w_w_w	CODE_FOR_crcc_w_w_w
-#define	CODE_FOR_loongarch_crcc_w_d_w	CODE_FOR_crcc_w_d_w
-
-/* Privileged state instruction.  */
-#define CODE_FOR_loongarch_cpucfg CODE_FOR_cpucfg
-#define CODE_FOR_loongarch_asrtle_d CODE_FOR_asrtle_d
-#define CODE_FOR_loongarch_asrtgt_d CODE_FOR_asrtgt_d
-#define CODE_FOR_loongarch_csrrd CODE_FOR_csrrd
-#define CODE_FOR_loongarch_dcsrrd CODE_FOR_dcsrrd
-#define CODE_FOR_loongarch_csrwr CODE_FOR_csrwr
-#define CODE_FOR_loongarch_dcsrwr CODE_FOR_dcsrwr
-#define CODE_FOR_loongarch_csrxchg CODE_FOR_csrxchg
-#define CODE_FOR_loongarch_dcsrxchg CODE_FOR_dcsrxchg
-#define CODE_FOR_loongarch_iocsrrd_b CODE_FOR_iocsrrd_b
-#define CODE_FOR_loongarch_iocsrrd_h CODE_FOR_iocsrrd_h
-#define CODE_FOR_loongarch_iocsrrd_w CODE_FOR_iocsrrd_w
-#define CODE_FOR_loongarch_iocsrrd_d CODE_FOR_iocsrrd_d
-#define CODE_FOR_loongarch_iocsrwr_b CODE_FOR_iocsrwr_b
-#define CODE_FOR_loongarch_iocsrwr_h CODE_FOR_iocsrwr_h
-#define CODE_FOR_loongarch_iocsrwr_w CODE_FOR_iocsrwr_w
-#define CODE_FOR_loongarch_iocsrwr_d CODE_FOR_iocsrwr_d
-#define CODE_FOR_loongarch_lddir CODE_FOR_lddir
-#define CODE_FOR_loongarch_dlddir CODE_FOR_dlddir
-#define CODE_FOR_loongarch_ldpte CODE_FOR_ldpte
-#define CODE_FOR_loongarch_dldpte CODE_FOR_dldpte
-#define CODE_FOR_loongarch_cacop CODE_FOR_cacop
-#define CODE_FOR_loongarch_dcacop CODE_FOR_dcacop
-#define CODE_FOR_loongarch_dbar CODE_FOR_dbar
-#define CODE_FOR_loongarch_ibar CODE_FOR_ibar
-
 /* LoongArch SX define CODE_FOR_lsx_xxx */
 #define CODE_FOR_lsx_vsadd_b CODE_FOR_ssaddv16qi3
 #define CODE_FOR_lsx_vsadd_h CODE_FOR_ssaddv8hi3
@@ -750,6 +692,7 @@ AVAIL_ALL (lasx, ISA_HAS_LASX)
 #define	CODE_FOR_lasx_xvfnmsub_d	CODE_FOR_xvfnmsubv4df4_nmsub4
 
 #define	CODE_FOR_lasx_xvpermi_q	CODE_FOR_lasx_xvpermi_q_v32qi
+#define CODE_FOR_lasx_xvpermi_d CODE_FOR_lasx_xvpermi_d_v4di
 #define	CODE_FOR_lasx_xbnz_v	CODE_FOR_lasx_xbnz_v_b
 #define	CODE_FOR_lasx_xbz_v	CODE_FOR_lasx_xbz_v_b
 
@@ -836,34 +779,15 @@ static const struct loongarch_builtin_description loongarch_builtins[] = {
 #define LARCH_MOVGR2FCSR 1
   DIRECT_NO_TARGET_BUILTIN (movgr2fcsr, LARCH_VOID_FTYPE_UQI_USI, hard_float),
 
-  DIRECT_NO_TARGET_BUILTIN (cacop, LARCH_VOID_FTYPE_USI_USI_SI, default),
-  DIRECT_NO_TARGET_BUILTIN (dcacop, LARCH_VOID_FTYPE_USI_UDI_SI, default),
+  DIRECT_NO_TARGET_BUILTIN (cacop_w, LARCH_VOID_FTYPE_USI_USI_SI, default),
+  DIRECT_NO_TARGET_BUILTIN (cacop_d, LARCH_VOID_FTYPE_USI_UDI_SI, default),
   DIRECT_NO_TARGET_BUILTIN (dbar, LARCH_VOID_FTYPE_USI, default),
   DIRECT_NO_TARGET_BUILTIN (ibar, LARCH_VOID_FTYPE_USI, default),
 
-  DIRECT_BUILTIN (fmax_sf, LARCH_SF_FTYPE_SF_SF, hard_float),
-  DIRECT_BUILTIN (fmax_df, LARCH_DF_FTYPE_DF_DF, hard_float),
-  DIRECT_BUILTIN (fmin_sf, LARCH_SF_FTYPE_SF_SF, hard_float),
-  DIRECT_BUILTIN (fmin_df, LARCH_DF_FTYPE_DF_DF, hard_float),
-  DIRECT_BUILTIN (fmaxa_sf, LARCH_SF_FTYPE_SF_SF, hard_float),
-  DIRECT_BUILTIN (fmaxa_df, LARCH_DF_FTYPE_DF_DF, hard_float),
-  DIRECT_BUILTIN (fmina_sf, LARCH_SF_FTYPE_SF_SF, hard_float),
-  DIRECT_BUILTIN (fmina_df, LARCH_DF_FTYPE_DF_DF, hard_float),
-  DIRECT_BUILTIN (fclass_s, LARCH_SF_FTYPE_SF, hard_float),
-  DIRECT_BUILTIN (fclass_d, LARCH_DF_FTYPE_DF, hard_float),
-  DIRECT_BUILTIN (frint_s, LARCH_SF_FTYPE_SF, hard_float),
-  DIRECT_BUILTIN (frint_d, LARCH_DF_FTYPE_DF, hard_float),
-  DIRECT_BUILTIN (bytepick_w, LARCH_SI_FTYPE_SI_SI_QI, default),
-  DIRECT_BUILTIN (bytepick_d, LARCH_DI_FTYPE_DI_DI_QI, default),
-  DIRECT_BUILTIN (bitrev_4b, LARCH_SI_FTYPE_SI, default),
-  DIRECT_BUILTIN (bitrev_8b, LARCH_DI_FTYPE_DI, default),
-  DIRECT_BUILTIN (cpucfg, LARCH_USI_FTYPE_USI, default),
-  DIRECT_BUILTIN (asrtle_d, LARCH_VOID_FTYPE_DI_DI, default),
-  DIRECT_BUILTIN (asrtgt_d, LARCH_VOID_FTYPE_DI_DI, default),
-  DIRECT_BUILTIN (dlddir, LARCH_DI_FTYPE_DI_UQI, default),
-  DIRECT_BUILTIN (lddir, LARCH_SI_FTYPE_SI_UQI, default),
-  DIRECT_NO_TARGET_BUILTIN (dldpte, LARCH_VOID_FTYPE_DI_UQI, default),
-  DIRECT_NO_TARGET_BUILTIN (ldpte, LARCH_VOID_FTYPE_SI_UQI, default),
+  DIRECT_BUILTIN (lddir_d, LARCH_DI_FTYPE_DI_UQI, default),
+  DIRECT_BUILTIN (lddir_w, LARCH_SI_FTYPE_SI_UQI, default),
+  DIRECT_NO_TARGET_BUILTIN (ldpte_d, LARCH_VOID_FTYPE_DI_UQI, default),
+  DIRECT_NO_TARGET_BUILTIN (ldpte_w, LARCH_VOID_FTYPE_SI_UQI, default),
 
   /* CRC Instrinsic */
 
@@ -876,12 +800,12 @@ static const struct loongarch_builtin_description loongarch_builtins[] = {
   DIRECT_BUILTIN (crcc_w_w_w, LARCH_SI_FTYPE_SI_SI, default),
   DIRECT_BUILTIN (crcc_w_d_w, LARCH_SI_FTYPE_DI_SI, default),
 
-  DIRECT_BUILTIN (csrrd, LARCH_USI_FTYPE_USI, default),
-  DIRECT_BUILTIN (dcsrrd, LARCH_UDI_FTYPE_USI, default),
-  DIRECT_BUILTIN (csrwr, LARCH_USI_FTYPE_USI_USI, default),
-  DIRECT_BUILTIN (dcsrwr, LARCH_UDI_FTYPE_UDI_USI, default),
-  DIRECT_BUILTIN (csrxchg, LARCH_USI_FTYPE_USI_USI_USI, default),
-  DIRECT_BUILTIN (dcsrxchg, LARCH_UDI_FTYPE_UDI_UDI_USI, default),
+  DIRECT_BUILTIN (csrrd_w, LARCH_USI_FTYPE_USI, default),
+  DIRECT_BUILTIN (csrrd_d, LARCH_UDI_FTYPE_USI, default),
+  DIRECT_BUILTIN (csrwr_w, LARCH_USI_FTYPE_USI_USI, default),
+  DIRECT_BUILTIN (csrwr_d, LARCH_UDI_FTYPE_UDI_USI, default),
+  DIRECT_BUILTIN (csrxchg_w, LARCH_USI_FTYPE_USI_USI_USI, default),
+  DIRECT_BUILTIN (csrxchg_d, LARCH_UDI_FTYPE_UDI_UDI_USI, default),
   DIRECT_BUILTIN (iocsrrd_b, LARCH_UQI_FTYPE_USI, default),
   DIRECT_BUILTIN (iocsrrd_h, LARCH_UHI_FTYPE_USI, default),
   DIRECT_BUILTIN (iocsrrd_w, LARCH_USI_FTYPE_USI, default),
@@ -891,6 +815,12 @@ static const struct loongarch_builtin_description loongarch_builtins[] = {
   DIRECT_NO_TARGET_BUILTIN (iocsrwr_w, LARCH_VOID_FTYPE_USI_USI, default),
   DIRECT_NO_TARGET_BUILTIN (iocsrwr_d, LARCH_VOID_FTYPE_UDI_USI, default),
 
+  DIRECT_BUILTIN (cpucfg, LARCH_USI_FTYPE_USI, default),
+  DIRECT_NO_TARGET_BUILTIN (asrtle_d, LARCH_VOID_FTYPE_DI_DI, default),
+  DIRECT_NO_TARGET_BUILTIN (asrtgt_d, LARCH_VOID_FTYPE_DI_DI, default),
+  DIRECT_NO_TARGET_BUILTIN (syscall, LARCH_VOID_FTYPE_USI, default),
+  DIRECT_NO_TARGET_BUILTIN (break, LARCH_VOID_FTYPE_USI, default),
+
   /* Built-in functions for LSX.  */
   LSX_BUILTIN (vsll_b, LARCH_V16QI_FTYPE_V16QI_V16QI),
   LSX_BUILTIN (vsll_h, LARCH_V8HI_FTYPE_V8HI_V8HI),
@@ -2377,6 +2307,20 @@ static GTY (()) tree loongarch_builtin_decls[ARRAY_SIZE (loongarch_builtins)];
    using the instruction code or return null if not defined for the target.  */
 static GTY (()) int loongarch_get_builtin_decl_index[NUM_INSN_CODES];
 
+/* Return a type for 'const volatile void*'.  */
+
+static tree
+loongarch_build_cvpointer_type (void)
+{
+  static tree cache;
+
+  if (cache == NULL_TREE)
+    cache = build_pointer_type (build_qualified_type (void_type_node,
+						      TYPE_QUAL_CONST
+						      | TYPE_QUAL_VOLATILE));
+  return cache;
+}
+
 
 /* MODE is a vector mode whose elements have type TYPE.  Return the type
    of the vector itself.  */
@@ -2397,20 +2341,6 @@ loongarch_builtin_vector_type (tree type, machine_mode mode)
   return types[mode_index];
 }
 
-/* Return a type for 'const volatile void *'.  */
-
-static tree
-loongarch_build_cvpointer_type (void)
-{
-  static tree cache;
-
-  if (cache == NULL_TREE)
-    cache = build_pointer_type (build_qualified_type (void_type_node,
-						      TYPE_QUAL_CONST
-						       | TYPE_QUAL_VOLATILE));
-  return cache;
-}
-
 /* Source-level argument types.  */
 #define LARCH_ATYPE_VOID void_type_node
 #define LARCH_ATYPE_INT integer_type_node
diff --git a/src/gcc/config/loongarch/loongarch-c.c b/src/gcc/config/loongarch/loongarch-c.c
index 70ffd3dc0..780a209ec 100644
--- a/src/gcc/config/loongarch/loongarch-c.c
+++ b/src/gcc/config/loongarch/loongarch-c.c
@@ -1,5 +1,5 @@
 /* LoongArch-specific code for C family languages.
-   Copyright (C) 2020-2021 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
@@ -121,7 +121,7 @@ loongarch_cpu_cpp_builtins (cpp_reader *pfile)
   builtin_define_with_int_value ("_LOONGARCH_SZINT", INT_TYPE_SIZE);
   builtin_define_with_int_value ("_LOONGARCH_SZLONG", LONG_TYPE_SIZE);
   builtin_define_with_int_value ("_LOONGARCH_SZPTR", POINTER_SIZE);
-  builtin_define_with_int_value ("_LOONGARCH_FPSET", 32 / MAX_FPRS_PER_FMT);
+  builtin_define_with_int_value ("_LOONGARCH_FPSET", 32);
   builtin_define_with_int_value ("_LOONGARCH_SPFPSET", 32);
 
 }
diff --git a/src/gcc/config/loongarch/loongarch-cpu.c b/src/gcc/config/loongarch/loongarch-cpu.c
index a886dd932..2e2467bf7 100644
--- a/src/gcc/config/loongarch/loongarch-cpu.c
+++ b/src/gcc/config/loongarch/loongarch-cpu.c
@@ -1,5 +1,5 @@
 /* Definitions for LoongArch CPU properties.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch-cpu.h b/src/gcc/config/loongarch/loongarch-cpu.h
index 93d656f70..d949c226b 100644
--- a/src/gcc/config/loongarch/loongarch-cpu.h
+++ b/src/gcc/config/loongarch/loongarch-cpu.h
@@ -1,5 +1,5 @@
 /* Definitions for loongarch native cpu property detection routines.
-   Copyright (C) 2020-2021 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
 
 This file is part of GCC.
 
diff --git a/src/gcc/config/loongarch/loongarch-def.c b/src/gcc/config/loongarch/loongarch-def.c
index c6ce13b64..555570845 100644
--- a/src/gcc/config/loongarch/loongarch-def.c
+++ b/src/gcc/config/loongarch/loongarch-def.c
@@ -1,5 +1,5 @@
 /* LoongArch static properties.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
@@ -33,7 +33,7 @@ along with GCC; see the file COPYING3.  If not see
     .int_mult_di	= COSTS_N_INSNS (1),	\
     .int_div_si		= COSTS_N_INSNS (4),	\
     .int_div_di		= COSTS_N_INSNS (6),	\
-    .branch_cost	= 2,			\
+    .branch_cost	= 6,			\
     .memory_latency	= 4
 
 /* CPU property tables.  */
diff --git a/src/gcc/config/loongarch/loongarch-def.h b/src/gcc/config/loongarch/loongarch-def.h
index 0caad9ff6..0051e463f 100644
--- a/src/gcc/config/loongarch/loongarch-def.h
+++ b/src/gcc/config/loongarch/loongarch-def.h
@@ -1,5 +1,5 @@
 /* LoongArch definitions.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch-driver.c b/src/gcc/config/loongarch/loongarch-driver.c
index 341ef361b..be4b7a1c6 100644
--- a/src/gcc/config/loongarch/loongarch-driver.c
+++ b/src/gcc/config/loongarch/loongarch-driver.c
@@ -1,5 +1,5 @@
 /* Subroutines for the gcc driver.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch-driver.h b/src/gcc/config/loongarch/loongarch-driver.h
index 8d661ce50..a903c2873 100644
--- a/src/gcc/config/loongarch/loongarch-driver.h
+++ b/src/gcc/config/loongarch/loongarch-driver.h
@@ -1,5 +1,5 @@
 /* Subroutine headers for the gcc driver.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
@@ -80,4 +80,3 @@ driver_get_normalized_m_opts (int argc, const char **argv);
   "%{mabi=lp64s:lp64s}" \
 
 #endif /* LOONGARCH_DRIVER_H */
-
diff --git a/src/gcc/config/loongarch/loongarch-ftypes.def b/src/gcc/config/loongarch/loongarch-ftypes.def
index e21ee469a..1ef4e2dc8 100644
--- a/src/gcc/config/loongarch/loongarch-ftypes.def
+++ b/src/gcc/config/loongarch/loongarch-ftypes.def
@@ -1,5 +1,5 @@
 /* Definitions of prototypes for LoongArch built-in functions.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Co. Ltd.
    Based on MIPS target for GNU compiler.
 
@@ -36,56 +36,51 @@ along with GCC; see the file COPYING3.  If not see
 
    Please keep this list lexicographically sorted by the LIST argument.  */
 
-DEF_LARCH_FTYPE (1, (DF, DF))
-DEF_LARCH_FTYPE (2, (DF, DF, DF))
-DEF_LARCH_FTYPE (1, (DF, V2DF))
-DEF_LARCH_FTYPE (1, (DF, V4DF))
+/* Non-vector builtin types.  */
+
+DEF_LARCH_FTYPE (1, (UQI, USI))
+DEF_LARCH_FTYPE (1, (UHI, USI))
+DEF_LARCH_FTYPE (1, (USI, USI))
+DEF_LARCH_FTYPE (1, (UDI, USI))
+DEF_LARCH_FTYPE (1, (USI, UQI))
+DEF_LARCH_FTYPE (1, (VOID, USI))
 
-DEF_LARCH_FTYPE (1, (DI, DI))
-DEF_LARCH_FTYPE (2, (DI, DI, DI))
-DEF_LARCH_FTYPE (3, (DI, DI, DI, QI))
-DEF_LARCH_FTYPE (2, (DI, DI, SI))
-DEF_LARCH_FTYPE (3, (DI, DI, SI, SI))
+DEF_LARCH_FTYPE (2, (VOID, UQI, USI))
+DEF_LARCH_FTYPE (2, (VOID, UHI, USI))
+DEF_LARCH_FTYPE (2, (VOID, USI, USI))
+DEF_LARCH_FTYPE (2, (VOID, UDI, USI))
+DEF_LARCH_FTYPE (2, (VOID, DI, UQI))
+DEF_LARCH_FTYPE (2, (VOID, SI, UQI))
+DEF_LARCH_FTYPE (2, (VOID, DI, DI))
+DEF_LARCH_FTYPE (2, (SI, SI, UQI))
 DEF_LARCH_FTYPE (2, (DI, DI, UQI))
-DEF_LARCH_FTYPE (3, (DI, DI, USI, USI))
+DEF_LARCH_FTYPE (2, (SI, QI, SI))
+DEF_LARCH_FTYPE (2, (SI, HI, SI))
+DEF_LARCH_FTYPE (2, (SI, SI, SI))
+DEF_LARCH_FTYPE (2, (SI, DI, SI))
+DEF_LARCH_FTYPE (2, (USI, USI, USI))
+DEF_LARCH_FTYPE (2, (UDI, UDI, USI))
+
+DEF_LARCH_FTYPE (3, (VOID, USI, USI, SI))
+DEF_LARCH_FTYPE (3, (VOID, USI, UDI, SI))
+DEF_LARCH_FTYPE (3, (USI, USI, USI, USI))
+DEF_LARCH_FTYPE (3, (UDI, UDI, UDI, USI))
+
+/* Vector builtin types.  */
+
+DEF_LARCH_FTYPE (1, (DF, V2DF))
+DEF_LARCH_FTYPE (1, (DF, V4DF))
 DEF_LARCH_FTYPE (3, (DI, DI, V2HI, V2HI))
 DEF_LARCH_FTYPE (3, (DI, DI, V4QI, V4QI))
-DEF_LARCH_FTYPE (2, (DI, POINTER, SI))
-DEF_LARCH_FTYPE (1, (DI, SI))
-DEF_LARCH_FTYPE (2, (DI, SI, SI))
-DEF_LARCH_FTYPE (1, (DI, UQI))
-DEF_LARCH_FTYPE (2, (DI, USI, USI))
-
 DEF_LARCH_FTYPE (2, (DI, V2DI, UQI))
 DEF_LARCH_FTYPE (2, (DI, V4DI, UQI))
 
-DEF_LARCH_FTYPE (2, (HI, HI, HI))
-
-DEF_LARCH_FTYPE (2, (INT, DF, DF))
-DEF_LARCH_FTYPE (2, (INT, SF, SF))
-
-DEF_LARCH_FTYPE (2, (QI, QI, QI))
-
 DEF_LARCH_FTYPE (2, (INT, V2SF, V2SF))
 DEF_LARCH_FTYPE (4, (INT, V2SF, V2SF, V2SF, V2SF))
 
-DEF_LARCH_FTYPE (1, (SF, SF))
-DEF_LARCH_FTYPE (2, (SF, SF, SF))
-
 DEF_LARCH_FTYPE (1, (SF, V2SF))
 DEF_LARCH_FTYPE (1, (SF, V4SF))
 
-DEF_LARCH_FTYPE (2, (SI, DI, SI))
-DEF_LARCH_FTYPE (2, (SI, HI, SI))
-DEF_LARCH_FTYPE (2, (SI, POINTER, SI))
-DEF_LARCH_FTYPE (2, (SI, QI, SI))
-DEF_LARCH_FTYPE (1, (SI, SI))
-DEF_LARCH_FTYPE (2, (SI, SI, SI))
-DEF_LARCH_FTYPE (3, (SI, SI, SI, QI))
-DEF_LARCH_FTYPE (3, (SI, SI, SI, SI))
-DEF_LARCH_FTYPE (2, (SI, SI, UQI))
-DEF_LARCH_FTYPE (1, (SI, UDI))
-DEF_LARCH_FTYPE (1, (SI, UQI))
 DEF_LARCH_FTYPE (1, (SI, UV16QI))
 DEF_LARCH_FTYPE (1, (SI, UV32QI))
 DEF_LARCH_FTYPE (1, (SI, UV2DI))
@@ -103,25 +98,7 @@ DEF_LARCH_FTYPE (2, (SI, V4QI, V4QI))
 DEF_LARCH_FTYPE (2, (SI, V4SI, UQI))
 DEF_LARCH_FTYPE (2, (SI, V8SI, UQI))
 DEF_LARCH_FTYPE (2, (SI, V8HI, UQI))
-DEF_LARCH_FTYPE (1, (SI, VOID))
-
-DEF_LARCH_FTYPE (2, (UDI, UDI, UDI))
-DEF_LARCH_FTYPE (3, (UDI, UDI, UDI, USI))
-DEF_LARCH_FTYPE (2, (UDI, UDI, USI))
-DEF_LARCH_FTYPE (1, (UDI, USI))
-
-DEF_LARCH_FTYPE (1, (UHI, USI))
-
-DEF_LARCH_FTYPE (1, (UQI, USI))
-
-DEF_LARCH_FTYPE (1, (USI, UQI))
-DEF_LARCH_FTYPE (1, (USI, USI))
-DEF_LARCH_FTYPE (2, (USI, USI, USI))
-DEF_LARCH_FTYPE (3, (USI, USI, USI, USI))
-DEF_LARCH_FTYPE (1, (USI, VOID))
 
-DEF_LARCH_FTYPE (2, (VOID, SI, UQI))
-DEF_LARCH_FTYPE (2, (VOID, DI, UQI))
 DEF_LARCH_FTYPE (2, (USI, V32QI, UQI))
 DEF_LARCH_FTYPE (2, (UDI, UV2SI, UV2SI))
 DEF_LARCH_FTYPE (2, (USI, V8SI, UQI))
@@ -487,19 +464,6 @@ DEF_LARCH_FTYPE (2, (V8QI, V4HI, V4HI))
 DEF_LARCH_FTYPE (1, (V8QI, V8QI))
 DEF_LARCH_FTYPE (2, (V8QI, V8QI, V8QI))
 
-DEF_LARCH_FTYPE (2, (VOID, SI, CVPOINTER))
-DEF_LARCH_FTYPE (2, (VOID, SI, SI))
-DEF_LARCH_FTYPE (2, (VOID, DI, DI))
-DEF_LARCH_FTYPE (2, (VOID, UQI, SI))
-DEF_LARCH_FTYPE (1, (VOID, USI))
-DEF_LARCH_FTYPE (2, (VOID, USI, UQI))
-DEF_LARCH_FTYPE (1, (VOID, UHI))
-DEF_LARCH_FTYPE (2, (VOID, UQI, USI))
-DEF_LARCH_FTYPE (2, (VOID, UHI, USI))
-DEF_LARCH_FTYPE (2, (VOID, USI, USI))
-DEF_LARCH_FTYPE (2, (VOID, UDI, USI))
-DEF_LARCH_FTYPE (3, (VOID, USI, USI, SI))
-DEF_LARCH_FTYPE (3, (VOID, USI, UDI, SI))
 DEF_LARCH_FTYPE (3, (VOID, V16QI, CVPOINTER, SI))
 DEF_LARCH_FTYPE (3, (VOID, V16QI, CVPOINTER, DI))
 DEF_LARCH_FTYPE (3, (VOID, V32QI, CVPOINTER, SI))
@@ -659,36 +623,36 @@ DEF_LARCH_FTYPE (3, (V4SI, V4SI, UV16QI, V16QI))
 DEF_LARCH_FTYPE (3, (UV4SI, UV4SI, UV16QI, UV16QI))
 
 
-DEF_LARCH_FTYPE(2,(V4DI,V16HI,V16HI))
-DEF_LARCH_FTYPE(2,(V4DI,UV4SI,V4SI))
-DEF_LARCH_FTYPE(2,(V8SI,UV16HI,V16HI))
-DEF_LARCH_FTYPE(2,(V16HI,UV32QI,V32QI))
-DEF_LARCH_FTYPE(2,(V4DI,UV8SI,V8SI))
-DEF_LARCH_FTYPE(3,(V4DI,V4DI,V16HI,V16HI))
-DEF_LARCH_FTYPE(2,(UV32QI,V32QI,UV32QI))
-DEF_LARCH_FTYPE(2,(UV16HI,V16HI,UV16HI))
-DEF_LARCH_FTYPE(2,(UV8SI,V8SI,UV8SI))
-DEF_LARCH_FTYPE(2,(UV4DI,V4DI,UV4DI))
-DEF_LARCH_FTYPE(3,(V4DI,V4DI,UV4DI,V4DI))
-DEF_LARCH_FTYPE(3,(V4DI,V4DI,UV8SI,V8SI))
-DEF_LARCH_FTYPE(3,(V8SI,V8SI,UV16HI,V16HI))
-DEF_LARCH_FTYPE(3,(V16HI,V16HI,UV32QI,V32QI))
-DEF_LARCH_FTYPE(2,(V4DI,UV4DI,V4DI))
-DEF_LARCH_FTYPE(2,(V8SI,V32QI,V32QI))
-DEF_LARCH_FTYPE(2,(UV4DI,UV16HI,UV16HI))
-DEF_LARCH_FTYPE(2,(V4DI,UV16HI,V16HI))
-DEF_LARCH_FTYPE(3,(V8SI,V8SI,V32QI,V32QI))
-DEF_LARCH_FTYPE(3,(UV8SI,UV8SI,UV32QI,UV32QI))
-DEF_LARCH_FTYPE(3,(UV4DI,UV4DI,UV16HI,UV16HI))
-DEF_LARCH_FTYPE(3,(V8SI,V8SI,UV32QI,V32QI))
-DEF_LARCH_FTYPE(3,(V4DI,V4DI,UV16HI,V16HI))
-DEF_LARCH_FTYPE(2,(UV8SI,UV32QI,UV32QI))
-DEF_LARCH_FTYPE(2,(V8SI,UV32QI,V32QI))
-
-DEF_LARCH_FTYPE(4,(VOID,V16QI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V8HI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V4SI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V2DI,CVPOINTER,SI,UQI))
+DEF_LARCH_FTYPE (2, (V4DI, V16HI, V16HI))
+DEF_LARCH_FTYPE (2, (V4DI, UV4SI, V4SI))
+DEF_LARCH_FTYPE (2, (V8SI, UV16HI, V16HI))
+DEF_LARCH_FTYPE (2, (V16HI, UV32QI, V32QI))
+DEF_LARCH_FTYPE (2, (V4DI, UV8SI, V8SI))
+DEF_LARCH_FTYPE (3, (V4DI, V4DI, V16HI, V16HI))
+DEF_LARCH_FTYPE (2, (UV32QI, V32QI, UV32QI))
+DEF_LARCH_FTYPE (2, (UV16HI, V16HI, UV16HI))
+DEF_LARCH_FTYPE (2, (UV8SI, V8SI, UV8SI))
+DEF_LARCH_FTYPE (2, (UV4DI, V4DI, UV4DI))
+DEF_LARCH_FTYPE (3, (V4DI, V4DI, UV4DI, V4DI))
+DEF_LARCH_FTYPE (3, (V4DI, V4DI, UV8SI, V8SI))
+DEF_LARCH_FTYPE (3, (V8SI, V8SI, UV16HI, V16HI))
+DEF_LARCH_FTYPE (3, (V16HI, V16HI, UV32QI, V32QI))
+DEF_LARCH_FTYPE (2, (V4DI, UV4DI, V4DI))
+DEF_LARCH_FTYPE (2, (V8SI, V32QI, V32QI))
+DEF_LARCH_FTYPE (2, (UV4DI, UV16HI, UV16HI))
+DEF_LARCH_FTYPE (2, (V4DI, UV16HI, V16HI))
+DEF_LARCH_FTYPE (3, (V8SI, V8SI, V32QI, V32QI))
+DEF_LARCH_FTYPE (3, (UV8SI, UV8SI, UV32QI, UV32QI))
+DEF_LARCH_FTYPE (3, (UV4DI, UV4DI, UV16HI, UV16HI))
+DEF_LARCH_FTYPE (3, (V8SI, V8SI, UV32QI, V32QI))
+DEF_LARCH_FTYPE (3, (V4DI, V4DI, UV16HI, V16HI))
+DEF_LARCH_FTYPE (2, (UV8SI, UV32QI, UV32QI))
+DEF_LARCH_FTYPE (2, (V8SI, UV32QI, V32QI))
+
+DEF_LARCH_FTYPE (4,  (VOID, V16QI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4,  (VOID, V8HI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4,  (VOID, V4SI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4,  (VOID, V2DI, CVPOINTER, SI, UQI))
 
 DEF_LARCH_FTYPE (2, (DI, V16QI, UQI))
 DEF_LARCH_FTYPE (2, (DI, V8HI, UQI))
@@ -710,16 +674,16 @@ DEF_LARCH_FTYPE (3, (UV16HI, UV16HI, V16HI, USI))
 DEF_LARCH_FTYPE (3, (UV8SI, UV8SI, V8SI, USI))
 DEF_LARCH_FTYPE (3, (UV4DI, UV4DI, V4DI, USI))
 
-DEF_LARCH_FTYPE(4,(VOID,V32QI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V16HI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V8SI,CVPOINTER,SI,UQI))
-DEF_LARCH_FTYPE(4,(VOID,V4DI,CVPOINTER,SI,UQI))
+DEF_LARCH_FTYPE (4, (VOID, V32QI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4, (VOID, V16HI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4, (VOID, V8SI, CVPOINTER, SI, UQI))
+DEF_LARCH_FTYPE (4, (VOID, V4DI, CVPOINTER, SI, UQI))
 
-DEF_LARCH_FTYPE (1, (BOOLEAN,V16QI))
-DEF_LARCH_FTYPE(2,(V16QI,CVPOINTER,CVPOINTER))
-DEF_LARCH_FTYPE(3,(VOID,V16QI,CVPOINTER,CVPOINTER))
-DEF_LARCH_FTYPE(2,(V32QI,CVPOINTER,CVPOINTER))
-DEF_LARCH_FTYPE(3,(VOID,V32QI,CVPOINTER,CVPOINTER))
+DEF_LARCH_FTYPE (1, (BOOLEAN, V16QI))
+DEF_LARCH_FTYPE (2, (V16QI, CVPOINTER, CVPOINTER))
+DEF_LARCH_FTYPE (3, (VOID, V16QI, CVPOINTER, CVPOINTER))
+DEF_LARCH_FTYPE (2, (V32QI, CVPOINTER, CVPOINTER))
+DEF_LARCH_FTYPE (3, (VOID, V32QI, CVPOINTER, CVPOINTER))
 
 DEF_LARCH_FTYPE (3, (V16QI, V16QI, SI, UQI))
 DEF_LARCH_FTYPE (3, (V2DI, V2DI, SI, UQI))
diff --git a/src/gcc/config/loongarch/loongarch-modes.def b/src/gcc/config/loongarch/loongarch-modes.def
index 46f5c3338..53392b484 100644
--- a/src/gcc/config/loongarch/loongarch-modes.def
+++ b/src/gcc/config/loongarch/loongarch-modes.def
@@ -1,5 +1,5 @@
 /* LoongArch extra machine modes.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Co. Ltd.
    Based on MIPS target for GNU compiler.
 
diff --git a/src/gcc/config/loongarch/loongarch-opts.c b/src/gcc/config/loongarch/loongarch-opts.c
index fed795f6a..a5c208043 100644
--- a/src/gcc/config/loongarch/loongarch-opts.c
+++ b/src/gcc/config/loongarch/loongarch-opts.c
@@ -1,5 +1,5 @@
 /* Subroutines for loongarch-specific option handling.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
@@ -366,7 +366,7 @@ config_target_isa:
   abi_tmp = t.abi;
   isa_min = &isa_required (abi_tmp);
 
-  if (isa_base_compat_p (&t.isa, isa_min)); /* OK.  */
+  if (isa_base_compat_p (&t.isa, isa_min)); /* OK */
   else if (!constrained.arch)
     {
       /* Base architecture can only be implied by -march,
@@ -397,7 +397,7 @@ config_target_isa:
   else
     goto fatal;
 
-  if (isa_fpu_compat_p (&t.isa, isa_min)); /* OK.  */
+  if (isa_fpu_compat_p (&t.isa, isa_min)); /* OK */
   else if (!constrained.fpu)
     t.isa.fpu = isa_min->fpu;
   else if (!constrained.abi_base)
diff --git a/src/gcc/config/loongarch/loongarch-opts.h b/src/gcc/config/loongarch/loongarch-opts.h
index 044abf777..2b82256ac 100644
--- a/src/gcc/config/loongarch/loongarch-opts.h
+++ b/src/gcc/config/loongarch/loongarch-opts.h
@@ -1,5 +1,5 @@
 /* Definitions for loongarch-specific option handling.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch-protos.h b/src/gcc/config/loongarch/loongarch-protos.h
index 2f349d37d..210805cc5 100644
--- a/src/gcc/config/loongarch/loongarch-protos.h
+++ b/src/gcc/config/loongarch/loongarch-protos.h
@@ -22,22 +22,6 @@ along with GCC; see the file COPYING3.  If not see
 #ifndef GCC_LOONGARCH_PROTOS_H
 #define GCC_LOONGARCH_PROTOS_H
 
-/* Describes how a symbol is used.
-
-   SYMBOL_CONTEXT_CALL
-       The symbol is used as the target of a call instruction.
-
-   SYMBOL_CONTEXT_LEA
-       The symbol is used in a load-address operation.
-
-   SYMBOL_CONTEXT_MEM
-       The symbol is used as the address in a MEM.  */
-enum loongarch_symbol_context {
-  SYMBOL_CONTEXT_CALL,
-  SYMBOL_CONTEXT_LEA,
-  SYMBOL_CONTEXT_MEM
-};
-
 /* Classifies a SYMBOL_REF, LABEL_REF or UNSPEC address.
 
    SYMBOL_GOT_DISP
@@ -59,41 +43,6 @@ enum loongarch_symbol_type {
 };
 #define NUM_SYMBOL_TYPES (SYMBOL_TLSLDM + 1)
 
-/* Classifies a type of call.
-
-   LARCH_CALL_NORMAL
-	A normal call or call_value pattern.
-
-   LARCH_CALL_SIBCALL
-	A sibcall or sibcall_value pattern.
-
-   LARCH_CALL_EPILOGUE
-	A call inserted in the epilogue.  */
-enum loongarch_call_type {
-  LARCH_CALL_NORMAL,
-  LARCH_CALL_SIBCALL,
-  LARCH_CALL_EPILOGUE
-};
-
-/* Controls the conditions under which certain instructions are split.
-
-   SPLIT_IF_NECESSARY
-	Only perform splits that are necessary for correctness
-	(because no unsplit version exists).
-
-   SPLIT_FOR_SPEED
-	Perform splits that are necessary for correctness or
-	beneficial for code speed.
-
-   SPLIT_FOR_SIZE
-	Perform splits that are necessary for correctness or
-	beneficial for code size.  */
-enum loongarch_split_type {
-  SPLIT_IF_NECESSARY,
-  SPLIT_FOR_SPEED,
-  SPLIT_FOR_SIZE
-};
-
 extern const char *const loongarch_fp_conditions[16];
 
 /* Routines implemented in loongarch.c.  */
@@ -102,11 +51,9 @@ extern HOST_WIDE_INT loongarch_initial_elimination_offset (int, int);
 extern void loongarch_expand_prologue (void);
 extern void loongarch_expand_epilogue (bool);
 extern bool loongarch_can_use_return_insn (void);
-extern rtx loongarch_function_value (const_tree, const_tree, enum machine_mode);
-extern bool loongarch_symbolic_constant_p (rtx, enum loongarch_symbol_context,
-					   enum loongarch_symbol_type *);
+
+extern bool loongarch_symbolic_constant_p (rtx, enum loongarch_symbol_type *);
 extern int loongarch_regno_mode_ok_for_base_p (int, machine_mode, bool);
-extern bool loongarch_stack_address_p (rtx, machine_mode);
 extern int loongarch_address_insns (rtx, machine_mode, bool);
 extern int loongarch_const_insns (rtx);
 extern int loongarch_split_const_insns (rtx);
@@ -124,9 +71,9 @@ extern bool loongarch_legitimize_move (machine_mode, rtx, rtx);
 extern rtx loongarch_legitimize_call_address (rtx);
 
 extern rtx loongarch_subword (rtx, bool);
-extern bool loongarch_split_move_p (rtx, rtx, enum loongarch_split_type);
-extern void loongarch_split_move (rtx, rtx, enum loongarch_split_type, rtx);
-extern bool loongarch_split_move_insn_p (rtx, rtx, rtx);
+extern bool loongarch_split_move_p (rtx, rtx);
+extern void loongarch_split_move (rtx, rtx, rtx);
+extern bool loongarch_split_move_insn_p (rtx, rtx);
 extern void loongarch_split_move_insn (rtx, rtx, rtx);
 extern void loongarch_split_128bit_move (rtx, rtx);
 extern bool loongarch_split_128bit_move_p (rtx, rtx);
@@ -142,7 +89,7 @@ extern void loongarch_expand_scc (rtx *);
 extern bool loongarch_expand_int_vec_cmp (rtx *);
 extern bool loongarch_expand_fp_vec_cmp (rtx *);
 extern void loongarch_expand_conditional_branch (rtx *);
-extern void loongarch_expand_conditional_move (rtx *);
+extern bool loongarch_expand_conditional_move_la464 (rtx *);
 extern void loongarch_expand_conditional_trap (rtx);
 #endif
 extern void loongarch_set_return_address (rtx, rtx);
@@ -157,20 +104,6 @@ extern HOST_WIDE_INT loongarch_debugger_offset (rtx, HOST_WIDE_INT);
 
 extern void loongarch_output_external (FILE *, tree, const char *);
 extern void loongarch_output_ascii (FILE *, const char *, size_t);
-extern void loongarch_output_aligned_decl_common (FILE *, tree, const char *,
-						  unsigned HOST_WIDE_INT,
-						  unsigned int);
-extern void loongarch_declare_common_object (FILE *, const char *,
-					     const char *,
-					     unsigned HOST_WIDE_INT,
-					     unsigned int, bool);
-extern void loongarch_declare_object (FILE *, const char *, const char *,
-				      const char *, ...) ATTRIBUTE_PRINTF_4;
-extern void loongarch_declare_object_name (FILE *, const char *, tree);
-extern void loongarch_finish_declare_object (FILE *, tree, int, int);
-extern void loongarch_set_text_contents_type (FILE *, const char *,
-					      unsigned long, bool);
-
 extern bool loongarch_small_data_pattern_p (rtx);
 extern rtx loongarch_rewrite_small_data (rtx);
 extern rtx loongarch_return_addr (int, rtx);
@@ -233,12 +166,16 @@ extern void loongarch_expand_atomic_qihi (union loongarch_gen_fn_ptrs,
 extern void loongarch_expand_vector_init (rtx, rtx);
 extern void loongarch_expand_vec_unpack (rtx op[2], bool, bool);
 extern void loongarch_expand_vec_perm (rtx, rtx, rtx, rtx);
+extern void loongarch_expand_vec_perm_1 (rtx[]);
+extern void loongarch_expand_vector_extract (rtx, rtx, int);
+extern void loongarch_expand_vector_reduc (rtx (*)(rtx, rtx, rtx), rtx, rtx);
 
 extern int loongarch_ldst_scaled_shift (machine_mode);
 extern bool loongarch_signed_immediate_p (unsigned HOST_WIDE_INT, int, int);
 extern bool loongarch_unsigned_immediate_p (unsigned HOST_WIDE_INT, int, int);
 extern bool loongarch_12bit_offset_address_p (rtx, machine_mode);
 extern bool loongarch_14bit_shifted_offset_address_p (rtx, machine_mode);
+extern bool loongarch_base_index_address_p (rtx, machine_mode);
 extern bool loongarch_9bit_offset_address_p (rtx, machine_mode);
 extern rtx loongarch_expand_thread_pointer (rtx);
 
@@ -256,8 +193,6 @@ typedef rtx (*mulsidi3_gen_fn) (rtx, rtx, rtx);
 extern void loongarch_register_frame_header_opt (void);
 extern void loongarch_expand_vec_cond_expr (machine_mode, machine_mode, rtx *);
 
-extern void loongarch_declare_function_name (FILE *, const char *, tree);
-
 /* Routines implemented in loongarch-c.c.  */
 void loongarch_cpu_cpp_builtins (cpp_reader *);
 
diff --git a/src/gcc/config/loongarch/loongarch-str.h b/src/gcc/config/loongarch/loongarch-str.h
index 25406ab5b..4c6f0a16b 100644
--- a/src/gcc/config/loongarch/loongarch-str.h
+++ b/src/gcc/config/loongarch/loongarch-str.h
@@ -1,7 +1,7 @@
 /* Generated automatically by "genstr" from "loongarch-strings".
    Please do not edit this file directly.
 
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch-tune.h b/src/gcc/config/loongarch/loongarch-tune.h
index 6f3530f5c..b1bf6e2e3 100644
--- a/src/gcc/config/loongarch/loongarch-tune.h
+++ b/src/gcc/config/loongarch/loongarch-tune.h
@@ -1,5 +1,5 @@
 /* Definitions for microarchitecture-related data structures.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
 
 This file is part of GCC.
diff --git a/src/gcc/config/loongarch/loongarch.c b/src/gcc/config/loongarch/loongarch.c
index 6b4096042..aad8fd138 100644
--- a/src/gcc/config/loongarch/loongarch.c
+++ b/src/gcc/config/loongarch/loongarch.c
@@ -1,5 +1,5 @@
 /* Subroutines used for LoongArch code generation.
-   Copyright (C) 2021-2022 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Technology Co. Ltd..
    Based on MIPS and RISC-V target for GNU compiler.
 
@@ -63,6 +63,9 @@ along with GCC; see the file COPYING3.  If not see
 #include "context.h"
 #include "builtins.h"
 #include "rtl-iter.h"
+#include "cfgloop.h"
+#include "gimple-iterator.h"
+#include "tree-vectorizer.h"
 
 /* This file should be included last.  */
 #include "target-def.h"
@@ -97,6 +100,9 @@ along with GCC; see the file COPYING3.  If not see
        A natural register + offset address.  The register satisfies
        loongarch_valid_base_register_p and the offset is a const_arith_operand.
 
+   ADDRESS_REG_REG
+       A base register indexed by (optionally scaled) register.
+
    ADDRESS_CONST_INT
        A signed 16-bit constant address.
 
@@ -105,21 +111,12 @@ along with GCC; see the file COPYING3.  If not see
 enum loongarch_address_type
 {
   ADDRESS_REG,
+  ADDRESS_REG_REG,
   ADDRESS_CONST_INT,
   ADDRESS_SYMBOLIC
 };
 
 
-/* Information about an address described by loongarch_address_type.
-
-   ADDRESS_CONST_INT
-       No fields are used.
-
-   ADDRESS_REG
-       REG is the base register and OFFSET is the constant offset.
-
-   ADDRESS_SYMBOLIC
-       SYMBOL_TYPE is the type of symbol that the address references.  */
 struct loongarch_address_info
 {
   enum loongarch_address_type type;
@@ -153,7 +150,7 @@ enum loongarch_load_imm_method
 struct loongarch_integer_op
 {
   enum rtx_code code;
-  unsigned HOST_WIDE_INT value;
+  HOST_WIDE_INT value;
   enum loongarch_load_imm_method method;
 };
 
@@ -182,7 +179,7 @@ const enum reg_class loongarch_regno_to_class[FIRST_PSEUDO_REGISTER] = {
     GR_REGS,	     GR_REGS,	      GR_REGS,	       GR_REGS,
     JIRL_REGS,       JIRL_REGS,       JIRL_REGS,       JIRL_REGS,
     JIRL_REGS,       JIRL_REGS,       JIRL_REGS,       JIRL_REGS,
-    SIBCALL_REGS,    SIBCALL_REGS,    SIBCALL_REGS,    SIBCALL_REGS,
+    SIBCALL_REGS,    JIRL_REGS,    SIBCALL_REGS,    SIBCALL_REGS,
     SIBCALL_REGS,    SIBCALL_REGS,    SIBCALL_REGS,    SIBCALL_REGS,
     SIBCALL_REGS,    GR_REGS,	      GR_REGS,	       JIRL_REGS,
     JIRL_REGS,       JIRL_REGS,       JIRL_REGS,       JIRL_REGS,
@@ -201,13 +198,6 @@ const enum reg_class loongarch_regno_to_class[FIRST_PSEUDO_REGISTER] = {
     FRAME_REGS,	FRAME_REGS
 };
 
-/* The value of TARGET_ATTRIBUTE_TABLE.  */
-static const struct attribute_spec loongarch_attribute_table[] = {
-    /* { name, min_len, max_len, decl_req, type_req, fn_type_req,
-       affects_type_identity, handler, exclude }  */
-      { NULL, 0, 0, false, false, false, false, NULL, NULL }
-};
-
 /* Which cost information to use.  */
 static const struct loongarch_rtx_cost_data *loongarch_cost;
 
@@ -392,7 +382,7 @@ loongarch_flatten_aggregate_argument (const_tree type,
 				      loongarch_aggregate_field fields[2],
 				      const int use_vecarg_p)
 {
-  if (!type || (TREE_CODE (type) != RECORD_TYPE 
+  if (!type || (TREE_CODE (type) != RECORD_TYPE
 		|| (use_vecarg_p && TREE_CODE (type) == VECTOR_TYPE)))
     return -1;
 
@@ -403,9 +393,9 @@ loongarch_flatten_aggregate_argument (const_tree type,
    two floating-point registers.  If so, populate FIELDS accordingly.  */
 
 static unsigned
-loongarch_pass_aggregate_in_fpr_pair_p (const_tree type,
-					loongarch_aggregate_field fields[2],
-					const int use_vecarg_p)
+loongarch_pass_aggregate_num_fpr (const_tree type,
+				  loongarch_aggregate_field fields[2],
+				  const int use_vecarg_p)
 {
   int n = loongarch_flatten_aggregate_argument (type, fields, use_vecarg_p);
 
@@ -416,7 +406,7 @@ loongarch_pass_aggregate_in_fpr_pair_p (const_tree type,
   return n > 0 ? n : 0;
 }
 
-/* See whether TYPE is a record whose fields should be returned in one or
+/* See whether TYPE is a record whose fields should be returned in one
    floating-point register and one integer register.  If so, populate
    FIELDS accordingly.  */
 
@@ -447,13 +437,14 @@ loongarch_pass_aggregate_in_fpr_and_gpr_p (const_tree type,
 
 static rtx
 loongarch_pass_fpr_single (machine_mode type_mode, unsigned regno,
-			   machine_mode value_mode)
+			   machine_mode value_mode,
+			   HOST_WIDE_INT offset)
 {
   rtx x = gen_rtx_REG (value_mode, regno);
 
   if (type_mode != value_mode)
     {
-      x = gen_rtx_EXPR_LIST (VOIDmode, x, const0_rtx);
+      x = gen_rtx_EXPR_LIST (VOIDmode, x, GEN_INT (offset));
       x = gen_rtx_PARALLEL (type_mode, gen_rtvec (1, x));
     }
   return x;
@@ -511,13 +502,14 @@ loongarch_get_arg_info (struct loongarch_arg_info *info,
 
       /* Pass one- or two-element floating-point aggregates in FPRs.  */
       if ((info->num_fprs
-	   = loongarch_pass_aggregate_in_fpr_pair_p (type, fields, use_vecarg_p))
+	   = loongarch_pass_aggregate_num_fpr (type, fields, use_vecarg_p))
 	  && info->fpr_offset + info->num_fprs <= MAX_ARGS_IN_REGISTERS)
 	switch (info->num_fprs)
 	  {
 	  case 1:
 	    return loongarch_pass_fpr_single (mode, fregno,
-					      TYPE_MODE (fields[0].type));
+					      TYPE_MODE (fields[0].type),
+					      fields[0].offset);
 
 	  case 2:
 	    return loongarch_pass_fpr_pair (mode, fregno,
@@ -629,7 +621,8 @@ loongarch_arg_partial_bytes (cumulative_args_t cum,
 {
   struct loongarch_arg_info arg;
 
-  loongarch_get_arg_info (&arg, get_cumulative_args (cum), mode, type, named, false);
+  loongarch_get_arg_info (&arg, get_cumulative_args (cum),
+			  mode, type, named, false);
   return arg.stack_p ? arg.num_gprs * UNITS_PER_WORD : 0;
 }
 
@@ -637,8 +630,9 @@ loongarch_arg_partial_bytes (cumulative_args_t cum,
    VALTYPE is the return type and MODE is VOIDmode.  For libcalls,
    VALTYPE is null and MODE is the mode of the return value.  */
 
-rtx
-loongarch_function_value (const_tree type, const_tree func, machine_mode mode)
+static rtx
+loongarch_function_value_1 (const_tree type, const_tree func,
+			    machine_mode mode)
 {
   struct loongarch_arg_info info;
   CUMULATIVE_ARGS args;
@@ -654,11 +648,30 @@ loongarch_function_value (const_tree type, const_tree func, machine_mode mode)
       mode = promote_function_mode (type, mode, &unsigned_p, func, 1);
     }
 
-  memset (&args, 0, sizeof args);
+  memset (&args, 0, sizeof (args));
   return loongarch_get_arg_info (&info, &args, mode, type, true, true);
 }
 
-/* Implement TARGET_PASS_BY_REFERENCE. */
+
+/* Implement TARGET_FUNCTION_VALUE.  */
+
+static rtx
+loongarch_function_value (const_tree valtype, const_tree fn_decl_or_type,
+			  bool outgoing ATTRIBUTE_UNUSED)
+{
+  return loongarch_function_value_1 (valtype, fn_decl_or_type, VOIDmode);
+}
+
+/* Implement TARGET_LIBCALL_VALUE.  */
+
+static rtx
+loongarch_libcall_value (machine_mode mode, const_rtx fun ATTRIBUTE_UNUSED)
+{
+  return loongarch_function_value_1 (NULL_TREE, NULL_TREE, mode);
+}
+
+
+/* Implement TARGET_PASS_BY_REFERENCE.  */
 
 static bool
 loongarch_pass_by_reference (cumulative_args_t cum_v, machine_mode mode,
@@ -686,7 +699,8 @@ loongarch_pass_by_reference (cumulative_args_t cum_v, machine_mode mode,
 /* Implement TARGET_RETURN_IN_MEMORY.  */
 
 static bool
-loongarch_return_in_memory (const_tree type, const_tree fndecl ATTRIBUTE_UNUSED)
+loongarch_return_in_memory (const_tree type,
+			    const_tree fndecl ATTRIBUTE_UNUSED)
 {
   CUMULATIVE_ARGS args;
   cumulative_args_t cum = pack_cumulative_args (&args);
@@ -712,7 +726,8 @@ loongarch_setup_incoming_varargs (cumulative_args_t cum,
      argument.  Advance a local copy of CUM past the last "real" named
      argument, to find out how many registers are left over.  */
   local_cum = *get_cumulative_args (cum);
-  loongarch_function_arg_advance (pack_cumulative_args (&local_cum), mode, type, 1);
+  loongarch_function_arg_advance (pack_cumulative_args (&local_cum),
+				  mode, type, 1);
 
   /* Found out how many registers we need to save.  */
   gp_saved = MAX_ARGS_IN_REGISTERS - local_cum.num_gprs;
@@ -866,8 +881,12 @@ loongarch_compute_frame_info (void)
   frame->frame_pointer_offset = offset;
   /* Next are the callee-saved FPRs.  */
   if (frame->fmask)
-    offset += LARCH_STACK_ALIGN (num_f_saved * UNITS_PER_FP_REG);
-  frame->fp_sp_offset = offset - UNITS_PER_FP_REG;
+    {
+      offset += LARCH_STACK_ALIGN (num_f_saved * UNITS_PER_FP_REG);
+      frame->fp_sp_offset = offset - UNITS_PER_FP_REG;
+    }
+  else
+    frame->fp_sp_offset = offset;
   /* Next are the callee-saved GPRs.  */
   if (frame->mask)
     {
@@ -880,7 +899,10 @@ loongarch_compute_frame_info (void)
 	frame->save_libcall_adjustment = x_save_size;
 
       offset += x_save_size;
+      frame->gp_sp_offset = offset - UNITS_PER_WORD;
     }
+  else
+    frame->gp_sp_offset = offset;
   frame->gp_sp_offset = offset - UNITS_PER_WORD;
   /* The hard frame pointer points above the callee-saved GPRs.  */
   frame->hard_frame_pointer_offset = offset;
@@ -1041,10 +1063,8 @@ loongarch_first_stack_step (struct loongarch_frame_info *frame)
 static void
 loongarch_emit_stack_tie (void)
 {
-  if (Pmode == SImode)
-    emit_insn (gen_stack_tiesi (stack_pointer_rtx, hard_frame_pointer_rtx));
-  else
-    emit_insn (gen_stack_tiedi (stack_pointer_rtx, hard_frame_pointer_rtx));
+  emit_insn (PMODE_INSN (gen_stack_tie,
+			 (stack_pointer_rtx, hard_frame_pointer_rtx)));
 }
 
 #define PROBE_INTERVAL (1 << STACK_CHECK_PROBE_INTERVAL_EXP)
@@ -1059,103 +1079,70 @@ loongarch_emit_stack_tie (void)
 static void
 loongarch_emit_probe_stack_range (HOST_WIDE_INT first, HOST_WIDE_INT size)
 {
-  /* See if we have a constant small number of probes to generate.  If so,
-     that's the easy case.  */
-  if ((TARGET_64BIT && (first + size <= 32768))
-      || (!TARGET_64BIT && (first + size <= 2048)))
-    {
-      HOST_WIDE_INT i;
+  HOST_WIDE_INT rounded_size;
+  rtx r12 = LARCH_PROLOGUE_TEMP2 (Pmode);
+  rtx r14 = LARCH_PROLOGUE_TEMP3 (Pmode);
 
-      /* Probe at FIRST + N * PROBE_INTERVAL for values of N from 1 until
-	 it exceeds SIZE.  If only one probe is needed, this will not
-	 generate any code.  Then probe at FIRST + SIZE.  */
-      for (i = PROBE_INTERVAL; i < size; i += PROBE_INTERVAL)
-	emit_stack_probe (plus_constant (Pmode, stack_pointer_rtx,
-					 -(first + i)));
+  size = size + first;
+  /* Sanity check for the addressing mode we're going to use.  */
+  gcc_assert (first <= 16384);
 
-      emit_stack_probe (plus_constant (Pmode, stack_pointer_rtx,
-				       -(first + size)));
-    }
-
-  /* Otherwise, do the same as above, but in a loop.  Note that we must be
-     extra careful with variables wrapping around because we might be at
-     the very top (or the very bottom) of the address space and we have
-     to be able to handle this case properly; in particular, we use an
-     equality test for the loop condition.  */
-  else
-    {
-      HOST_WIDE_INT rounded_size;
-      rtx r13 = LARCH_PROLOGUE_TEMP (Pmode);
-      rtx r12 = LARCH_PROLOGUE_TEMP2 (Pmode);
-      rtx r14 = LARCH_PROLOGUE_TEMP3 (Pmode);
+  /* Step 1: round SIZE to the previous multiple of the interval.  */
 
-      /* Sanity check for the addressing mode we're going to use.  */
-      gcc_assert (first <= 16384);
+  rounded_size = ROUND_DOWN (size, PROBE_INTERVAL);
 
+  /* Step 2: compute initial and final value of the loop counter.  */
 
-      /* Step 1: round SIZE to the previous multiple of the interval.  */
-
-      rounded_size = ROUND_DOWN (size, PROBE_INTERVAL);
-
-      /* TEST_ADDR = SP + FIRST */
-      if (first != 0)
-	{
-	  emit_move_insn (r14, GEN_INT (first));
-	  emit_insn (gen_rtx_SET (r13, gen_rtx_MINUS (Pmode,
-						      stack_pointer_rtx,
-						      r14)));
-	}
-      else
-	emit_move_insn (r13, stack_pointer_rtx);
-
-      /* Step 2: compute initial and final value of the loop counter.  */
+  emit_move_insn (r14, GEN_INT (PROBE_INTERVAL));
+  /* LAST_ADDR = SP + FIRST + ROUNDED_SIZE.  */
+  if (rounded_size != 0)
+    {
+      emit_move_insn (r12, GEN_INT (rounded_size));
+      emit_insn (gen_rtx_SET (r12, gen_rtx_MINUS (Pmode,
+						  stack_pointer_rtx, r12)));
 
-      emit_move_insn (r14, GEN_INT (PROBE_INTERVAL));
-      /* LAST_ADDR = SP + FIRST + ROUNDED_SIZE.  */
-      if (rounded_size == 0)
-	emit_move_insn (r12, r13);
-      else
-	{
-	  emit_move_insn (r12, GEN_INT (rounded_size));
-	  emit_insn (gen_rtx_SET (r12, gen_rtx_MINUS (Pmode, r13, r12)));
-	  /* Step 3: the loop
+      /* Step 3: the loop
 
-	     do
-	     {
-	     TEST_ADDR = TEST_ADDR + PROBE_INTERVAL
-	     probe at TEST_ADDR
-	     }
-	     while (TEST_ADDR != LAST_ADDR)
+	 do
+	 {
+	 TEST_ADDR = TEST_ADDR + PROBE_INTERVAL
+	 probe at TEST_ADDR
+	 }
+	 while (TEST_ADDR != LAST_ADDR)
 
-	     probes at FIRST + N * PROBE_INTERVAL for values of N from 1
-	     until it is equal to ROUNDED_SIZE.  */
+	 probes at FIRST + N * PROBE_INTERVAL for values of N from 1
+	 until it is equal to ROUNDED_SIZE.  */
 
-	  emit_insn (PMODE_INSN (gen_probe_stack_range, (r13, r13, r12, r14)));
-	}
+      emit_insn (PMODE_INSN (gen_probe_stack_range, (stack_pointer_rtx,
+						     stack_pointer_rtx, r12, r14)));
+    }
 
-      /* Step 4: probe at FIRST + SIZE if we cannot assert at compile-time
-	 that SIZE is equal to ROUNDED_SIZE.  */
+  /* Step 4: probe at FIRST + SIZE if we cannot assert at compile-time
+     that SIZE is equal to ROUNDED_SIZE.  */
 
-      if (size != rounded_size)
+  if (size != rounded_size)
+    {
+      if (size - rounded_size >= PROBE_INTERVAL/2)
 	{
-	  if (TARGET_64BIT)
-	    emit_stack_probe (plus_constant (Pmode, r12, rounded_size - size));
-	  else
-	    {
-	      HOST_WIDE_INT i;
-	      for (i = 2048; i < (size - rounded_size); i += 2048)
-		{
-		  emit_stack_probe (plus_constant (Pmode, r12, -i));
-		  emit_insn (gen_rtx_SET (r12,
-					  plus_constant (Pmode, r12, -2048)));
-		}
-	      rtx r1 = plus_constant (Pmode, r12,
-				      -(size - rounded_size - i + 2048));
-	      emit_stack_probe (r1);
-	    }
+	  emit_move_insn (r14, GEN_INT (size - rounded_size));
+	  emit_insn (gen_rtx_SET (stack_pointer_rtx, gen_rtx_MINUS (Pmode,
+								    stack_pointer_rtx,
+								    r14)));
 	}
+      else
+	emit_insn (gen_rtx_SET (stack_pointer_rtx, gen_rtx_PLUS (Pmode,
+								 stack_pointer_rtx,
+								 GEN_INT (rounded_size - size))));
+
     }
 
+  if (first)
+    {
+      emit_move_insn (r12, GEN_INT (first));
+      emit_insn (gen_rtx_SET (stack_pointer_rtx, gen_rtx_PLUS (Pmode,
+							       stack_pointer_rtx, r12)));
+    }
+ 
   /* Make sure nothing is scheduled before we are done.  */
   emit_insn (gen_blockage ());
 }
@@ -1202,29 +1189,11 @@ loongarch_expand_prologue (void)
 {
   struct loongarch_frame_info *frame = &cfun->machine->frame;
   HOST_WIDE_INT size = frame->total_size;
-  HOST_WIDE_INT tmp;
-  unsigned mask = frame->mask;
   rtx insn;
 
   if (flag_stack_usage_info)
     current_function_static_stack_size = size;
 
-  if (flag_stack_check == STATIC_BUILTIN_STACK_CHECK
-      || flag_stack_clash_protection)
-    {
-      if (crtl->is_leaf && !cfun->calls_alloca)
-	{
-	  if (size > PROBE_INTERVAL && size > get_stack_check_protect ())
-	    {
-	      tmp = size - get_stack_check_protect ();
-	      loongarch_emit_probe_stack_range (get_stack_check_protect (),
-						tmp);
-	    }
-	}
-      else if (size > 0)
-	loongarch_emit_probe_stack_range (get_stack_check_protect (), size);
-    }
-
   /* Save the registers.  */
   if ((frame->mask | frame->fmask) != 0)
     {
@@ -1237,8 +1206,6 @@ loongarch_expand_prologue (void)
       loongarch_for_each_saved_reg (size, loongarch_save_reg);
     }
 
-  frame->mask = mask; /* Undo the above fib.  */
-
   /* Set up the frame pointer, if we're using one.  */
   if (frame_pointer_needed)
     {
@@ -1250,24 +1217,39 @@ loongarch_expand_prologue (void)
     }
 
   /* Allocate the rest of the frame.  */
-  if (size > 0)
+  if ((flag_stack_check == STATIC_BUILTIN_STACK_CHECK
+       || flag_stack_clash_protection)
+      && size > 0)
     {
-      if (IMM12_OPERAND (-size))
-	{
-	  insn = gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
-				GEN_INT (-size));
-	  RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
-	}
-      else
-	{
-	  loongarch_emit_move (LARCH_PROLOGUE_TEMP (Pmode), GEN_INT (-size));
-	  emit_insn (gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
-				    LARCH_PROLOGUE_TEMP (Pmode)));
+      loongarch_emit_probe_stack_range (get_stack_check_protect (), size);
 
-	  /* Describe the effect of the previous instructions.  */
-	  insn = plus_constant (Pmode, stack_pointer_rtx, -size);
-	  insn = gen_rtx_SET (stack_pointer_rtx, insn);
-	  loongarch_set_frame_expr (insn);
+      /* Describe the effect of the previous instructions.  */
+      insn = plus_constant (Pmode, stack_pointer_rtx, -size);
+      insn = gen_rtx_SET (stack_pointer_rtx, insn);
+      loongarch_set_frame_expr (insn);
+    }
+  else
+    {
+      /* Allocate the rest of the frame.  */
+      if (size > 0)
+	{
+	  if (IMM12_OPERAND (-size))
+	    {
+	      insn = gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
+				    GEN_INT (-size));
+	      RTX_FRAME_RELATED_P (emit_insn (insn)) = 1;
+	    }
+	  else
+	    {
+	      loongarch_emit_move (LARCH_PROLOGUE_TEMP (Pmode), GEN_INT (-size));
+	      emit_insn (gen_add3_insn (stack_pointer_rtx, stack_pointer_rtx,
+					LARCH_PROLOGUE_TEMP (Pmode)));
+
+	      /* Describe the effect of the previous instructions.  */
+	      insn = plus_constant (Pmode, stack_pointer_rtx, -size);
+	      insn = gen_rtx_SET (stack_pointer_rtx, insn);
+	      loongarch_set_frame_expr (insn);
+	    }
 	}
     }
 }
@@ -1328,8 +1310,7 @@ loongarch_expand_epilogue (bool sibcall_p)
 				       adjust));
 
       rtx dwarf = NULL_RTX;
-      rtx minus_offset = NULL_RTX;
-      minus_offset = GEN_INT (-frame->hard_frame_pointer_offset);
+      rtx minus_offset = GEN_INT (-frame->hard_frame_pointer_offset);
       rtx cfa_adjust_value = gen_rtx_PLUS (Pmode,
 					   hard_frame_pointer_rtx,
 					   minus_offset);
@@ -1423,7 +1404,7 @@ loongarch_build_integer (struct loongarch_integer_op *codes,
   unsigned int cost = 0;
 
   /* Get the lower 32 bits of the value.  */
-  HOST_WIDE_INT low_part = TARGET_64BIT ? value << 32 >> 32 : value;
+  HOST_WIDE_INT low_part = (int32_t)value;
 
   if (IMM12_OPERAND (low_part) || IMM12_OPERAND_UNSIGNED (low_part))
     {
@@ -1457,6 +1438,7 @@ loongarch_build_integer (struct loongarch_integer_op *codes,
       bool lu52i[2] = {(value & LU52I_B) == 0, (value & LU52I_B) == LU52I_B};
 
       int sign31 = (value & (1UL << 31)) >> 31;
+      int sign51 = (value & (1UL << 51)) >> 51;
       /* Determine whether the upper 32 bits are sign-extended from the lower
 	 32 bits. If it is, the instructions to load the high order can be
 	 ommitted.  */
@@ -1467,12 +1449,12 @@ loongarch_build_integer (struct loongarch_integer_op *codes,
       else if (lu32i[sign31])
 	{
 	  codes[cost].method = METHOD_LU52I;
-	  codes[cost].value = (value >> 52) << 52;
+	  codes[cost].value = value & LU52I_B;
 	  return cost + 1;
 	}
 
       codes[cost].method = METHOD_LU32I;
-      codes[cost].value = ((value << 12) >> 44) << 32;
+      codes[cost].value = (value & LU32I_B) | (sign51 ? LU52I_B : 0);
       cost++;
 
       /* Determine whether the 52-61 bits are sign-extended from the low order,
@@ -1480,7 +1462,7 @@ loongarch_build_integer (struct loongarch_integer_op *codes,
       if (!lu52i[(value & (1ULL << 51)) >> 51])
 	{
 	  codes[cost].method = METHOD_LU52I;
-	  codes[cost].value = (value >> 52) << 52;
+	  codes[cost].value = value & LU52I_B;
 	  cost++;
 	}
     }
@@ -1514,7 +1496,7 @@ loongarch_legitimate_constant_p (machine_mode mode ATTRIBUTE_UNUSED, rtx x)
 static bool
 loongarch_tls_symbol_p (rtx x)
 {
-  return GET_CODE (x) == SYMBOL_REF && SYMBOL_REF_TLS_MODEL (x) != 0;
+  return SYMBOL_REF_P (x) && SYMBOL_REF_TLS_MODEL (x) != 0;
 }
 
 /* Return true if SYMBOL_REF X is associated with a global symbol
@@ -1531,9 +1513,6 @@ loongarch_global_symbol_p (const_rtx x)
   if (!decl)
     return !SYMBOL_REF_LOCAL_P (x) || SYMBOL_REF_EXTERNAL_P (x);
 
-  /* Weakref symbols are not TREE_PUBLIC, but their targets are global
-     or weak symbols.  Relocations in the object file will be against
-     the target symbol, so it's that symbol's binding that matters here.  */
   return DECL_P (decl) && (TREE_PUBLIC (decl) || DECL_WEAK (decl));
 }
 
@@ -1548,9 +1527,6 @@ loongarch_global_symbol_noweak_p (const_rtx x)
   if (!decl)
     return !SYMBOL_REF_LOCAL_P (x) || SYMBOL_REF_EXTERNAL_P (x);
 
-  /* Weakref symbols are not TREE_PUBLIC, but their targets are global
-     or weak symbols.  Relocations in the object file will be against
-     the target symbol, so it's that symbol's binding that matters here.  */
   return DECL_P (decl) && TREE_PUBLIC (decl);
 }
 
@@ -1719,35 +1695,30 @@ loongarch_rtx_constant_in_small_data_p (machine_mode mode)
 }
 
 /* Return the method that should be used to access SYMBOL_REF or
-   LABEL_REF X in context CONTEXT.  */
+   LABEL_REF X.  */
 
 static enum loongarch_symbol_type
-loongarch_classify_symbol (const_rtx x,
-			   enum loongarch_symbol_context context  \
-			   ATTRIBUTE_UNUSED)
+loongarch_classify_symbol (const_rtx x)
 {
   if (GET_CODE (x) == LABEL_REF)
-    {
-      return SYMBOL_GOT_DISP;
-    }
+    return SYMBOL_GOT_DISP;
 
-  gcc_assert (GET_CODE (x) == SYMBOL_REF);
+  gcc_assert (SYMBOL_REF_P (x));
 
   if (SYMBOL_REF_TLS_MODEL (x))
     return SYMBOL_TLS;
 
-  if (GET_CODE (x) == SYMBOL_REF)
+  if (SYMBOL_REF_P (x))
     return SYMBOL_GOT_DISP;
 
   return SYMBOL_GOT_DISP;
 }
 
-/* Return true if X is a symbolic constant that can be used in context
-   CONTEXT.  If it is, store the type of the symbol in *SYMBOL_TYPE.  */
+/* Return true if X is a symbolic constant.  If it is,
+   store the type of the symbol in *SYMBOL_TYPE.  */
 
 bool
-loongarch_symbolic_constant_p (rtx x, enum loongarch_symbol_context context,
-			       enum loongarch_symbol_type *symbol_type)
+loongarch_symbolic_constant_p (rtx x, enum loongarch_symbol_type *symbol_type)
 {
   rtx offset;
 
@@ -1757,9 +1728,9 @@ loongarch_symbolic_constant_p (rtx x, enum loongarch_symbol_context context,
       *symbol_type = UNSPEC_ADDRESS_TYPE (x);
       x = UNSPEC_ADDRESS (x);
     }
-  else if (GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF)
+  else if (SYMBOL_REF_P (x) || GET_CODE (x) == LABEL_REF)
     {
-      *symbol_type = loongarch_classify_symbol (x, context);
+      *symbol_type = loongarch_classify_symbol (x);
       if (*symbol_type == SYMBOL_TLS)
 	return true;
     }
@@ -1773,8 +1744,6 @@ loongarch_symbolic_constant_p (rtx x, enum loongarch_symbol_context context,
      relocations.  */
   switch (*symbol_type)
     {
-      /* Fall through.  */
-
     case SYMBOL_GOT_DISP:
     case SYMBOL_TLSGD:
     case SYMBOL_TLSLDM:
@@ -1784,11 +1753,25 @@ loongarch_symbolic_constant_p (rtx x, enum loongarch_symbol_context context,
   gcc_unreachable ();
 }
 
-/* Like loongarch_symbol_insns We rely on the fact that, in the worst case.  */
+/* If MODE is MAX_MACHINE_MODE, return the number of instructions needed
+   to load symbols of type TYPE into a register.  Return 0 if the given
+   type of symbol cannot be used as an immediate operand.
+
+   Otherwise, return the number of instructions needed to load or store
+   values of mode MODE to or from addresses of type TYPE.  Return 0 if
+   the given type of symbol is not valid in addresses.  */
 
 static int
-loongarch_symbol_insns_1 (enum loongarch_symbol_type type, machine_mode mode)
+loongarch_symbol_insns (enum loongarch_symbol_type type, machine_mode mode)
 {
+  /* LSX LD.* and ST.* cannot support loading symbols via an immediate
+     operand.  */
+  if (LSX_SUPPORTED_MODE_P (mode))
+    return 0;
+
+  if (LASX_SUPPORTED_MODE_P (mode))
+    return 0;
+
   switch (type)
     {
     case SYMBOL_GOT_DISP:
@@ -1797,8 +1780,6 @@ loongarch_symbol_insns_1 (enum loongarch_symbol_type type, machine_mode mode)
       if (mode != MAX_MACHINE_MODE)
 	return 0;
 
-      /* Fall through.  */
-
       return 3;
 
     case SYMBOL_TLSGD:
@@ -1812,30 +1793,6 @@ loongarch_symbol_insns_1 (enum loongarch_symbol_type type, machine_mode mode)
   gcc_unreachable ();
 }
 
-/* If MODE is MAX_MACHINE_MODE, return the number of instructions needed
-   to load symbols of type TYPE into a register.  Return 0 if the given
-   type of symbol cannot be used as an immediate operand.
-
-   Otherwise, return the number of instructions needed to load or store
-   values of mode MODE to or from addresses of type TYPE.  Return 0 if
-   the given type of symbol is not valid in addresses.
-
-   In both cases, instruction counts are based off BASE_INSN_LENGTH.  */
-
-static int
-loongarch_symbol_insns (enum loongarch_symbol_type type, machine_mode mode)
-{
-  /* LSX LD.* and ST.* cannot support loading symbols via an immediate
-     operand.  */
-  if (LSX_SUPPORTED_MODE_P (mode))
-    return 0;
-
-  if (LASX_SUPPORTED_MODE_P (mode))
-    return 0;
-
-  return loongarch_symbol_insns_1 (type, mode) * (1);
-}
-
 /* Implement TARGET_CANNOT_FORCE_CONST_MEM.  */
 
 static bool
@@ -1857,7 +1814,7 @@ loongarch_cannot_force_const_mem (machine_mode mode, rtx x)
     return true;
 
   split_const (x, &base, &offset);
-  if (loongarch_symbolic_constant_p (base, SYMBOL_CONTEXT_LEA, &type))
+  if (loongarch_symbolic_constant_p (base, &type))
     {
       /* The same optimization as for CONST_INT.  */
       if (IMM12_INT (offset)
@@ -1902,7 +1859,7 @@ loongarch_regno_mode_ok_for_base_p (int regno,
 static bool
 loongarch_valid_base_register_p (rtx x, machine_mode mode, bool strict_p)
 {
-  if (!strict_p && GET_CODE (x) == SUBREG)
+  if (!strict_p && SUBREG_P (x))
     x = SUBREG_REG (x);
 
   return (REG_P (x)
@@ -1916,8 +1873,8 @@ static bool
 loongarch_valid_offset_p (rtx x, machine_mode mode)
 {
   /* Check that X is a signed 12-bit number,
-   * or check that X is a signed 16-bit number
-   * and offset 4 byte aligned.  */
+     or check that X is a signed 16-bit number
+     and offset 4 byte aligned.  */
   if (!(const_arith_operand (x, Pmode)
 	|| ((mode == E_SImode || mode == E_DImode)
 	    && const_imm16_operand (x, Pmode)
@@ -1944,6 +1901,35 @@ loongarch_valid_offset_p (rtx x, machine_mode mode)
   return true;
 }
 
+static bool
+loongarch_valid_index_p (struct loongarch_address_info *info, rtx x,
+			 machine_mode mode, bool strict_p)
+{
+  rtx index;
+
+  if ((REG_P (x) || SUBREG_P (x))
+      && GET_MODE (x) == Pmode)
+    {
+      index = x;
+    }
+  else
+    return false;
+
+  if (!strict_p
+      && SUBREG_P (index)
+      && contains_reg_of_mode[GENERAL_REGS][GET_MODE (SUBREG_REG (index))])
+    index = SUBREG_REG (index);
+
+  if (loongarch_valid_base_register_p (index, mode, strict_p))
+    {
+      info->type = ADDRESS_REG_REG;
+      info->offset = index;
+      return true;
+    }
+
+  return false;
+}
+
 /* Return true if X is a valid address for machine mode MODE.  If it is,
    fill in INFO appropriately.  STRICT_P is true if REG_OK_STRICT is in
    effect.  */
@@ -1962,12 +1948,26 @@ loongarch_classify_address (struct loongarch_address_info *info, rtx x,
       return loongarch_valid_base_register_p (info->reg, mode, strict_p);
 
     case PLUS:
+/*
+      if (loongarch_valid_base_register_p (XEXP (x, 0), mode, strict_p)
+	  && loongarch_valid_index_p (info, XEXP (x, 1), mode, strict_p))
+	{
+	  info->reg = XEXP (x, 0);
+	  return true;
+	}
+
+      if (loongarch_valid_base_register_p (XEXP (x, 1), mode, strict_p)
+	  && loongarch_valid_index_p (info, XEXP (x, 0), mode, strict_p))
+	{
+	  info->reg = XEXP (x, 1);
+	  return true;
+	}
+*/
       info->type = ADDRESS_REG;
       info->reg = XEXP (x, 0);
       info->offset = XEXP (x, 1);
       return (loongarch_valid_base_register_p (info->reg, mode, strict_p)
 	      && loongarch_valid_offset_p (info->offset, mode));
-
     default:
       return false;
     }
@@ -1983,38 +1983,21 @@ loongarch_legitimate_address_p (machine_mode mode, rtx x, bool strict_p)
   return loongarch_classify_address (&addr, x, mode, strict_p);
 }
 
-/* Return true if X is a legitimate $sp-based address for mode MODE.  */
-
-bool
-loongarch_stack_address_p (rtx x, machine_mode mode)
-{
-  struct loongarch_address_info addr;
-
-  return (loongarch_classify_address (&addr, x, mode, false)
-	  && addr.type == ADDRESS_REG
-	  && addr.reg == stack_pointer_rtx);
-}
-
-/* Return true if ADDR matches the pattern for the L{B,H,W,D}{,U}X load
-   indexed address instruction.  Note that such addresses are
-   not considered legitimate in the TARGET_LEGITIMATE_ADDRESS_P
-   sense, because their use is so restricted.  */
+/* Return true if ADDR matches the pattern for the indexed address
+   instruction.  */
 
 static bool
-loongarch_lx_address_p (rtx addr, machine_mode mode ATTRIBUTE_UNUSED)
+loongarch_index_address_p (rtx addr, machine_mode mode ATTRIBUTE_UNUSED)
 {
   if (GET_CODE (addr) != PLUS
       || !REG_P (XEXP (addr, 0))
       || !REG_P (XEXP (addr, 1)))
     return false;
-  if (LSX_SUPPORTED_MODE_P (mode))
-    return true;
-  return false;
+  return true;
 }
 
 /* Return the number of instructions needed to load or store a value
-   of mode MODE at address X, assuming that BASE_INSN_LENGTH is the
-   length of one instruction.  Return 0 if X isn't valid for MODE.
+   of mode MODE at address X.  Return 0 if X isn't valid for MODE.
    Assume that multiword moves may need to be split into word moves
    if MIGHT_SPLIT_P, otherwise assume that a single load or store is
    enough.  */
@@ -2024,7 +2007,7 @@ loongarch_address_insns (rtx x, machine_mode mode, bool might_split_p)
 {
   struct loongarch_address_info addr;
   int factor;
-  bool lsx_p = (!might_split_p && 
+  bool lsx_p = (!might_split_p &&
 		(LSX_SUPPORTED_MODE_P (mode) || LASX_SUPPORTED_MODE_P (mode)));
 
   if (!loongarch_classify_address (&addr, x, mode, false))
@@ -2054,6 +2037,9 @@ loongarch_address_insns (rtx x, machine_mode mode, bool might_split_p)
 	  }
 	return factor;
 
+      case ADDRESS_REG_REG:
+	return lsx_p ? 0 : factor;
+
       case ADDRESS_CONST_INT:
 	return lsx_p ? 0 : factor;
 
@@ -2126,8 +2112,17 @@ loongarch_14bit_shifted_offset_address_p (rtx x, machine_mode mode)
 	  && LARCH_SHIFT_2_OFFSET_P (INTVAL (addr.offset)));
 }
 
+bool
+loongarch_base_index_address_p (rtx x, machine_mode mode)
+{
+  struct loongarch_address_info addr;
+
+  return (loongarch_classify_address (&addr, x, mode, false)
+	  && addr.type == ADDRESS_REG_REG
+	  && REG_P (addr.offset));
+}
+
 /* Return the number of instructions needed to load constant X,
-   assuming that BASE_INSN_LENGTH is the length of one instruction.
    Return 0 if X isn't a valid constant.  */
 
 int
@@ -2147,12 +2142,11 @@ loongarch_const_insns (rtx x)
 	return 1;
       /* Fall through.  */
     case CONST_DOUBLE:
-      /* Allow zeros for normal mode, where we can use $0.  */
       return x == CONST0_RTX (GET_MODE (x)) ? 1 : 0;
 
     case CONST:
       /* See if we can refer to X directly.  */
-      if (loongarch_symbolic_constant_p (x, SYMBOL_CONTEXT_LEA, &symbol_type))
+      if (loongarch_symbolic_constant_p (x, &symbol_type))
 	return loongarch_symbol_insns (symbol_type, MAX_MACHINE_MODE);
 
       /* Otherwise try splitting the constant into a base and offset.
@@ -2180,7 +2174,7 @@ loongarch_const_insns (rtx x)
     case SYMBOL_REF:
     case LABEL_REF:
       return loongarch_symbol_insns (
-	loongarch_classify_symbol (x, SYMBOL_CONTEXT_LEA), MAX_MACHINE_MODE);
+	loongarch_classify_symbol (x), MAX_MACHINE_MODE);
 
     default:
       return 0;
@@ -2189,8 +2183,7 @@ loongarch_const_insns (rtx x)
 
 /* X is a doubleword constant that can be handled by splitting it into
    two words and loading each word separately.  Return the number of
-   instructions required to do this, assuming that BASE_INSN_LENGTH
-   is the length of one instruction.  */
+   instructions required to do this.  */
 
 int
 loongarch_split_const_insns (rtx x)
@@ -2224,8 +2217,7 @@ loongarch_subword_at_byte (rtx op, unsigned int byte)
 }
 
 /* Return the number of instructions needed to implement INSN,
-   given that it loads from or stores to MEM.  Assume that
-   BASE_INSN_LENGTH is the length of one instruction.  */
+   given that it loads from or stores to MEM.  */
 
 int
 loongarch_load_store_insns (rtx mem, rtx_insn *insn)
@@ -2243,16 +2235,14 @@ loongarch_load_store_insns (rtx mem, rtx_insn *insn)
     {
       set = single_set (insn);
       if (set
-	  && !loongarch_split_move_insn_p (SET_DEST (set), SET_SRC (set),
-					   insn))
+	  && !loongarch_split_move_insn_p (SET_DEST (set), SET_SRC (set)))
 	might_split_p = false;
     }
 
   return loongarch_address_insns (XEXP (mem, 0), mode, might_split_p);
 }
 
-/* Return the number of instructions needed for an integer division,
-   assuming that BASE_INSN_LENGTH is the length of one instruction.  */
+/* Return the number of instructions needed for an integer division.  */
 
 int
 loongarch_idiv_insns (machine_mode mode ATTRIBUTE_UNUSED)
@@ -2374,10 +2364,7 @@ static GTY (()) rtx loongarch_tls_symbol;
 static rtx
 loongarch_got_load_tls_gd (rtx dest, rtx sym)
 {
-  if (Pmode == DImode)
-    return gen_got_load_tls_gddi (dest, sym);
-  else
-    return gen_got_load_tls_gdsi (dest, sym);
+  return PMODE_INSN (gen_got_load_tls_gd, (dest, sym));
 }
 
 /* Load an entry from the GOT for a TLS LD access.  */
@@ -2385,10 +2372,7 @@ loongarch_got_load_tls_gd (rtx dest, rtx sym)
 static rtx
 loongarch_got_load_tls_ld (rtx dest, rtx sym)
 {
-  if (Pmode == DImode)
-    return gen_got_load_tls_lddi (dest, sym);
-  else
-    return gen_got_load_tls_ldsi (dest, sym);
+  return PMODE_INSN (gen_got_load_tls_ld, (dest, sym));
 }
 
 /* Load an entry from the GOT for a TLS IE access.  */
@@ -2396,10 +2380,7 @@ loongarch_got_load_tls_ld (rtx dest, rtx sym)
 static rtx
 loongarch_got_load_tls_ie (rtx dest, rtx sym)
 {
-  if (Pmode == DImode)
-    return gen_got_load_tls_iedi (dest, sym);
-  else
-    return gen_got_load_tls_iesi (dest, sym);
+  return PMODE_INSN (gen_got_load_tls_ie, (dest, sym));
 }
 
 /* Add in the thread pointer for a TLS LE access.  */
@@ -2407,10 +2388,7 @@ loongarch_got_load_tls_ie (rtx dest, rtx sym)
 static rtx
 loongarch_got_load_tls_le (rtx dest, rtx sym)
 {
-  if (Pmode == DImode)
-    return gen_got_load_tls_ledi (dest, sym);
-  else
-    return gen_got_load_tls_lesi (dest, sym);
+  return PMODE_INSN (gen_got_load_tls_le, (dest, sym));
 }
 
 /* Return an instruction sequence that calls __tls_get_addr.  SYM is
@@ -2685,7 +2663,7 @@ loongarch_legitimize_move (machine_mode mode, rtx dest, rtx src)
 
   /* Both src and dest are non-registers;  one special case is supported where
      the source is (const_int 0) and the store can source the zero register.
-     LSX and lasx are never able to source the zero register directly in 
+     LSX and LASX are never able to source the zero register directly in
      memory operations.  */
   if (!register_operand (dest, mode) && !register_operand (src, mode)
       && (!const_0_operand (src, mode)
@@ -2703,16 +2681,14 @@ loongarch_legitimize_move (machine_mode mode, rtx dest, rtx src)
       set_unique_reg_note (get_last_insn (), REG_EQUAL, copy_rtx (src));
       return true;
     }
+
   return false;
 }
 
-/* Return true if OP refers to small data symbols directly, not through
-   a LO_SUM.  CONTEXT is the context in which X appears.  */
+/* Return true if OP refers to small data symbols directly.  */
 
 static int
-loongarch_small_data_pattern_1 (rtx x,
-				enum loongarch_symbol_context context \
-				ATTRIBUTE_UNUSED)
+loongarch_small_data_pattern_1 (rtx x)
 {
   subrtx_var_iterator::array_type array;
   FOR_EACH_SUBRTX_VAR (iter, array, x, ALL)
@@ -2725,7 +2701,7 @@ loongarch_small_data_pattern_1 (rtx x,
 	iter.skip_subrtxes ();
       else if (MEM_P (x))
 	{
-	  if (loongarch_small_data_pattern_1 (XEXP (x, 0), SYMBOL_CONTEXT_MEM))
+	  if (loongarch_small_data_pattern_1 (XEXP (x, 0)))
 	    return true;
 	  iter.skip_subrtxes ();
 	}
@@ -2733,22 +2709,19 @@ loongarch_small_data_pattern_1 (rtx x,
   return false;
 }
 
-/* Return true if OP refers to small data symbols directly, not through
-   a LO_SUM.  */
+/* Return true if OP refers to small data symbols directly.  */
 
 bool
 loongarch_small_data_pattern_p (rtx op)
 {
-  return loongarch_small_data_pattern_1 (op, SYMBOL_CONTEXT_LEA);
+  return loongarch_small_data_pattern_1 (op);
 }
 
 /* Rewrite *LOC so that it refers to small data using explicit
-   relocations.  CONTEXT is the context in which *LOC appears.  */
+   relocation.  */
 
 static void
-loongarch_rewrite_small_data_1 (rtx *loc,
-				enum loongarch_symbol_context context \
-				ATTRIBUTE_UNUSED)
+loongarch_rewrite_small_data_1 (rtx *loc)
 {
   subrtx_ptr_iterator::array_type array;
   FOR_EACH_SUBRTX_PTR (iter, array, loc, ALL)
@@ -2756,7 +2729,7 @@ loongarch_rewrite_small_data_1 (rtx *loc,
       rtx *loc = *iter;
       if (MEM_P (*loc))
 	{
-	  loongarch_rewrite_small_data_1 (&XEXP (*loc, 0), SYMBOL_CONTEXT_MEM);
+	  loongarch_rewrite_small_data_1 (&XEXP (*loc, 0));
 	  iter.skip_subrtxes ();
 	}
     }
@@ -2769,7 +2742,7 @@ rtx
 loongarch_rewrite_small_data (rtx pattern)
 {
   pattern = copy_insn (pattern);
-  loongarch_rewrite_small_data_1 (&pattern, SYMBOL_CONTEXT_LEA);
+  loongarch_rewrite_small_data_1 (&pattern);
   return pattern;
 }
 
@@ -2793,7 +2766,6 @@ loongarch_immediate_operand_p (int code, HOST_WIDE_INT x)
 
     case ROTATE:
     case ROTATERT:
-      /* Likewise rotates, if the target supports rotates at all.  */
       return true;
 
     case AND:
@@ -2881,16 +2853,12 @@ loongarch_fp_div_cost (machine_mode mode)
    cost of OP itself.  */
 
 static int
-loongarch_sign_extend_cost (machine_mode mode, rtx op)
+loongarch_sign_extend_cost (rtx op)
 {
   if (MEM_P (op))
     /* Extended loads are as cheap as unextended ones.  */
     return 0;
 
-  if (TARGET_64BIT && mode == DImode && GET_MODE (op) == SImode)
-    /* A sign extension from SImode to DImode in 64-bit mode is free.  */
-    return 0;
-
   return COSTS_N_INSNS (1);
 }
 
@@ -2898,16 +2866,12 @@ loongarch_sign_extend_cost (machine_mode mode, rtx op)
    cost of OP itself.  */
 
 static int
-loongarch_zero_extend_cost (machine_mode mode, rtx op)
+loongarch_zero_extend_cost (rtx op)
 {
   if (MEM_P (op))
     /* Extended loads are as cheap as unextended ones.  */
     return 0;
 
-  if (TARGET_64BIT && mode == DImode && GET_MODE (op) == SImode)
-    /* We need a shift left by 32 bits and a shift right by 32 bits.  */
-    return COSTS_N_INSNS (2);
-
   /* We can use ANDI.  */
   return COSTS_N_INSNS (1);
 }
@@ -3005,16 +2969,16 @@ loongarch_rtx_costs (rtx x, machine_mode mode, int outer_code,
       /* If the address is legitimate, return the number of
 	 instructions it needs.  */
       addr = XEXP (x, 0);
-      cost = loongarch_address_insns (addr, mode, true);
-      if (cost > 0)
+      /* Check for a scaled indexed address.  */
+      if (loongarch_index_address_p (addr, mode))
 	{
-	  *total = COSTS_N_INSNS (cost + 1);
+	  *total = COSTS_N_INSNS (2);
 	  return true;
 	}
-      /* Check for a scaled indexed address.  */
-      if (loongarch_lx_address_p (addr, mode))
+      cost = loongarch_address_insns (addr, mode, true);
+      if (cost > 0)
 	{
-	  *total = COSTS_N_INSNS (2);
+	  *total = COSTS_N_INSNS (cost + 1);
 	  return true;
 	}
       /* Otherwise use the default handling.  */
@@ -3034,7 +2998,7 @@ loongarch_rtx_costs (rtx x, machine_mode mode, int outer_code,
       if (TARGET_64BIT && mode == DImode && CONST_INT_P (XEXP (x, 1))
 	  && UINTVAL (XEXP (x, 1)) == 0xffffffff)
 	{
-	  *total = (loongarch_zero_extend_cost (mode, XEXP (x, 0))
+	  *total = (loongarch_zero_extend_cost (XEXP (x, 0))
 		    + set_src_cost (XEXP (x, 0), mode, speed));
 	  return true;
 	}
@@ -3200,11 +3164,11 @@ loongarch_rtx_costs (rtx x, machine_mode mode, int outer_code,
       return false;
 
     case SIGN_EXTEND:
-      *total = loongarch_sign_extend_cost (mode, XEXP (x, 0));
+      *total = loongarch_sign_extend_cost (XEXP (x, 0));
       return false;
 
     case ZERO_EXTEND:
-      *total = loongarch_zero_extend_cost (mode, XEXP (x, 0));
+      *total = loongarch_zero_extend_cost (XEXP (x, 0));
       return false;
     case TRUNCATE:
       /* Costings for highpart multiplies.  Matching patterns of the form:
@@ -3281,46 +3245,144 @@ loongarch_rtx_costs (rtx x, machine_mode mode, int outer_code,
 
 static int
 loongarch_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,
-                                    tree vectype,
-                                    int misalign ATTRIBUTE_UNUSED)
+				      tree vectype,
+				      int misalign ATTRIBUTE_UNUSED)
 {
-  unsigned elements;
-
+  int elements;
   switch (type_of_cost)
     {
-      case scalar_stmt:
-      case scalar_load:
-      case vector_stmt:
-      case vector_load:
-      case vec_to_scalar:
-      case scalar_to_vec:
-      case cond_branch_not_taken:
-      case vec_perm:
-      case vec_promote_demote:
-      case scalar_store:
-      case vector_store:
-	return 1;
+    case scalar_stmt:
+    case vector_stmt:
+    case vec_to_scalar:
+    case scalar_to_vec:
+    case vec_perm:
+    case vec_promote_demote:
+      return 1;
 
-      case unaligned_load:
-      case vector_gather_load:
-	return 2;
+    case scalar_store:
+    case scalar_load:
+      return 3;
 
-      case unaligned_store:
-      case vector_scatter_store:
-	return 10;
+    case vector_store:
+    case vector_load:
+      return loongarch_vector_access_cost;
 
-      case cond_branch_taken:
-	return 3;
+    case unaligned_load:
+    case unaligned_store:
+    case vector_gather_load:
+    case vector_scatter_store:
+      return 5;
 
-      case vec_construct:
-	elements = TYPE_VECTOR_SUBPARTS (vectype);
-	return elements / 2 + 1;
+    case cond_branch_taken:
+      return 4;
 
-      default:
-	gcc_unreachable ();
+    case cond_branch_not_taken:
+      return 2;
+
+    case vec_construct:
+	{
+	  elements = TYPE_VECTOR_SUBPARTS (vectype);
+	  if (ISA_HAS_LASX)
+	    return elements + 1;
+	  else
+	    return elements;
+	}
+
+    default:
+      gcc_unreachable ();
     }
 }
 
+/* Implement targetm.vectorize.add_stmt_cost.  */
+static unsigned
+loongarch_add_stmt_cost (void *data, int count, enum vect_cost_for_stmt kind,
+			 struct _stmt_vec_info *stmt_info, int misalign,
+			 enum vect_cost_model_location where)
+{
+  unsigned *cost = (unsigned *) data;
+  unsigned retval = 0;
+
+  tree vectype = stmt_info ? stmt_vectype (stmt_info) : NULL_TREE;
+  int stmt_cost = - 1;
+
+  if ((kind == vector_stmt || kind == scalar_stmt)
+      && stmt_info
+      && stmt_info->stmt && gimple_code (stmt_info->stmt) == GIMPLE_ASSIGN)
+    {
+      tree_code subcode = gimple_assign_rhs_code (stmt_info->stmt);
+      bool fp = false;
+      machine_mode mode = TImode;
+
+      if (vectype != NULL)
+	{
+	  fp = FLOAT_TYPE_P (vectype);
+	  mode = TYPE_MODE (vectype);
+	}
+      machine_mode inner_mode = mode;
+      if (VECTOR_MODE_P (mode))
+	inner_mode = GET_MODE_INNER (mode);
+
+      switch (subcode)
+	{
+	case PLUS_EXPR:
+	case POINTER_PLUS_EXPR:
+	case MINUS_EXPR:
+	case MULT_EXPR:
+	case WIDEN_MULT_EXPR:
+	case MULT_HIGHPART_EXPR:
+	  stmt_cost = fp ? 2 : 1;
+	  break;
+
+	case TRUNC_DIV_EXPR:
+	case CEIL_DIV_EXPR:
+	case FLOOR_DIV_EXPR:
+	case ROUND_DIV_EXPR:
+	case TRUNC_MOD_EXPR:
+	case CEIL_MOD_EXPR:
+	case FLOOR_MOD_EXPR:
+	case RDIV_EXPR:
+	case ROUND_MOD_EXPR:
+	case EXACT_DIV_EXPR:
+	  stmt_cost = fp ? 4 : 1;
+	  break;
+
+	case NOP_EXPR:
+	  /* Only sign-conversions are free.  */
+	  if (tree_nop_conversion_p
+	      (TREE_TYPE (gimple_assign_lhs (stmt_info->stmt)),
+	       TREE_TYPE (gimple_assign_rhs1 (stmt_info->stmt))))
+	    stmt_cost = 0;
+	  break;
+
+	default:
+	  break;
+	}
+    }
+  if (kind == vec_construct
+      && stmt_info
+      && (STMT_VINFO_TYPE (stmt_info) == load_vec_info_type
+	  || STMT_VINFO_TYPE (stmt_info) == store_vec_info_type)
+      && STMT_VINFO_MEMORY_ACCESS_TYPE (stmt_info) == VMAT_ELEMENTWISE
+      && TREE_CODE (DR_STEP (STMT_VINFO_DATA_REF (stmt_info))) != INTEGER_CST)
+    {
+      stmt_cost = loongarch_builtin_vectorization_cost (kind, vectype, misalign);
+      stmt_cost *= TYPE_VECTOR_SUBPARTS (vectype);
+    }
+  if (stmt_cost == -1)
+    stmt_cost = loongarch_builtin_vectorization_cost (kind, vectype, misalign);
+
+  /* Statements in an inner loop relative to the loop being
+     vectorized are weighted more heavily.  The value here is
+     arbitrary and could potentially be improved with analysis.  */
+  if (where == vect_body && stmt_info && stmt_in_inner_loop_p (stmt_info))
+    count *= 50;  /* FIXME.  */
+
+  retval = (unsigned) (count * stmt_cost);
+
+  cost[where] += retval;
+
+  return retval;
+}
 
 /* Implement TARGET_ADDRESS_COST.  */
 
@@ -3339,24 +3401,16 @@ loongarch_address_cost (rtx addr, machine_mode mode,
 rtx
 loongarch_subword (rtx op, bool high_p)
 {
-  unsigned int byte, offset;
+  unsigned int byte;
   machine_mode mode;
 
+  byte = high_p ? UNITS_PER_WORD : 0;
   mode = GET_MODE (op);
   if (mode == VOIDmode)
     mode = TARGET_64BIT ? TImode : DImode;
 
-  if (high_p)
-    byte = UNITS_PER_WORD;
-  else
-    byte = 0;
-
   if (FP_REG_RTX_P (op))
-    {
-      /* Paired FPRs are always ordered little-endian.  */
-      offset = (UNITS_PER_WORD < UNITS_PER_HWFPVALUE ? high_p : byte != 0);
-      return gen_rtx_REG (word_mode, REGNO (op) + offset);
-    }
+    return gen_rtx_REG (word_mode, REGNO (op) + high_p);
 
   if (MEM_P (op))
     return loongarch_rewrite_small_data (adjust_address (op, word_mode, byte));
@@ -3364,12 +3418,10 @@ loongarch_subword (rtx op, bool high_p)
   return simplify_gen_subreg (word_mode, op, mode, byte);
 }
 
-/* Return true if a move from SRC to DEST should be split into two.
-   SPLIT_TYPE describes the split condition.  */
+/* Return true if a move from SRC to DEST should be split into two.  */
 
 bool
-loongarch_split_move_p (rtx dest, rtx src,
-			enum loongarch_split_type split_type ATTRIBUTE_UNUSED)
+loongarch_split_move_p (rtx dest, rtx src)
 {
   /* FPR-to-FPR moves can be done in a single instruction, if they're
      allowed at all.  */
@@ -3398,20 +3450,18 @@ loongarch_split_move_p (rtx dest, rtx src,
   return size > UNITS_PER_WORD;
 }
 
-/* Split a move from SRC to DEST, given that loongarch_split_move_p holds.
-   SPLIT_TYPE describes the split condition.  */
+/* Split a move from SRC to DEST, given that loongarch_split_move_p holds.  */
 
 void
-loongarch_split_move (rtx dest, rtx src, enum loongarch_split_type split_type,
-		      rtx insn_)
+loongarch_split_move (rtx dest, rtx src, rtx insn_)
 {
   rtx low_dest;
 
-  gcc_checking_assert (loongarch_split_move_p (dest, src, split_type));
+  gcc_checking_assert (loongarch_split_move_p (dest, src));
   if (LSX_SUPPORTED_MODE_P (GET_MODE (dest)))
     loongarch_split_128bit_move (dest, src);
   else if (LASX_SUPPORTED_MODE_P (GET_MODE (dest)))
-    loongarch_split_256bit_move (dest, src); 
+    loongarch_split_256bit_move (dest, src);
   else if (FP_REG_RTX_P (dest) || FP_REG_RTX_P (src))
     {
       if (!TARGET_64BIT && GET_MODE (dest) == DImode)
@@ -3470,24 +3520,6 @@ loongarch_split_move (rtx dest, rtx src, enum loongarch_split_type split_type,
     }
 }
 
-/* Return the split type for instruction INSN.  */
-
-static enum loongarch_split_type
-loongarch_insn_split_type (rtx insn)
-{
-  basic_block bb = BLOCK_FOR_INSN (insn);
-  if (bb)
-    {
-      if (optimize_bb_for_speed_p (bb))
-	return SPLIT_FOR_SPEED;
-      else
-	return SPLIT_FOR_SIZE;
-    }
-  /* Once CFG information has been removed, we should trust the optimization
-     decisions made by previous passes and only split where necessary.  */
-  return SPLIT_IF_NECESSARY;
-}
-
 /* Return true if a 128-bit move from SRC to DEST should be split.  */
 
 bool
@@ -3804,9 +3836,9 @@ loongarch_split_lsx_fill_d (rtx dest, rtx src)
 /* Return true if a move from SRC to DEST in INSN should be split.  */
 
 bool
-loongarch_split_move_insn_p (rtx dest, rtx src, rtx insn)
+loongarch_split_move_insn_p (rtx dest, rtx src)
 {
-  return loongarch_split_move_p (dest, src, loongarch_insn_split_type (insn));
+  return loongarch_split_move_p (dest, src);
 }
 
 /* Split a move from SRC to DEST in INSN, given that
@@ -3815,15 +3847,81 @@ loongarch_split_move_insn_p (rtx dest, rtx src, rtx insn)
 void
 loongarch_split_move_insn (rtx dest, rtx src, rtx insn)
 {
-  loongarch_split_move (dest, src, loongarch_insn_split_type (insn), insn);
+  loongarch_split_move (dest, src, insn);
 }
 
-/* Forward declaration.  Used below.  */
-static HOST_WIDE_INT
-loongarch_constant_alignment (const_tree exp, HOST_WIDE_INT align);
+/* Implement TARGET_CONSTANT_ALIGNMENT.  */
 
-/* Return the appropriate instructions to move SRC into DEST.  Assume
-   that SRC is operand 1 and DEST is operand 0.  */
+static HOST_WIDE_INT
+loongarch_constant_alignment (const_tree exp, HOST_WIDE_INT align)
+{
+  if (TREE_CODE (exp) == STRING_CST || TREE_CODE (exp) == CONSTRUCTOR)
+    return MAX (align, BITS_PER_WORD);
+  return align;
+}
+
+const char *
+loongarch_output_move_index (rtx x, machine_mode mode, bool ldr)
+{
+  int index = exact_log2 (GET_MODE_SIZE (mode));
+  if (!IN_RANGE (index, 0, 3))
+    return NULL;
+
+  struct loongarch_address_info info;
+  if ((loongarch_classify_address (&info, x, mode, false)
+       && !(info.type == ADDRESS_REG_REG))
+      || !loongarch_legitimate_address_p (mode, x, false))
+    return NULL;
+
+  const char *const insn[][4] =
+    {
+      {
+	"stx.b\t%z1,%0",
+	"stx.h\t%z1,%0",
+	"stx.w\t%z1,%0",
+	"stx.d\t%z1,%0",
+      },
+      {
+	"ldx.bu\t%0,%1",
+	"ldx.hu\t%0,%1",
+	"ldx.w\t%0,%1",
+	"ldx.d\t%0,%1",
+      }
+    };
+
+  return insn[ldr][index];
+}
+
+const char *
+loongarch_output_move_index_float (rtx x, machine_mode mode, bool ldr)
+{
+  int index = exact_log2 (GET_MODE_SIZE (mode));
+  if (!IN_RANGE (index, 2, 3))
+    return NULL;
+
+  struct loongarch_address_info info;
+  if ((loongarch_classify_address (&info, x, mode, false)
+       && !(info.type == ADDRESS_REG_REG))
+      || !loongarch_legitimate_address_p (mode, x, false))
+    return NULL;
+
+  const char *const insn[][2] =
+    {
+	{
+	  "fstx.s\t%1,%0",
+	  "fstx.d\t%1,%0"
+	},
+	{
+	  "fldx.s\t%0,%1",
+	  "fldx.d\t%0,%1"
+	}
+    };
+
+  return insn[ldr][index-2];
+}
+
+/* Return the appropriate instructions to move SRC into DEST.  Assume
+   that SRC is operand 1 and DEST is operand 0.  */
 
 const char *
 loongarch_output_move (rtx dest, rtx src)
@@ -3835,7 +3933,7 @@ loongarch_output_move (rtx dest, rtx src)
   bool lsx_p = LSX_SUPPORTED_MODE_P (mode);
   bool lasx_p = LASX_SUPPORTED_MODE_P (mode);
 
-  if (loongarch_split_move_p (dest, src, SPLIT_IF_NECESSARY))
+  if (loongarch_split_move_p (dest, src))
     return "#";
 
   if ((lsx_p || lasx_p)
@@ -3844,7 +3942,7 @@ loongarch_output_move (rtx dest, rtx src)
       && CONST_INT_P (CONST_VECTOR_ELT (src, 0)))
     {
       gcc_assert (loongarch_const_vector_same_int_p (src, mode, -512, 511));
-      if(lsx_p || lasx_p)
+      if (lsx_p || lasx_p)
       {
         switch (GET_MODE_SIZE (mode))
            {
@@ -3852,7 +3950,8 @@ loongarch_output_move (rtx dest, rtx src)
             return "vrepli.%v0\t%w0,%E1";
           case 32:
             return "xvrepli.%v0\t%u0,%E1";
-          default: gcc_unreachable ();
+	  default:
+	    gcc_unreachable ();
           }
       }
     }
@@ -3876,7 +3975,8 @@ loongarch_output_move (rtx dest, rtx src)
 		      return "vrepli.b\t%w0,0";
     	            case 32:
 		      return "xvrepli.b\t%u0,0";
-    	            default: gcc_unreachable ();
+		    default:
+		      gcc_unreachable ();
     	            }
 		}
 
@@ -3885,9 +3985,17 @@ loongarch_output_move (rtx dest, rtx src)
 	}
       if (dest_code == MEM)
 	{
+	  const char *insn = NULL;
+	  insn = loongarch_output_move_index (XEXP (dest, 0), GET_MODE (dest),
+					      false);
+	  if (insn)
+	    return insn;
+
 	  rtx offset = XEXP (dest, 0);
 	  if (GET_CODE (offset) == PLUS)
 	    offset = XEXP (offset, 1);
+	  else
+	    offset = const0_rtx;
 	  switch (GET_MODE_SIZE (mode))
 	    {
 	    case 1:
@@ -3895,12 +4003,12 @@ loongarch_output_move (rtx dest, rtx src)
 	    case 2:
 	      return "st.h\t%z1,%0";
 	    case 4:
-	      if (const_arith_operand (offset, Pmode))
+	      if (const_arith_operand (offset, Pmode) || (offset == const0_rtx))
 		return "st.w\t%z1,%0";
 	      else
 		return "stptr.w\t%z1,%0";
 	    case 8:
-	      if (const_arith_operand (offset, Pmode))
+	      if (const_arith_operand (offset, Pmode) || (offset == const0_rtx))
 		return "st.d\t%z1,%0";
 	      else
 		return "stptr.d\t%z1,%0";
@@ -3914,15 +4022,23 @@ loongarch_output_move (rtx dest, rtx src)
       if (src_code == REG)
 	if (FP_REG_P (REGNO (src)))
 	  {
-	    gcc_assert (!lsx_p);
+	    gcc_assert (!lsx_p && !lasx_p);
 	    return dbl_p ? "movfr2gr.d\t%0,%1" : "movfr2gr.s\t%0,%1";
 	  }
 
       if (src_code == MEM)
 	{
+	  const char *insn = NULL;
+	  insn = loongarch_output_move_index (XEXP (src, 0), GET_MODE (src),
+					      true);
+	  if (insn)
+	    return insn;
+
 	  rtx offset = XEXP (src, 0);
 	  if (GET_CODE (offset) == PLUS)
 	    offset = XEXP (offset, 1);
+	  else
+	    offset = const0_rtx;
 	  switch (GET_MODE_SIZE (mode))
 	    {
 	    case 1:
@@ -3930,12 +4046,12 @@ loongarch_output_move (rtx dest, rtx src)
 	    case 2:
 	      return "ld.hu\t%0,%1";
 	    case 4:
-	      if (const_arith_operand (offset, Pmode))
+	      if (const_arith_operand (offset, Pmode) || (offset == const0_rtx))
 		return "ld.w\t%0,%1";
 	      else
 		return "ldptr.w\t%0,%1";
 	    case 8:
-	      if (const_arith_operand (offset, Pmode))
+	      if (const_arith_operand (offset, Pmode) || (offset == const0_rtx))
 		return "ld.d\t%0,%1";
 	      else
 		return "ldptr.d\t%0,%1";
@@ -3968,7 +4084,7 @@ loongarch_output_move (rtx dest, rtx src)
 	      unsigned int align;
 
 	      if (GET_CODE (src) == LABEL_REF)
-		align = 128 /* Whatever.  */;
+		align = 32 /* Whatever.  */;
 	      else if (CONSTANT_POOL_ADDRESS_P (src))
 		align = GET_MODE_ALIGNMENT (get_pool_mode (src));
 	      else if (TREE_CONSTANT_POOL_ADDRESS_P (src))
@@ -4002,7 +4118,9 @@ loongarch_output_move (rtx dest, rtx src)
 	  if (TARGET_CMODEL_EXTREME)
 	    {
 	      sorry ("Normal symbol loading not implemented in extreme mode.");
+	      gcc_unreachable ();
 	    }
+
 	}
     }
   if (src_code == REG && FP_REG_P (REGNO (src)))
@@ -4011,14 +4129,14 @@ loongarch_output_move (rtx dest, rtx src)
 	{
 	  if (lsx_p || lasx_p)
 	  {
-
     	    switch (GET_MODE_SIZE (mode))
     	      {
     	      case 16:
 	        return "vori.b\t%w0,%w1,0";
     	      case 32:
 	        return "xvori.b\t%u0,%u1,0";
-    	      default: gcc_unreachable ();
+	      default:
+		gcc_unreachable ();
     	      }
 	  }
 	  else
@@ -4029,16 +4147,22 @@ loongarch_output_move (rtx dest, rtx src)
 	{
 	  if (lsx_p || lasx_p)
 	  {
-
     	    switch (GET_MODE_SIZE (mode))
     	      {
     	      case 16:
 	        return "vst\t%w1,%0";
     	      case 32:
 	        return "xvst\t%u1,%0";
-    	      default: gcc_unreachable ();
+	      default:
+		gcc_unreachable ();
     	      }
 	  }
+	  const char *insn = NULL;
+	  insn = loongarch_output_move_index_float (XEXP (dest, 0),
+						    GET_MODE (dest),
+						    false);
+	  if (insn)
+	    return insn;
 
 	  return dbl_p ? "fst.d\t%1,%0" : "fst.s\t%1,%0";
 	}
@@ -4055,9 +4179,17 @@ loongarch_output_move (rtx dest, rtx src)
 	        return "vld\t%w0,%1";
     	      case 32:
 	        return "xvld\t%u0,%1";
-    	      default: gcc_unreachable ();
+	      default:
+		gcc_unreachable ();
     	      }
 	  }
+	  const char *insn = NULL;
+	  insn = loongarch_output_move_index_float (XEXP (src, 0),
+						    GET_MODE (src),
+						    true);
+	  if (insn)
+	    return insn;
+
 	  return dbl_p ? "fld.d\t%0,%1" : "fld.s\t%0,%1";
 	}
     }
@@ -4246,6 +4378,7 @@ loongarch_extend_comparands (rtx_code code, rtx *op0, rtx *op1)
     }
 }
 
+
 /* Convert a comparison into something that can be used in a branch.  On
    entry, *OP0 and *OP1 are the values being compared and *CODE is the code
    used to compare them.  Update them to describe the final comparison.  */
@@ -4340,6 +4473,9 @@ loongarch_expand_scc (rtx operands[])
   rtx op0 = operands[2];
   rtx op1 = operands[3];
 
+  loongarch_extend_comparands (code, &op0, &op1);
+  op0 = force_reg (word_mode, op0);
+
   gcc_assert (GET_MODE_CLASS (GET_MODE (op0)) == MODE_INT);
 
   if (code == EQ || code == NE)
@@ -4373,14 +4509,16 @@ loongarch_expand_conditional_branch (rtx *operands)
 
 /* Perform the comparison in OPERANDS[1].  Move OPERANDS[2] into OPERANDS[0]
    if the condition holds, otherwise move OPERANDS[3] into OPERANDS[0].  */
-
-void
-loongarch_expand_conditional_move (rtx *operands)
+bool
+loongarch_expand_conditional_move_la464 (rtx *operands)
 {
   enum rtx_code code = GET_CODE (operands[1]);
   rtx op0 = XEXP (operands[1], 0);
   rtx op1 = XEXP (operands[1], 1);
+  machine_mode cmp_mode = GET_MODE(op0);
+  machine_mode sel_mode = GET_MODE(operands[2]);
 
+	/*ffii means Selecting a fixed point based on floating point comparison results */
   if (FLOAT_MODE_P (GET_MODE (op1)))
     loongarch_emit_float_compare (&code, &op0, &op1);
   else
@@ -4393,6 +4531,15 @@ loongarch_expand_conditional_move (rtx *operands)
 	{
 	  op0 = loongarch_zero_if_equal (op0, op1);
 	  op1 = const0_rtx;
+	  /*Be careful iiff*/
+	  if(FLOAT_MODE_P(sel_mode)){
+		  rtx target = gen_reg_rtx (GET_MODE (op0));
+		  bool invert = false;
+		  loongarch_emit_int_order_test (LTU, NULL, op0,
+				  force_reg (GET_MODE (op0), const0_rtx),
+				  op0);
+		  op1 = const0_rtx;
+	  }
 	}
       else
 	{
@@ -4410,10 +4557,11 @@ loongarch_expand_conditional_move (rtx *operands)
   rtx cond = gen_rtx_fmt_ee (code, GET_MODE (op0), op0, op1);
   /* There is no direct support for general conditional GP move involving
      two registers using SEL.  */
-  if (INTEGRAL_MODE_P (GET_MODE (operands[2]))
+   if (INTEGRAL_MODE_P (cmp_mode)
+          &&(INTEGRAL_MODE_P (sel_mode))
       && register_operand (operands[2], VOIDmode)
-      && register_operand (operands[3], VOIDmode))
-    {
+      && register_operand (operands[3], VOIDmode)) {
+
       machine_mode mode = GET_MODE (operands[0]);
       rtx temp = gen_reg_rtx (mode);
       rtx temp2 = gen_reg_rtx (mode);
@@ -4431,13 +4579,72 @@ loongarch_expand_conditional_move (rtx *operands)
 
       /* Merge the two results, at least one is guaranteed to be zero.  */
       emit_insn (gen_rtx_SET (operands[0], gen_rtx_IOR (mode, temp, temp2)));
-    }
-  else
-    emit_insn (gen_rtx_SET (operands[0],
-			    gen_rtx_IF_THEN_ELSE (GET_MODE (operands[0]), cond,
-						  operands[2], operands[3])));
-}
 
+	  return true;
+  /*For ffii, iiff due to movgr2fr, movfr2gr overhead is relatively large,
+   * so we use some compromise*/
+    } else if (INTEGRAL_MODE_P (cmp_mode)
+      &&(FLOAT_MODE_P (sel_mode))
+      && register_operand (operands[2], VOIDmode)
+      && register_operand (operands[3], VOIDmode)) {
+		 rtx temp = gen_reg_rtx(sel_mode);
+		 rtx fcc_reg =loongarch_allocate_fcc (FCCmode);
+		 rtx diop0 = convert_to_mode(E_DImode, op0, true);
+		/*stl t0 i i-> movgr2fr f0 t0 -> movfr2cf fcc0 f0 -> fsel f f*/
+		 if(sel_mode == E_DFmode){
+			 emit_insn(gen_movdgr2frdf(temp, diop0));
+			 emit_insn(gen_movfr2fccdf(fcc_reg, temp));
+		 }else if(sel_mode == E_SFmode){
+			 emit_insn(gen_movdgr2frsf(temp, diop0));
+			 emit_insn(gen_movfr2fccsf(fcc_reg, temp));
+		 }
+		 cond = gen_rtx_fmt_ee (code, GET_MODE(fcc_reg), fcc_reg, const0_rtx);
+
+		 emit_insn (gen_rtx_SET (operands[0],
+					 gen_rtx_IF_THEN_ELSE (GET_MODE (operands[0]), cond,
+						 operands[2], operands[3])));
+		 return true;
+	 } else if (FLOAT_MODE_P (cmp_mode)
+       &&(INTEGRAL_MODE_P (sel_mode))) {
+		/*movgr2fr f0 i -> movgr2fr f1 i -> fcmp fcc0 f f
+		 * -> fsel f3 f0 f1 -> movfr2gr t0 f3*/
+       machine_mode dst_mode = GET_MODE (operands[0]);
+       rtx temp = gen_reg_rtx (E_DFmode);
+       rtx temp2 = gen_reg_rtx (E_DFmode);
+       rtx temp3 = gen_reg_rtx (E_DFmode);
+
+	   if(CONST_INT_P(operands[2])){
+		    operands[2] = copy_to_mode_reg(dst_mode, operands[2]);
+	   }
+	   if(CONST_INT_P(operands[3])){
+		    operands[3] = copy_to_mode_reg(dst_mode, operands[3]);
+	   }
+	   if(GET_MODE(operands[2]) != E_DImode)
+		   operands[2] = convert_to_mode(E_DImode, operands[2], false);
+	   if(GET_MODE(operands[3]) != E_DImode)
+		   operands[3] = convert_to_mode(E_DImode, operands[3], false);
+
+		emit_insn(gen_movdgr2frdf(temp2, operands[2]));
+		emit_insn(gen_movdgr2frdf(temp3, operands[3]));
+
+		emit_insn (gen_rtx_SET (temp,
+					gen_rtx_IF_THEN_ELSE (E_DFmode, cond,
+						temp2, temp3)));
+		if(GET_MODE(operands[0]) == E_DImode)
+			emit_insn(gen_movdfr2grdi(operands[0], temp));
+		else if(GET_MODE(operands[0]) == E_SImode)
+			emit_insn(gen_movdfr2grsi(operands[0], temp));
+		return true;
+	 } else if(FLOAT_MODE_P (cmp_mode)
+			 &&FLOAT_MODE_P (sel_mode)){
+		 emit_insn (gen_rtx_SET (operands[0],
+					 gen_rtx_IF_THEN_ELSE (GET_MODE (operands[0]), cond,
+						 operands[2], operands[3])));
+		 return true;
+	 }
+
+   return false;
+}
 /* Implement TARGET_EXPAND_BUILTIN_VA_START.  */
 
 static void
@@ -4447,19 +4654,6 @@ loongarch_va_start (tree valist, rtx nextarg)
   std_expand_builtin_va_start (valist, nextarg);
 }
 
-
-/* Start a definition of function NAME.  */
-
-static void
-loongarch_start_function_definition (const char *name)
-{
-  ASM_OUTPUT_TYPE_DIRECTIVE (asm_out_file, name, "function");
-
-  /* Start the definition proper.  */
-  assemble_name (asm_out_file, name);
-  fputs (":\n", asm_out_file);
-}
-
 /* Implement TARGET_FUNCTION_OK_FOR_SIBCALL.  */
 
 static bool
@@ -4539,7 +4733,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
     {
       if (ISA_HAS_LASX)
 	{
-	  if(length - offset >= 16)
+	  if (length - offset >= 16)
 	    {
 	      rtx *regs_tmp = XALLOCAVEC (rtx, 1);
 	      regs_tmp[0] = gen_reg_rtx (V2DImode);
@@ -4547,7 +4741,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
 	      loongarch_emit_move (adjust_address (dest, V2DImode, offset), regs_tmp[0]);
 	      offset += 16;
 	    }
-	  if(length - offset  >= 8)
+	  if (length - offset  >= 8)
 	    {
 	      rtx *regs_tmp = XALLOCAVEC (rtx, 1);
 	      regs_tmp[0] = gen_reg_rtx (DImode);
@@ -4555,7 +4749,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
 	      loongarch_emit_move (adjust_address (dest, DImode, offset), regs_tmp[0]);
 	      offset += 8;
 	    }
-	  if(length - offset >= 4)
+	  if (length - offset >= 4)
 	    {
 	      rtx *regs_tmp = XALLOCAVEC (rtx, 1);
 	      regs_tmp[0] = gen_reg_rtx (SImode);
@@ -4563,7 +4757,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
 	      loongarch_emit_move (adjust_address (dest, SImode, offset), regs_tmp[0]);
 	      offset += 4;
 	    }
-	  if(length - offset >= 2)
+	  if (length - offset >= 2)
 	    {
 	      rtx *regs_tmp = XALLOCAVEC (rtx, 1);
 	      regs_tmp[0] = gen_reg_rtx (HImode);
@@ -4571,7 +4765,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
 	      loongarch_emit_move (adjust_address (dest, HImode, offset), regs_tmp[0]);
 	      offset += 2;
 	    }
-	  if(length - offset >= 1)
+	  if (length - offset >= 1)
 	    {
 	      rtx *regs_tmp = XALLOCAVEC (rtx, 1);
 	      regs_tmp[0] = gen_reg_rtx (QImode);
@@ -4580,7 +4774,7 @@ loongarch_block_move_straight (rtx dest, rtx src, HOST_WIDE_INT length)
 	      offset += 1;
 	    }
 
-	  if(length - offset != 0)
+	  if (length - offset != 0)
 	    gcc_unreachable ();
 	}
       else
@@ -4666,7 +4860,7 @@ loongarch_block_move_loop (rtx dest, rtx src, HOST_WIDE_INT length,
     emit_insn (gen_nop ());
 }
 
-/* Expand a cpymemsi instruction, which copies LENGTH bytes from
+/* Expand a movmemsi instruction, which copies LENGTH bytes from
    memory reference SRC to memory reference DEST.  */
 
 bool
@@ -5254,6 +5448,11 @@ loongarch_print_operand_address (FILE *file, machine_mode /* mode  */, rtx x)
 	loongarch_print_operand (file, addr.offset, 0);
 	return;
 
+      case ADDRESS_REG_REG:
+	fprintf (file, "%s,%s", reg_names[REGNO (addr.reg)],
+		 reg_names[REGNO (addr.offset)]);
+	return;
+
       case ADDRESS_CONST_INT:
 	fprintf (file, "%s,", reg_names[GP_REG_FIRST]);
 	output_addr_const (file, x);
@@ -5263,7 +5462,7 @@ loongarch_print_operand_address (FILE *file, machine_mode /* mode  */, rtx x)
 	output_addr_const (file, loongarch_strip_unspec_address (x));
 	return;
       }
-  if (GET_CODE (x) == CONST_INT)
+  if (CONST_INT_P (x))
     output_addr_const (file, x);
   else
     gcc_unreachable ();
@@ -5284,12 +5483,10 @@ loongarch_select_rtx_section (machine_mode mode, rtx x,
 
 /* Implement TARGET_ASM_FUNCTION_RODATA_SECTION.
 
-   The complication here is that, with the combination
-   !TARGET_ABSOLUTE_ABICALLS , jump tables will use
-   absolute addresses, and should therefore not be included in the
-   read-only part of a DSO.  Handle such cases by selecting a normal
-   data section instead of a read-only one.  The logic apes that in
-   default_function_rodata_section.  */
+   The complication here is that jump atbles will use absolute addresses,
+   and should therefore not be included in the read-only part of a DSO.
+   Handle such cases by selecting a normal data section instead of a
+   read-only one.  The logic apes that in default_function_rodata_section.  */
 
 static section *
 loongarch_function_rodata_section (tree decl)
@@ -5328,17 +5525,6 @@ loongarch_in_small_data_p (const_tree decl)
   return size > 0 && size <= g_switch_value;
 }
 
-/* Implement TARGET_USE_ANCHORS_FOR_SYMBOL_P.  We don't want to use
-   anchors for small data: the GP register acts as an anchor in that
-   case.  We also don't want to use them for PC-relative accesses,
-   where the PC acts as an anchor.  */
-
-static bool
-loongarch_use_anchors_for_symbol_p (const_rtx symbol)
-{
-  return default_use_anchors_for_symbol_p (symbol);
-}
-
 /* The LoongArch debug format wants all automatic variables and arguments
    to be in terms of the virtual frame pointer (stack pointer before
    any adjustment in the function), while the LoongArch linker wants
@@ -5421,16 +5607,6 @@ loongarch_output_dwarf_dtprel (FILE *file, int size, rtx x)
   fputs ("+0x8000", file);
 }
 
-/* Implement TARGET_DWARF_FRAME_REG_MODE.  */
-
-static machine_mode
-loongarch_dwarf_frame_reg_mode (int regno)
-{
-  machine_mode mode = default_dwarf_frame_reg_mode (regno);
-
-  return mode;
-}
-
 /* Implement ASM_OUTPUT_ASCII.  */
 
 void
@@ -5471,165 +5647,6 @@ loongarch_output_ascii (FILE *stream, const char *string, size_t len)
   fprintf (stream, "\"\n");
 }
 
-/* Emit either a label, .comm, or .lcomm directive.  When using assembler
-   macros, mark the symbol as written so that loongarch_asm_output_external
-   won't emit an .extern for it.  STREAM is the output file, NAME is the
-   name of the symbol, INIT_STRING is the string that should be written
-   before the symbol and FINAL_STRING is the string that should be
-   written after it.  FINAL_STRING is a printf format that consumes the
-   remaining arguments.  */
-
-void
-loongarch_declare_object (FILE *stream, const char *name,
-			  const char *init_string, const char *final_string,
-			  ...)
-{
-  va_list ap;
-
-  fputs (init_string, stream);
-  assemble_name (stream, name);
-  va_start (ap, final_string);
-  vfprintf (stream, final_string, ap);
-  va_end (ap);
-
-  tree name_tree = get_identifier (name);
-  TREE_ASM_WRITTEN (name_tree) = 1;
-}
-
-/* Declare a common object of SIZE bytes using asm directive INIT_STRING.
-   NAME is the name of the object and ALIGN is the required alignment
-   in bytes.  TAKES_ALIGNMENT_P is true if the directive takes a third
-   alignment argument.  */
-
-void
-loongarch_declare_common_object (FILE *stream, const char *name,
-				 const char *init_string,
-				 unsigned HOST_WIDE_INT size,
-				 unsigned int align, bool takes_alignment_p)
-{
-  if (!takes_alignment_p)
-    {
-      size += (align / BITS_PER_UNIT) - 1;
-      size -= size % (align / BITS_PER_UNIT);
-      loongarch_declare_object (stream, name, init_string,
-				"," HOST_WIDE_INT_PRINT_UNSIGNED "\n", size);
-    }
-  else
-    loongarch_declare_object (stream, name, init_string,
-			      "," HOST_WIDE_INT_PRINT_UNSIGNED ",%u\n", size,
-			      align / BITS_PER_UNIT);
-}
-
-/* Implement ASM_OUTPUT_ALIGNED_DECL_COMMON.  This is usually the same as the
-   elfos.h version.  */
-
-void
-loongarch_output_aligned_decl_common (FILE *stream,
-				      tree decl ATTRIBUTE_UNUSED,
-				      const char *name,
-				      unsigned HOST_WIDE_INT size,
-				      unsigned int align)
-{
-  loongarch_declare_common_object (stream, name, "\n\t.comm\t", size, align,
-				   true);
-}
-
-#ifdef ASM_OUTPUT_SIZE_DIRECTIVE
-extern int size_directive_output;
-
-/* Implement ASM_DECLARE_OBJECT_NAME.  This is like most of the standard ELF
-   definitions except that it uses loongarch_declare_object to emit the label.
-*/
-
-void
-loongarch_declare_object_name (FILE *stream, const char *name,
-			       tree decl ATTRIBUTE_UNUSED)
-{
-#ifdef ASM_OUTPUT_TYPE_DIRECTIVE
-#ifdef USE_GNU_UNIQUE_OBJECT
-  /* As in elfos.h.  */
-  if (USE_GNU_UNIQUE_OBJECT && DECL_ONE_ONLY (decl)
-      && (!DECL_ARTIFICIAL (decl) || !TREE_READONLY (decl)))
-    ASM_OUTPUT_TYPE_DIRECTIVE (stream, name, "gnu_unique_object");
-  else
-#endif
-    ASM_OUTPUT_TYPE_DIRECTIVE (stream, name, "object");
-#endif
-
-  size_directive_output = 0;
-  if (!flag_inhibit_size_directive && DECL_SIZE (decl))
-    {
-      HOST_WIDE_INT size;
-
-      size_directive_output = 1;
-      size = int_size_in_bytes (TREE_TYPE (decl));
-      ASM_OUTPUT_SIZE_DIRECTIVE (stream, name, size);
-    }
-
-  loongarch_declare_object (stream, name, "", ":\n");
-}
-
-/* Implement ASM_FINISH_DECLARE_OBJECT.  This is generic ELF stuff.  */
-
-void
-loongarch_finish_declare_object (FILE *stream, tree decl, int top_level,
-				 int at_end)
-{
-  const char *name;
-
-  name = XSTR (XEXP (DECL_RTL (decl), 0), 0);
-  if (!flag_inhibit_size_directive
-      && DECL_SIZE (decl) != 0
-      && !at_end
-      && top_level
-      && DECL_INITIAL (decl) == error_mark_node
-      && !size_directive_output)
-    {
-      HOST_WIDE_INT size;
-
-      size_directive_output = 1;
-      size = int_size_in_bytes (TREE_TYPE (decl));
-      ASM_OUTPUT_SIZE_DIRECTIVE (stream, name, size);
-    }
-}
-#endif
-
-/* Mark text contents as code or data, mainly for the purpose of correct
-   disassembly.  Emit a local symbol and set its type appropriately for
-   that purpose.  */
-
-void
-loongarch_set_text_contents_type (FILE *file ATTRIBUTE_UNUSED,
-				  const char *prefix ATTRIBUTE_UNUSED,
-				  unsigned long num ATTRIBUTE_UNUSED,
-				  bool function_p ATTRIBUTE_UNUSED)
-{
-#ifdef ASM_OUTPUT_TYPE_DIRECTIVE
-  char buf[(sizeof (num) * 10) / 4 + 2];
-  const char *fnname;
-  char *sname;
-  rtx symbol;
-
-  sprintf (buf, "%lu", num);
-  symbol = XEXP (DECL_RTL (current_function_decl), 0);
-  fnname = targetm.strip_name_encoding (XSTR (symbol, 0));
-  sname = ACONCAT ((prefix, fnname, "_", buf, NULL));
-
-  ASM_OUTPUT_TYPE_DIRECTIVE (file, sname, function_p ? "function" : "object");
-  assemble_name (file, sname);
-  fputs (":\n", file);
-#endif
-}
-
-/* Implement TARGET_ASM_FILE_START.  */
-
-static void
-loongarch_file_start (void)
-{
-  default_file_start ();
-}
-
-
 /* Implement TARGET_FRAME_POINTER_REQUIRED.  */
 
 static bool
@@ -5688,7 +5705,7 @@ loongarch_set_return_address (rtx address, rtx scratch)
 
 /* LOONGSON LA464 Emit insn pattern for gssq and gslq*/
 void
-loongarch_la464_emit_128bit_load(rtx operands[])
+loongarch_la464_emit_128bit_load (rtx operands[])
 {
   rtx op0;
   rtx op1;
@@ -5696,9 +5713,9 @@ loongarch_la464_emit_128bit_load(rtx operands[])
   rtx op3;
 
 #if 0 /*for debug*/
-  printf("464po: emit 128 PO LOAD!\n");
-  printf("reg num of op0 is: %d\n",REGNO(operands[0]));
-  printf("reg num of op2 is: %d\n",REGNO(operands[2]));
+  printf ("464po: emit 128 PO LOAD!\n");
+  printf ("reg num of op0 is: %d\n",REGNO (operands[0]));
+  printf ("reg num of op2 is: %d\n",REGNO (operands[2]));
 #endif
   op0 = gen_rtx_REG (GET_MODE (operands[0]), REGNO (operands[0]));
   op1 = operands[1];
@@ -5710,8 +5727,8 @@ loongarch_la464_emit_128bit_load(rtx operands[])
 				     gen_rtx_SET (op2,op3))));
 }
 
-void 
-loongarch_la464_emit_128bit_store(rtx operands[])
+void
+loongarch_la464_emit_128bit_store (rtx operands[])
 {
   rtx op0;
   rtx op1;
@@ -5719,10 +5736,10 @@ loongarch_la464_emit_128bit_store(rtx operands[])
   rtx op3;
 
 #if 0 /*for debug*/
-  printf("464po: emit 128 PO STORE!\n");
-  printf("reg num of op1 is: %d\n",REGNO(operands[1]));
-  printf("reg num of op3 is: %d\n",REGNO(operands[3]));
-#endif 
+  printf ("464po: emit 128 PO STORE!\n");
+  printf ("reg num of op1 is: %d\n",REGNO (operands[1]));
+  printf ("reg num of op3 is: %d\n",REGNO (operands[3]));
+#endif
   op0 = operands[0];
   op1 = gen_rtx_REG (GET_MODE (operands[1]), REGNO (operands[1]));
   op2 = operands[2];
@@ -5734,16 +5751,6 @@ loongarch_la464_emit_128bit_store(rtx operands[])
 
 }
 
-/* Implement ASM_DECLARE_FUNCTION_NAME.  */
-
-void
-loongarch_declare_function_name (FILE *stream ATTRIBUTE_UNUSED,
-				 const char *name,
-				 tree fndecl ATTRIBUTE_UNUSED)
-{
-  loongarch_start_function_definition (name);
-}
-
 /* Return true if register REGNO can store a value of mode MODE.
    The result of this function is cached in loongarch_hard_regno_mode_ok.  */
 
@@ -5770,9 +5777,7 @@ loongarch_hard_regno_mode_ok_uncached (unsigned int regno, machine_mode mode)
   if (FP_REG_P (regno) && LASX_SUPPORTED_MODE_P (mode))
     return true;
 
-  if (FP_REG_P (regno)
-      && (((regno - FP_REG_FIRST) % MAX_FPRS_PER_FMT) == 0
-	  || (MIN_FPRS_PER_FMT == 1 && size <= UNITS_PER_FPREG)))
+  if (FP_REG_P (regno))
     {
       if (mclass == MODE_FLOAT
 	  || mclass == MODE_COMPLEX_FLOAT
@@ -5798,15 +5803,6 @@ loongarch_hard_regno_mode_ok (unsigned int regno, machine_mode mode)
   return loongarch_hard_regno_mode_ok_p[mode][regno];
 }
 
-/* Return nonzero if register OLD_REG can be renamed to register NEW_REG.  */
-
-bool
-loongarch_hard_regno_rename_ok (unsigned int old_reg ATTRIBUTE_UNUSED,
-				unsigned int new_reg ATTRIBUTE_UNUSED)
-{
-  return true;
-}
-
 static bool
 loongarch_hard_regno_call_part_clobbered (unsigned int regno, machine_mode mode)
 {
@@ -6057,26 +6053,6 @@ loongarch_memory_move_cost (machine_mode mode, reg_class_t rclass, bool in)
 	  + memory_move_secondary_cost (mode, rclass, in));
 }
 
-/* Implement TARGET_SECONDARY_MEMORY_NEEDED.
-
-   This can be achieved using MOVFRH2GR.S/MOVGR2FRH.W when these instructions
-   are available but otherwise moves must go via memory.  Using
-   MOVGR2FR/MOVFR2GR to access the lower-half of these registers would require
-   a forbidden single-precision access.  We require all double-word moves to
-   use memory because adding even and odd floating-point registers classes
-   would have a significant impact on the backend.  */
-
-static bool
-loongarch_secondary_memory_needed (machine_mode mode ATTRIBUTE_UNUSED,
-				   reg_class_t class1, reg_class_t class2)
-{
-  /* Ignore spilled pseudos.  */
-  if (lra_in_progress && (class1 == NO_REGS || class2 == NO_REGS))
-    return false;
-
-  return false;
-}
-
 /* Return the register class required for a secondary register when
    copying between one of the registers in RCLASS and value X, which
    has mode MODE.  X is the source of the move if IN_P, otherwise it
@@ -6130,6 +6106,19 @@ loongarch_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x,
   return NO_REGS;
 }
 
+/* Implement TARGET_MODE_REP_EXTENDED  */
+
+static int
+loongarch_mode_rep_extended (scalar_int_mode mode, scalar_int_mode mode_rep)
+{
+  /* On 64-bit targets, SImode register values are sign-extended to DImode.  */
+  if (TARGET_64BIT && mode == SImode && mode_rep == DImode)
+    return SIGN_EXTEND;
+
+  return UNKNOWN;
+}
+
+
 /* Implement TARGET_VALID_POINTER_MODE.  */
 
 static bool
@@ -6422,13 +6411,13 @@ loongarch_lsx_output_division (const char *division, rtx *operands)
   s = division;
   if (TARGET_CHECK_ZERO_DIV)
     {
-      if(ISA_HAS_LASX && GET_MODE_SIZE (mode) == 32)
+      if (ISA_HAS_LASX && GET_MODE_SIZE (mode) == 32)
 	{
 	  output_asm_insn ("xvsetallnez.%v0\t$fcc7,%u2",operands);
           output_asm_insn (s, operands);
 	  output_asm_insn ("bcnez\t$fcc7,1f", operands);
 	}
-      else if(ISA_HAS_LSX)
+      else if (ISA_HAS_LSX)
 	{
 	  output_asm_insn ("vsetallnez.%v0\t$fcc7,%w2",operands);
           output_asm_insn (s, operands);
@@ -6530,101 +6519,6 @@ loongarch_variable_issue (FILE *file ATTRIBUTE_UNUSED,
   return more;
 }
 
-/* A structure representing the state of the processor pipeline.
-   Used by the loongarch_sim_* family of functions.  */
-
-struct loongarch_sim
-{
-  /* The maximum number of instructions that can be issued in a cycle.
-     (Caches loongarch_issue_rate.)  */
-  unsigned int issue_rate;
-
-  /* The current simulation time.  */
-  unsigned int time;
-
-  /* How many more instructions can be issued in the current cycle.  */
-  unsigned int insns_left;
-
-  /* LAST_SET[X].INSN is the last instruction to set register X.
-     LAST_SET[X].TIME is the time at which that instruction was issued.
-     INSN is null if no instruction has yet set register X.  */
-  struct
-  {
-    rtx_insn *insn;
-    unsigned int time;
-  } last_set[FIRST_PSEUDO_REGISTER];
-
-  /* The pipeline's current DFA state.  */
-  state_t dfa_state;
-};
-
-/* Reset STATE to the initial simulation state.  */
-
-static void
-loongarch_sim_reset (struct loongarch_sim *state)
-{
-  curr_state = state->dfa_state;
-
-  state->time = 0;
-  state->insns_left = state->issue_rate;
-  memset (&state->last_set, 0, sizeof (state->last_set));
-  state_reset (curr_state);
-
-  targetm.sched.init (0, false, 0);
-  advance_state (curr_state);
-}
-
-/* Initialize STATE before its first use.  DFA_STATE points to an
-   allocated but uninitialized DFA state.  */
-
-static void
-loongarch_sim_init (struct loongarch_sim *state, state_t dfa_state)
-{
-  if (targetm.sched.init_dfa_pre_cycle_insn)
-    targetm.sched.init_dfa_pre_cycle_insn ();
-
-  if (targetm.sched.init_dfa_post_cycle_insn)
-    targetm.sched.init_dfa_post_cycle_insn ();
-
-  state->issue_rate = loongarch_issue_rate ();
-  state->dfa_state = dfa_state;
-  loongarch_sim_reset (state);
-}
-
-/* Set up costs based on the current architecture and tuning settings.  */
-
-static void
-loongarch_set_tuning_info (void)
-{
-  dfa_start ();
-
-  struct loongarch_sim state;
-  loongarch_sim_init (&state, alloca (state_size ()));
-
-  dfa_finish ();
-}
-
-/* Implement TARGET_EXPAND_TO_RTL_HOOK.  */
-
-static void
-loongarch_expand_to_rtl_hook (void)
-{
-  /* We need to call this at a point where we can safely create sequences
-     of instructions, so TARGET_OVERRIDE_OPTIONS is too early.  We also
-     need to call it at a point where the DFA infrastructure is not
-     already in use, so we can't just call it lazily on demand.
-
-     At present, loongarch_tuning_info is only needed during post-expand
-     RTL passes such as split_insns, so this hook should be early enough.
-     We may need to move the call elsewhere if loongarch_tuning_info starts
-     to be used for other things (such as rtx_costs, or expanders that
-     could be called during gimple optimization).  */
-
-  loongarch_set_tuning_info ();
-}
-
-
-
 /* Implement TARGET_ASM_OUTPUT_MI_THUNK.  Generate rtl rather than asm text
    in order to avoid duplicating too much logic from elsewhere.  */
 
@@ -6707,8 +6601,7 @@ loongarch_output_mi_thunk (FILE *file, tree thunk_fndecl ATTRIBUTE_UNUSED,
   final (insn, file, 1);
   final_end_function ();
 
-  /* Clean up the vars set above.  Note that final_end_function resets
-     the global pointer for us.  */
+  /* Stop pretending to be a post-reload pass.  */
   reload_completed = 0;
 }
 
@@ -6720,26 +6613,12 @@ loongarch_init_machine_status (void)
   return ggc_cleared_alloc<machine_function> ();
 }
 
-static void
-loongarch_option_override_internal (struct gcc_options *opts,
-				    struct gcc_options *opts_set);
-
-/* Implement TARGET_OPTION_OVERRIDE.  */
-
-static void
-loongarch_option_override (void)
-{
-  loongarch_option_override_internal (&global_options, &global_options_set);
-}
-
 static void
 loongarch_option_override_internal (struct gcc_options *opts,
 				    struct gcc_options *opts_set)
 {
   int i, regno, mode;
 
-  (void) opts_set;
-
   if (flag_pic)
     g_switch_value = 0;
 
@@ -6749,8 +6628,6 @@ loongarch_option_override_internal (struct gcc_options *opts,
 			   la_opt_abi_base, la_opt_abi_ext, la_opt_cmodel, 0);
 
   loongarch_update_gcc_opt_status (opts, &la_target);
-
-  /* End of code shared with GAS.  */
   if (TARGET_ABI_LP64)
     flag_pcc_struct_return = 0;
 
@@ -6765,6 +6642,9 @@ loongarch_option_override_internal (struct gcc_options *opts,
   if (loongarch_branch_cost == 0)
     loongarch_branch_cost = loongarch_cost->branch_cost;
 
+  if (loongarch_vector_access_cost == 0)
+    loongarch_vector_access_cost = 5;
+
 
   switch (la_target.cmodel)
     {
@@ -6805,6 +6685,27 @@ loongarch_option_override_internal (struct gcc_options *opts,
 
   /* Function to allocate machine-dependent function status.  */
   init_machine_status = &loongarch_init_machine_status;
+
+  /* If not optimizing for size, set the default
+     alignment to what the target wants.  */
+  if (!opts->x_optimize_size)
+    {
+      if (opts->x_align_loops <= 0)
+	opts->x_align_loops = 16;
+      if (opts->x_align_jumps <= 0)
+	opts->x_align_jumps = 16;
+      if (opts->x_align_functions <= 0)
+	opts->x_align_functions = 16;
+    }
+}
+
+
+/* Implement TARGET_OPTION_OVERRIDE.  */
+
+static void
+loongarch_option_override (void)
+{
+  loongarch_option_override_internal (&global_options, &global_options_set);
 }
 
 /* Implement TARGET_CONDITIONAL_REGISTER_USAGE.  */
@@ -6961,16 +6862,6 @@ loongarch_trampoline_init (rtx m_tramp, tree fndecl, rtx chain_value)
   emit_insn (gen_clear_cache (addr, end_addr));
 }
 
-/* Implement TARGET_SHIFT_TRUNCATION_MASK.  We want to keep the default
-   behavior of TARGET_SHIFT_TRUNCATION_MASK for non-vector modes.  */
-
-static unsigned HOST_WIDE_INT
-loongarch_shift_truncation_mask (machine_mode mode)
-{
-  return GET_MODE_BITSIZE (mode) - 1;
-}
-
-
 /* Generate or test for an insn that supports a constant permutation.  */
 
 #define MAX_VECT_LEN 32
@@ -7062,23 +6953,390 @@ loongarch_expand_lsx_shuffle (struct expand_vec_perm_d *d)
   return true;
 }
 
-void
-loongarch_expand_vec_perm (rtx target, rtx op0, rtx op1, rtx sel)
+/* Try to simplify a two vector permutation using 2 intra-lane interleave
+   insns and cross-lane shuffle for 32-byte vectors.  */
+
+static bool
+loongarch_expand_vec_perm_interleave (struct expand_vec_perm_d *d)
 {
-  machine_mode vmode = GET_MODE (target);
+  unsigned i, nelt;
+  rtx t1,t2,t3;
+  rtx (*gen_high) (rtx, rtx, rtx);
+  rtx (*gen_low) (rtx, rtx, rtx);
+  machine_mode mode = GET_MODE (d->target);
 
-  gcc_checking_assert (vmode == E_V16QImode
-      || vmode == E_V2DImode || vmode == E_V2DFmode
-      || vmode == E_V4SImode || vmode == E_V4SFmode
-      || vmode == E_V8HImode);
-  gcc_checking_assert (GET_MODE (op0) == vmode);
-  gcc_checking_assert (GET_MODE (op1) == vmode);
-  gcc_checking_assert (GET_MODE (sel) == vmode);
-  gcc_checking_assert (ISA_HAS_LSX);
+  if (d->one_vector_p)
+    return false;
+  if (TARGET_LASX && GET_MODE_SIZE (d->vmode) == 32)
+    ;
+  else
+    return false;
 
-  switch (vmode)
+  nelt = d->nelt;
+  if (d->perm[0] != 0 && d->perm[0] != nelt / 2)
+    return false;
+  for (i = 0; i < nelt; i += 2)
+    if (d->perm[i] != d->perm[0] + i / 2
+	|| d->perm[i + 1] != d->perm[0] + i / 2 + nelt)
+      return false;
+
+  if (d->testing_p)
+    return true;
+
+  switch (d->vmode)
     {
-    case E_V16QImode:
+    case E_V32QImode:
+      gen_high = gen_lasx_xvilvh_b;
+      gen_low = gen_lasx_xvilvl_b;
+      break;
+    case E_V16HImode:
+      gen_high = gen_lasx_xvilvh_h;
+      gen_low = gen_lasx_xvilvl_h;
+      break;
+    case E_V8SImode:
+      gen_high = gen_lasx_xvilvh_w;
+      gen_low = gen_lasx_xvilvl_w;
+      break;
+    case E_V4DImode:
+      gen_high = gen_lasx_xvilvh_d;
+      gen_low = gen_lasx_xvilvl_d;
+      break;
+    case E_V8SFmode:
+      gen_high = gen_lasx_xvilvh_w_f;
+      gen_low = gen_lasx_xvilvl_w_f;
+      break;
+    case E_V4DFmode:
+      gen_high = gen_lasx_xvilvh_d_f;
+      gen_low = gen_lasx_xvilvl_d_f;
+      break;
+    default:
+      gcc_unreachable ();
+    }
+
+  t1 = gen_reg_rtx (mode);
+  t2 = gen_reg_rtx (mode);
+  emit_insn (gen_high (t1, d->op0, d->op1));
+  emit_insn (gen_low (t2, d->op0, d->op1));
+  if(mode == V4DFmode || mode == V8SFmode)
+    {
+      t3 = gen_reg_rtx (V4DFmode);
+      if (d->perm[0])
+	emit_insn(gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
+					   gen_lowpart (V4DFmode, t2),GEN_INT(0x31)));
+      else
+	emit_insn(gen_lasx_xvpermi_q_v4df (t3, gen_lowpart (V4DFmode, t1),
+					   gen_lowpart (V4DFmode, t2),GEN_INT(0x20)));
+    }
+  else
+    {
+      t3 = gen_reg_rtx (V4DImode);
+      if (d->perm[0])
+	emit_insn(gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
+					   gen_lowpart (V4DImode, t2),GEN_INT(0x31)));
+      else
+	emit_insn(gen_lasx_xvpermi_q_v4di (t3, gen_lowpart (V4DImode, t1),
+					   gen_lowpart (V4DImode, t2),GEN_INT(0x20)));
+    }
+  emit_move_insn (d->target, gen_lowpart (mode, t3));
+  return true;
+}
+
+/* Implement extract-even and extract-odd permutations.*/
+
+static bool
+loongarch_expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)
+{
+  rtx t1;
+  machine_mode mode = GET_MODE (d->target);
+  t1 = gen_reg_rtx (mode);
+
+  if (d->testing_p)
+    return true;
+
+  switch (d->vmode)
+    {
+    case E_V4DFmode:
+      /* Shuffle the lanes around into { 0 4 2 6 } and { 1 5 3 7 }.  */
+      if (odd)
+	emit_insn (gen_lasx_xvilvh_d_f (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvilvl_d_f (t1, d->op0, d->op1));
+
+      /* Shuffle within the 256-bit lanes to produce the result required.
+	 { 0 2 4 6 } | { 1 3 5 7 }.  */
+      emit_insn (gen_lasx_xvpermi_d_v4df (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    case E_V4DImode:
+      if (odd)
+	emit_insn (gen_lasx_xvilvh_d (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvilvl_d (t1, d->op0, d->op1));
+
+      emit_insn (gen_lasx_xvpermi_d_v4di (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    case E_V8SFmode:
+      /* Shuffle the lanes around into:
+	 { 0 2 8 a 4 6 c e } | { 1 3 9 b 5 7 d f }.  */
+      if (odd)
+	emit_insn (gen_lasx_xvpickod_w_f (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvpickev_w_f (t1, d->op0, d->op1));
+
+      /* Shuffle within the 256-bit lanes to produce the result required.
+	 { 0 2 4 6 8 a c e } | { 1 3 5 7 9 b d f }.  */
+      emit_insn (gen_lasx_xvpermi_d_v8sf (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    case E_V8SImode:
+      if (odd)
+	emit_insn (gen_lasx_xvpickod_w (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvpickev_w (t1, d->op0, d->op1));
+
+      emit_insn (gen_lasx_xvpermi_d_v8si (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    case E_V16HImode:
+      if (odd)
+	emit_insn (gen_lasx_xvpickod_h (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvpickev_h (t1, d->op0, d->op1));
+
+      emit_insn (gen_lasx_xvpermi_d_v16hi (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    case E_V32QImode:
+      if (odd)
+	emit_insn (gen_lasx_xvpickod_b (t1, d->op0, d->op1));
+      else
+	emit_insn (gen_lasx_xvpickev_b (t1, d->op0, d->op1));
+
+      emit_insn (gen_lasx_xvpermi_d_v32qi (d->target, t1, GEN_INT (0xd8)));
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return true;
+}
+
+/* Pattern match extract-even and extract-odd permutations.  */
+
+static bool
+loongarch_expand_vec_perm_even_odd (struct expand_vec_perm_d *d)
+{
+  unsigned i, odd, nelt = d->nelt;
+  if(!TARGET_LASX)
+    return false;
+
+  odd = d->perm[0];
+  if (odd != 0 && odd != 1)
+    return false;
+
+  for (i = 1; i < nelt; ++i)
+    if (d->perm[i] != 2 * i + odd)
+      return false;
+
+  return loongarch_expand_vec_perm_even_odd_1 (d, odd);
+}
+
+/* Expand a variable vector permutation for LASX.  */
+
+void
+loongarch_expand_vec_perm_1 (rtx operands[])
+{
+  rtx target = operands[0];
+  rtx op0 = operands[1];
+  rtx op1 = operands[2];
+  rtx mask = operands[3];
+  machine_mode vmode = GET_MODE (target);
+  bool one_operand_shuffle = rtx_equal_p (op0, op1);
+  rtx t1, t2, t3, t4, t5, t6, vt, vec[32];
+  machine_mode mode = GET_MODE (op0);
+  machine_mode maskmode = GET_MODE (mask);
+  int w, e, i;
+
+  /* Number of elements in the vector.  */
+  w = GET_MODE_NUNITS (mode);
+  e = GET_MODE_UNIT_SIZE (mode);
+
+  if (mode == V4DImode || mode == V4DFmode)
+    {
+      maskmode = mode = V8SImode;
+      w = 8;
+      e = 4;
+      t1 = gen_reg_rtx (maskmode);
+
+      /* Replicate the low bits of the V4DImode mask into V8SImode:
+	 mask = { A B C D }
+	 t1 = { A A B B C C D D }.  */
+      for (i = 0; i < w / 2; ++i)
+	vec[i*2 + 1] = vec[i*2] = GEN_INT (i * 2);
+      vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));
+      vt = force_reg (maskmode, vt);
+      mask = gen_lowpart (maskmode, mask);
+      emit_insn (gen_lasx_xvperm_w (t1, mask, vt));
+
+      /* Multiply the shuffle indicies by two.  */
+      t1 = expand_simple_binop (maskmode, PLUS, t1, t1, t1, 1,
+				OPTAB_DIRECT);
+
+      /* Add one to the odd shuffle indicies:
+	 t1 = { A*2, A*2+1, B*2, B*2+1, ... }.  */
+      for (i = 0; i < w / 2; ++i)
+	{
+	  vec[i * 2] = const0_rtx;
+	  vec[i * 2 + 1] = const1_rtx;
+	}
+      vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));
+      vt = validize_mem (force_const_mem (maskmode, vt));
+      t1 = expand_simple_binop (maskmode, PLUS, t1, vt, t1, 1,
+				OPTAB_DIRECT);
+
+      /* Continue as if V8SImode (resp. V32QImode) was used initially.  */
+      operands[3] = mask = t1;
+      target = gen_reg_rtx (mode);
+      op0 = gen_lowpart (mode, op0);
+      op1 = gen_lowpart (mode, op1);
+    }
+  switch (mode)
+    {
+    case E_V8SImode:
+      if (one_operand_shuffle)
+	{
+	  emit_insn (gen_lasx_xvperm_w (target, op0, mask));
+	  if (target != operands[0])
+	    emit_move_insn (operands[0],
+			    gen_lowpart (GET_MODE (operands[0]), target));
+	}
+      else
+	{
+	  t1 = gen_reg_rtx (V8SImode);
+	  t2 = gen_reg_rtx (V8SImode);
+	  emit_insn (gen_lasx_xvperm_w (t1, op0, mask));
+	  emit_insn (gen_lasx_xvperm_w (t2, op1, mask));
+	  goto merge_two;
+	}
+      return;
+
+    case E_V8SFmode:
+      mask = gen_lowpart (V8SImode, mask);
+      if (one_operand_shuffle)
+	emit_insn (gen_lasx_xvperm_w_f (target, op0, mask));
+      else
+	{
+	  t1 = gen_reg_rtx (V8SFmode);
+	  t2 = gen_reg_rtx (V8SFmode);
+	  emit_insn (gen_lasx_xvperm_w_f (t1, op0, mask));
+	  emit_insn (gen_lasx_xvperm_w_f (t2, op1, mask));
+	  goto merge_two;
+	}
+      return;
+
+    case E_V16HImode:
+      if (one_operand_shuffle)
+	{
+	  t1 = gen_reg_rtx (V16HImode);
+	  t2 = gen_reg_rtx (V16HImode);
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t1, op0, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t2, op0, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (target, mask, t2, t1));
+	}
+      else
+	{
+	  t1 = gen_reg_rtx (V16HImode);
+	  t2 = gen_reg_rtx (V16HImode);
+	  t3 = gen_reg_rtx (V16HImode);
+	  t4 = gen_reg_rtx (V16HImode);
+	  t5 = gen_reg_rtx (V16HImode);
+	  t6 = gen_reg_rtx (V16HImode);
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t3, op0, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t4, op0, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (t1, mask, t4, t3));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t5, op1, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v16hi (t6, op1, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_h (t2, mask, t6, t5));
+	  goto merge_two;
+	}
+      return;
+
+    case E_V32QImode:
+      if (one_operand_shuffle)
+	{
+	  t1 = gen_reg_rtx (V32QImode);
+	  t2 = gen_reg_rtx (V32QImode);
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t1, op0, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t2, op0, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (target, t2, t1, mask));
+	}
+      else
+	{
+	  t1 = gen_reg_rtx (V32QImode);
+	  t2 = gen_reg_rtx (V32QImode);
+	  t3 = gen_reg_rtx (V32QImode);
+	  t4 = gen_reg_rtx (V32QImode);
+	  t5 = gen_reg_rtx (V32QImode);
+	  t6 = gen_reg_rtx (V32QImode);
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t3, op0, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t4, op0, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (t1, t4, t3, mask));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t5, op1, GEN_INT(0x44)));
+	  emit_insn (gen_lasx_xvpermi_d_v32qi (t6, op1, GEN_INT(0xee)));
+	  emit_insn (gen_lasx_xvshuf_b (t2, t6, t5, mask));
+	  goto merge_two;
+	}
+      return;
+
+    default:
+      gcc_assert (GET_MODE_SIZE (mode) == 32);
+      break;
+    }
+
+merge_two:
+  /* Then merge them together.  The key is whether any given control
+     element contained a bit set that indicates the second word.  */
+  rtx xops[6];
+  mask = operands[3];
+  vt = GEN_INT (w);
+  vt = gen_const_vec_duplicate (maskmode, vt);
+  vt = force_reg (maskmode, vt);
+  mask = expand_simple_binop (maskmode, AND, mask, vt,
+			      NULL_RTX, 0, OPTAB_DIRECT);
+  if (GET_MODE (target) != mode)
+    target = gen_reg_rtx (mode);
+  xops[0] = target;
+  xops[1] = gen_lowpart (mode, t2);
+  xops[2] = gen_lowpart (mode, t1);
+  xops[3] = gen_rtx_EQ (maskmode, mask, vt);
+  xops[4] = mask;
+  xops[5] = vt;
+
+  loongarch_expand_vec_cond_expr (mode, maskmode, xops);
+  if (target != operands[0])
+    emit_move_insn (operands[0],
+		    gen_lowpart (GET_MODE (operands[0]), target));
+}
+
+void
+loongarch_expand_vec_perm (rtx target, rtx op0, rtx op1, rtx sel)
+{
+  machine_mode vmode = GET_MODE (target);
+
+  gcc_checking_assert (vmode == E_V16QImode
+      || vmode == E_V2DImode || vmode == E_V2DFmode
+      || vmode == E_V4SImode || vmode == E_V4SFmode
+      || vmode == E_V8HImode);
+  gcc_checking_assert (GET_MODE (op0) == vmode);
+  gcc_checking_assert (GET_MODE (op1) == vmode);
+  gcc_checking_assert (GET_MODE (sel) == vmode);
+  gcc_checking_assert (ISA_HAS_LSX);
+
+  switch (vmode)
+    {
+    case E_V16QImode:
       emit_insn (gen_lsx_vshuf_b (target, op1, op0, sel));
       break;
     case E_V2DFmode:
@@ -7101,6 +7359,80 @@ loongarch_expand_vec_perm (rtx target, rtx op0, rtx op1, rtx sel)
     }
 }
 
+static bool
+loongarch_try_expand_lsx_vshuf_const (struct expand_vec_perm_d *d)
+{
+  int i;
+  rtx target, op0, op1, sel, tmp;
+  rtx rperm[MAX_VECT_LEN];
+
+  if (d->vmode == E_V2DImode || d->vmode == E_V2DFmode
+	|| d->vmode == E_V4SImode || d->vmode == E_V4SFmode
+	|| d->vmode == E_V8HImode || d->vmode == E_V16QImode)
+    {
+      target = d->target;
+      op0 = d->op0;
+      op1 = d->one_vector_p ? d->op0 : d->op1;
+
+      if (GET_MODE (op0) != GET_MODE (op1)
+	  || GET_MODE (op0) != GET_MODE (target))
+	return false;
+
+      if (d->testing_p)
+	return true;
+
+      for (i = 0; i < d->nelt; i += 1)
+	{
+	  rperm[i] = GEN_INT (d->perm[i]);
+	}
+
+      if (d->vmode == E_V2DFmode)
+	{
+	  sel = gen_rtx_CONST_VECTOR (E_V2DImode, gen_rtvec_v (d->nelt, rperm));
+	  tmp = gen_rtx_SUBREG (E_V2DImode, d->target, 0);
+	  emit_move_insn (tmp, sel);
+	}
+      else if (d->vmode == E_V4SFmode)
+	{
+	  sel = gen_rtx_CONST_VECTOR (E_V4SImode, gen_rtvec_v (d->nelt, rperm));
+	  tmp = gen_rtx_SUBREG (E_V4SImode, d->target, 0);
+	  emit_move_insn (tmp, sel);
+	}
+      else
+	{
+	  sel = gen_rtx_CONST_VECTOR (d->vmode, gen_rtvec_v (d->nelt, rperm));
+	  emit_move_insn (d->target, sel);
+	}
+
+      switch (d->vmode)
+	{
+	case E_V2DFmode:
+	  emit_insn (gen_lsx_vshuf_d_f (target, target, op1, op0));
+	  break;
+	case E_V2DImode:
+	  emit_insn (gen_lsx_vshuf_d (target, target, op1, op0));
+	  break;
+	case E_V4SFmode:
+	  emit_insn (gen_lsx_vshuf_w_f (target, target, op1, op0));
+	  break;
+	case E_V4SImode:
+	  emit_insn (gen_lsx_vshuf_w (target, target, op1, op0));
+	  break;
+	case E_V8HImode:
+	  emit_insn (gen_lsx_vshuf_h (target, target, op1, op0));
+	  break;
+	case E_V16QImode:
+	  emit_insn (gen_lsx_vshuf_b (target, op1, op0, target));
+	  break;
+	default:
+	  break;
+	}
+
+      return true;
+    }
+  return false;
+}
+
 static bool
 loongarch_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)
 {
@@ -7131,13 +7463,17 @@ loongarch_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)
 
   if (loongarch_expand_lsx_shuffle (d))
     return true;
+  if (loongarch_expand_vec_perm_even_odd(d))
+    return true;
+  if (loongarch_expand_vec_perm_interleave(d))
+    return true;
   return false;
 }
 
+// Following are the assist function for const vector permutation support.
 static bool
-loongarch_is_same_vector_and_duplicate_operation_1 (struct expand_vec_perm_d *d)
+loongarch_is_quad_duplicate (struct expand_vec_perm_d *d)
 {
-  // Pattern example: E_V8SImode, { 0, 0, 0, 0, 4, 4, 4, 4 }
   if (d->perm[0] >= d->nelt / 2)
     return false;
 
@@ -7166,12 +7502,14 @@ loongarch_is_same_vector_and_duplicate_operation_1 (struct expand_vec_perm_d *d)
 }
 
 static bool
-loongarch_is_same_vector_and_duplicate_operation_2 (struct expand_vec_perm_d *d)
+loongarch_is_double_duplicate (struct expand_vec_perm_d *d)
 {
-  // Pattern example: E_V8SImode, { 1, 1, 3, 3, 5, 5, 7, 7 }
   if (!d->one_vector_p)
     return false;
 
+  if (d->nelt < 8)
+    return false;
+
   bool result = true;
   unsigned char buf = d->perm[0];
 
@@ -7193,6 +7531,44 @@ loongarch_is_same_vector_and_duplicate_operation_2 (struct expand_vec_perm_d *d)
   return result;
 }
 
+static bool
+loongarch_is_odd_extraction (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = 1;
+
+  for (int i = 0; i < d->nelt; i += 1)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 2;
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_even_extraction (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = 0;
+
+  for (int i = 0; i < d->nelt; i += 1)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 2;
+    }
+
+  return result;
+}
+
 static bool
 loongarch_is_extraction_permutation (struct expand_vec_perm_d *d)
 {
@@ -7215,40 +7591,35 @@ loongarch_is_extraction_permutation (struct expand_vec_perm_d *d)
   return result;
 }
 
-/* temporarily unused */
-/*
 static bool
-loongarch_is_duplicate_permutation (struct expand_vec_perm_d *d)
+loongarch_is_center_extraction (struct expand_vec_perm_d *d)
 {
   bool result = true;
-  unsigned char buf = d->perm[0];
+  unsigned buf = d->nelt / 2;
 
-  for (int i = 1; i < d->nelt; i += 1)
+  for (int i = 0; i < d->nelt; i += 1)
     {
-      if (d->perm[i] != buf)
+      if (buf != d->perm[i])
 	{
 	  result = false;
 	  break;
 	}
-
-      buf = d->perm[i];
+      buf += 1;
     }
 
   return result;
 }
-*/
 
 static bool
 loongarch_is_reversing_permutation (struct expand_vec_perm_d *d)
 {
-  //sample: E_V8SImode, {7,6,5,4,3,2,1,0}
   if (!d->one_vector_p)
     return false;
 
   bool result = true;
   unsigned char buf = d->nelt - 1;
 
-  for (int i = 1; i < d->nelt; i += 1)
+  for (int i = 0; i < d->nelt; i += 1)
     {
       if (d->perm[i] != buf)
 	{
@@ -7262,44 +7633,447 @@ loongarch_is_reversing_permutation (struct expand_vec_perm_d *d)
   return result;
 }
 
-/* This function will expand permutation operation that can be done
- * with single instruction. */
+static bool
+loongarch_is_di_misalign_extract (struct expand_vec_perm_d *d)
+{
+  if (d->nelt != 4 && d->nelt != 8)
+    return false;
+
+  bool result = true;
+  unsigned char buf;
+
+  if (d->nelt == 4)
+    {
+      buf = 1;
+      for (int i = 0; i < d->nelt; i += 1)
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+
+	  buf += 1;
+	}
+    }
+  else if (d->nelt == 8)
+    {
+      buf = 2;
+      for (int i = 0; i < d->nelt; i += 1)
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+
+	  buf += 1;
+	}
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_si_misalign_extract (struct expand_vec_perm_d *d)
+{
+  if (d->vmode != E_V8SImode && d->vmode != E_V8SFmode)
+    return false;
+  bool result = true;
+  unsigned char buf = 1;
+
+  for (int i = 0; i < d->nelt; i += 1)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 1;
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_lasx_lowpart_interleave (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = 0;
+
+  for (int i = 0;i < d->nelt; i += 2)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 1;
+    }
+
+  if (result)
+    {
+      buf = d->nelt;
+      for (int i = 1; i < d->nelt; i += 2)
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_lasx_lowpart_interleave_2 (struct expand_vec_perm_d *d)
+{
+  if (d->vmode != E_V32QImode)
+    return false;
+  bool result = true;
+  unsigned char buf = 0;
+
+#define COMPARE_SELECTOR(INIT, BEGIN, END) \
+  buf = INIT; \
+  for (int i = BEGIN; i < END && result; i += 1) \
+    { \
+      if (buf != d->perm[i]) \
+	{ \
+	  result = false; \
+	  break; \
+	} \
+      buf += 1; \
+    }
+
+  COMPARE_SELECTOR (0, 0, 8);
+  COMPARE_SELECTOR (32, 8, 16);
+  COMPARE_SELECTOR (8, 16, 24);
+  COMPARE_SELECTOR (40, 24, 32);
+
+#undef COMPARE_SELECTOR
+  return result;
+}
+
+static bool
+loongarch_is_lasx_lowpart_extract (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = 0;
+
+  for (int i = 0; i < d->nelt / 2; i += 1)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 1;
+    }
+
+  if (result)
+    {
+      buf = d->nelt;
+      for (int i = d->nelt / 2; i < d->nelt; i += 1)
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_lasx_highpart_interleave (expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = d->nelt / 2;
+
+  for (int i = 0; i < d->nelt; i += 2)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+      buf += 1;
+    }
+
+  if (result)
+    {
+      buf = d->nelt + d->nelt / 2;
+      for (int i = 1; i < d->nelt;i += 2)
+	{
+	  if (buf != d->perm[i])
+	    {
+	      result = false;
+	      break;
+	    }
+	  buf += 1;
+	}
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_lasx_highpart_interleave_2 (struct expand_vec_perm_d *d)
+{
+  if (d->vmode != E_V32QImode)
+    return false;
+
+  bool result = true;
+  unsigned char buf = 0;
+
+#define COMPARE_SELECTOR(INIT, BEGIN, END) \
+  buf = INIT; \
+  for (int i = BEGIN; i < END && result; i += 1) \
+    { \
+      if (buf != d->perm[i]) \
+	{ \
+	  result = false; \
+	  break; \
+	} \
+      buf += 1; \
+    }
+
+  COMPARE_SELECTOR (16, 0, 8);
+  COMPARE_SELECTOR (48, 8, 16);
+  COMPARE_SELECTOR (24, 16, 24);
+  COMPARE_SELECTOR (56, 24, 32);
+
+#undef COMPARE_SELECTOR
+  return result;
+}
+
+static bool
+loongarch_is_elem_duplicate (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+  unsigned char buf = d->perm[0];
+
+  for (int i = 0; i < d->nelt; i += 1)
+    {
+      if (buf != d->perm[i])
+	{
+	  result = false;
+	  break;
+	}
+    }
+
+  return result;
+}
+
+inline bool
+loongarch_is_op_reverse_perm (struct expand_vec_perm_d *d)
+{
+  return (d->vmode == E_V4DFmode)
+    && d->perm[0] == 2 && d->perm[1] == 3
+    && d->perm[2] == 0 && d->perm[3] == 1;
+}
+
+static bool
+loongarch_is_single_op_perm (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+
+  for (int i = 0; i < d->nelt; i += 1)
+    {
+      if (d->perm[i] >= d->nelt)
+	{
+	  result = false;
+	  break;
+	}
+    }
+
+  return result;
+}
+
+static bool
+loongarch_is_divisible_perm (struct expand_vec_perm_d *d)
+{
+  bool result = true;
+
+  for (int i = 0; i < d->nelt / 2; i += 1)
+    {
+      if (d->perm[i] >= d->nelt)
+	{
+	  result = false;
+	  break;
+	}
+    }
+
+  if (result)
+    {
+      for (int i = d->nelt / 2; i < d->nelt; i += 1)
+	{
+	  if (d->perm[i] < d->nelt)
+	    {
+	      result = false;
+	      break;
+	    }
+	}
+    }
+
+  return result;
+}
+
+inline bool
+loongarch_is_triple_stride_extract (struct expand_vec_perm_d *d)
+{
+  return (d->vmode == E_V4DImode || d->vmode == E_V4DFmode)
+    && d->perm[0] == 1 && d->perm[1] == 4
+    && d->perm[2] == 7 && d->perm[3] == 0;
+}
+
+/* In LASX, xvshuf.* insn does not have the behavior that gcc expects when
+ * compiler wants to emit a vector permutation.
+ *
+ * 1. What GCC provides via vectorize_vec_perm_const()'s paramater:
+ * When GCC wants to performs a vector permutation, it provides two op 
+ * reigster, one target register, and a selector. 
+ * In const vector permutation case, GCC provides selector as a char array 
+ * that contains original value; in variable vector permuatation
+ * (performs via vec_perm<mode> insn template), it provides a vector register.
+ * We assume that nelt is the elements numbers inside single vector in current
+ * 256bit vector mode.
+ *
+ * 2. What GCC expects to perform:
+ * Two op registers(op0, op1) will "combine" into a 512bit temp vector storage
+ * that has 2*nelt elements inside it; the low 256bit is op0, and high 256bit
+ * is op1, then the elements are indexed as below:
+ *               0 ~ nelt - 1          nelt ~ 2 * nelt - 1       
+ *       |-------------------------|-------------------------|
+ *             Low 256bit (op0)         High 256bit(op1)
+ * For example, the second element in op1(V8SImode) will be indexed with 9.
+ * Selector is a vector that has the same mode and number of elements  with
+ * op0,op1 and target, it's look like this:
+ *              0 ~ nelt - 1
+ *       |-------------------------|
+ *            256bit (selector)
+ * It describes which element from 512bit temp vector storage will fit into
+ * target's every element slot. 
+ * GCC expects that every element in selector can be ANY indices of 512bit
+ * vector storage(Selector can pick literally any element from op0 and op1, and
+ * then fits into any place of target register). This is also what LSX 128bit 
+ * vshuf.* instruction do similarly, so we can handle 128bit vector permutation
+ * by single instruction easily.
+ *
+ * 3. What xvshuf.* instruction does:
+ * In short, it just do TWO 128bit vector permuatation, it's the reason that we
+ * need to do these jobs. We will explain it.
+ * op0, op1, target, and selector will be separate into high 128bit and low
+ * 128bit, and do permutation as the description below:
+ *
+ *  a) op0's low 128bit and op1's low 128bit "combines" into a 256bit temp
+ * vector storage(TVS1), elements are indexed as below:
+ *         0 ~ nelt / 2 - 1     nelt / 2 ~ nelt - 1
+ *      |---------------------|---------------------| TVS1
+ *         op0's low 128bit      op1's low 128bit
+ *    op0's high 128bit and op1's high 128bit are "combined" into TVS2 in the
+ *    same way.
+ *         0 ~ nelt / 2 - 1     nelt / 2 ~ nelt - 1
+ *      |---------------------|---------------------| TVS2
+ *         op0's high 128bit      op1's high 128bit
+ *  b) Selector's low 128bit describes which elements from TVS1 will fit into
+ *  target vector's low 128bit. No TVS2 elements are allowed.
+ *  c) Selector's high 128bit describes which elements from TVS2 will fit into
+ *  target vector's high 128bit. No TVS1 elements are allowed.
+ *
+ * As we can see, if we want to handle vector permutation correctly, we can
+ * achieve it in three ways:
+ *  a) Modify selector's elements, to make sure that every elements can inform
+ *  correct value that will put into target vector.
+    b) Generate extra instruction before/after xvshuf.* instruction, for
+    adjusting op vector or target vector, to make sure target vector's value is
+    what GCC expects.
+    c) Use other instructions to process op and put correct result into target.
+ */
+
+/* Implementation of constant vector permuatation. This function identifies
+ * recognized pattern of permuation selector argument, and use one or more
+ * instruction(s) to finish the permutation job correctly. For unsupported
+ * patterns, it will return false.  */
 
 static bool
 loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
 {
+  // Although we have the LSX vec_perm<mode> template, there's still some
+  // 128bit vector permuatation operations send to vectorize_vec_perm_const.
+  // In this case, we just simpliy wrap them by single vshuf.* instruction,
+  // because LSX vshuf.* instruction just have the same behavior that GCC
+  // expects.
   if (d->vmode != E_V32QImode && d->vmode != E_V16HImode
       && d->vmode != E_V4DImode && d->vmode != E_V4DFmode
       && d->vmode != E_V8SImode && d->vmode != E_V8SFmode)
-    return false;
+    return loongarch_try_expand_lsx_vshuf_const (d);
 
-  bool ok = false, reverse_hi_lo = false;
+  bool ok = false, reverse_hi_lo = false, extract_ev_od = false,
+       use_alt_op = false;
   unsigned char idx;
   int i;
-  rtx target, op0, op1, sel_reg;
+  rtx target, op0, op1, sel, tmp;
+  rtx op0_alt = NULL_RTX, op1_alt = NULL_RTX;
   rtx rperm[MAX_VECT_LEN];
   unsigned char remapped[MAX_VECT_LEN];
 
-  /* Remapping the indices to correct place for xvshuf instructions. */
-  if (loongarch_is_same_vector_and_duplicate_operation_1 (d))
+  // Try to figure out whether is a recognized permutation selector pattern, if
+  // yes, we will reassign some elements with new value in selector argument,
+  // and in some cases we will generate some assist insn to complete the
+  // permutation. (Even in some cases, we use other insn to impl permutation
+  // instead of xvshuf!)
+
+  // Make sure to check d->testing_p is false everytime if you want to emit new
+  // insn, unless you want to crash into ICE directly.
+  if (loongarch_is_quad_duplicate (d))
     {
+      // Selector example: E_V8SImode, { 0, 0, 0, 0, 4, 4, 4, 4 }
+      // copy first elem from original selector to all elem in new selector.
       idx = d->perm[0];
       for (i = 0; i < d->nelt; i += 1)
 	{
 	  remapped[i] = idx;
 	}
+      // Selector after: { 0, 0, 0, 0, 0, 0, 0, 0 }
+    }
+  else if (loongarch_is_double_duplicate (d))
+    {
+      // Selector example: E_V8SImode, { 1, 1, 3, 3, 5, 5, 7, 7 }
+      // one_vector_p == true
+      for (i = 0; i < d->nelt / 2; i += 1)
+	{
+	  idx = d->perm[i];
+	  remapped[i] = idx;
+	  remapped[i + d->nelt / 2] = idx;
+	}
+      // Selector after: { 1, 1, 3, 3, 1, 1, 3, 3 }
     }
-  else if (loongarch_is_same_vector_and_duplicate_operation_2 (d))
+  else if (loongarch_is_odd_extraction (d)
+	   || loongarch_is_even_extraction (d))
     {
+      // Odd extraction selector sample: E_V4DImode, { 1, 3, 5, 7 }
+      // Selector after: { 1, 3, 1, 3 }
+      // Even extraction selector sample: E_V4DImode, { 0, 2, 4, 6 }
+      // Selector after: { 0, 2, 0, 2 }
       for (i = 0; i < d->nelt / 2; i += 1)
 	{
 	  idx = d->perm[i];
 	  remapped[i] = idx;
 	  remapped[i + d->nelt / 2] = idx;
 	}
+      // Additional insn is required for correct result. See codes below.
+      extract_ev_od = true;
     }
   else if (loongarch_is_extraction_permutation (d))
     {
+      // Selector sample: E_V8SImode, { 0, 1, 2, 3, 4, 5, 6, 7 }
       if (d->perm[0] == 0)
 	{
 	  for (i = 0; i < d->nelt / 2; i += 1)
@@ -7310,6 +8084,7 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
 	}
       else
 	{
+	  // { 8, 9, 10, 11, 12, 13, 14, 15 }
 	  for (i = 0; i < d->nelt / 2; i += 1)
 	    {
 	      idx = i + d->nelt / 2;
@@ -7317,9 +8092,49 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
 	      remapped[i + d->nelt / 2] = idx;
 	    }
 	}
+      // Selector after: { 0, 1, 2, 3, 0, 1, 2, 3 }
+      // { 8, 9, 10, 11, 8, 9, 10, 11 }
+    }
+  else if (loongarch_is_center_extraction (d))
+    {
+      // sample: E_V4DImode, { 2, 3, 4, 5 }
+      // In this condition, we can just copy high 128bit of op0 and low 128bit
+      // of op1 to the target register by using xvpermi.q insn.
+      if (!d->testing_p)
+	{
+	  emit_move_insn (d->target, d->op1);
+	  switch (d->vmode)
+	    {
+	      case E_V4DImode:
+		emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V4DFmode:
+		emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V8SImode:
+		emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V8SFmode:
+		emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V16HImode:
+		emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      case E_V32QImode:
+		emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target, d->op0, GEN_INT (0x21)));
+		break;
+	      default:
+		break;
+	    }
+	}
+      ok = true;
+      // Finish the funtion directly.
+      goto expand_perm_const_2_end;
     }
   else if (loongarch_is_reversing_permutation (d))
     {
+      // Selector sample: E_V8SImode, { 7, 6, 5, 4, 3, 2, 1, 0 }
+      // one_vector_p == true
       idx = d->nelt / 2 - 1;
       for (i = 0; i < d->nelt / 2; i += 1)
 	{
@@ -7327,139 +8142,599 @@ loongarch_expand_vec_perm_const_2 (struct expand_vec_perm_d *d)
 	  remapped[i + d->nelt / 2] = idx;
 	  idx -= 1;
 	}
-
+      // Selector after: { 3, 2, 1, 0, 3, 2, 1, 0 }
+      // Additional insn will be generated to swap hi and lo 128bit of target
+      // register.
       reverse_hi_lo = true;
     }
+  else if (loongarch_is_di_misalign_extract (d)
+	   || loongarch_is_si_misalign_extract (d))
+    {
+      // Selector Sample:
+      // DI misalign: E_V4DImode, { 1, 2, 3, 4 }
+      // SI misalign: E_V8SImode, { 1, 2, 3, 4, 5, 6, 7, 8 }
+      if (!d->testing_p)
+	{
+	  // Copy original op0/op1 value to new temp register.
+	  // In some cases, operand register may be used in multiple place, so
+	  // we need new regiter instead modify original one, to avoid runtime
+	  // crashing or wrong value after execution.
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  // Adjust op1 for selecting correct value in high 128bit of target
+	  // register.
+	  // op1: E_V4DImode, { 4, 5, 6, 7 } -> { 2, 3, 4, 5 }
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x21)));
+
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	      remapped[i + d->nelt / 2] = d->perm[i];
+	    }
+	  // Selector after:
+	  // DI misalign: { 1, 2, 1, 2 }
+	  // SI misalign: { 1, 2, 3, 4, 1, 2, 3, 4 }
+	}
+    }
+  else if (loongarch_is_lasx_lowpart_interleave (d))
+    {
+      // Elements from op0's low 18bit and op1's 128bit are inserted into
+      // target register alternately.
+      //sample: E_V4DImode, { 0, 4, 1, 5 }
+      if (!d->testing_p)
+	{
+	  // Prepare temp register instead of modify original op.
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  // Generate subreg for fitting into insn gen function.
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  
+	  // Adjust op value in temp register.
+	  // op0 = {0,1,2,3}, op1 = {4,5,0,1}
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x02)));
+	  // op0 = {0,1,4,5}, op1 = {4,5,0,1}
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+
+	  // Remap indices in selector based on the location of index inside
+	  // selector, and vector element numbers in current vector mode.
+	  
+	  // Filling low 128bit of new selector.
+	  for (i = 0; i < d->nelt / 2; i += 1)
+	    {
+	      // value in odd-indexed slot of low 128bit part of selector
+	      // vector.
+	      remapped[i] = i % 2 != 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
+	    }
+	  // Then filling the high 128bit.
+	  for (i = d->nelt / 2; i < d->nelt; i += 1)
+	    {
+	      // value in even-indexed slot of high 128bit part of 
+	      // selector vector.
+	      remapped[i] = i % 2 == 0 ? d->perm[i] + (d->nelt / 2) * 3 : d->perm[i];
+	    }
+	}
+    }
+  else if (loongarch_is_lasx_lowpart_interleave_2 (d))
+    {
+      // Special lowpart interleave case in V32QI vector mode. It does the same
+      // thing as we can see in if branch that above this line.
+      // Selector sample: E_V32QImode,
+      //	    {0, 1, 2, 3, 4, 5, 6, 7, 32, 33, 34, 35, 36, 37, 38, 39, 8, 9, 10,
+      //	    11, 12, 13, 14, 15, 40, 41, 42, 43, 44, 45, 46, 47}
+      if (!d->testing_p)
+	{
+	  // Solution for this case in very simple - covert op into V4DI mode,
+	  // and do same thing as previous if branch.
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x02)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+	  remapped[0] = 0;
+	  remapped[1] = 4;
+	  remapped[2] = 1;
+	  remapped[3] = 5;
+
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      rperm[i] = GEN_INT (remapped[i]);
+	    }
+
+	  sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v(4, rperm));
+	  sel = force_reg (E_V4DImode, sel);
+	  emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
+					conv_op1, conv_op0));
+	}
+
+      ok = true;
+      goto expand_perm_const_2_end;
+    }
+  else if (loongarch_is_lasx_lowpart_extract (d))
+    {
+      // Copy op0's low 128bit to target's low 128bit, and copy op1's low
+      // 128bit to target's high 128bit.
+      // Selector sample: E_V4DImode, { 0, 1, 4 ,5 }
+      if (!d->testing_p)
+	{
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  // We can achieve the expectation by using sinple xvpermi.q insn.
+	  emit_move_insn (conv_target, conv_op1);
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_target, conv_target,
+					      conv_op0, GEN_INT(0x20)));
+	}
+
+      ok = true;
+      goto expand_perm_const_2_end;
+    }
+  else if (loongarch_is_lasx_highpart_interleave (d))
+    {
+      // Similar to lowpart interleave, elements from op0's high 128bit and
+      // op1's high 128bit are inserted into target regiter alternately.
+      // Selector sample: E_V8SImode, { 4, 12, 5, 13, 6, 14, 7, 15 }
+      if (!d->testing_p)
+	{
+	  // Prepare temp op register.
+	  use_alt_op = true;
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  // Adjust op value in temp regiter.
+	  // op0 = { 0, 1, 2, 3 }, op1 = { 6, 7, 2, 3 }
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+					      conv_op0, GEN_INT (0x13)));
+	  // op0 = { 2, 3, 6, 7 }, op1 = { 6, 7, 2, 3 }
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+					      conv_op1, GEN_INT (0x01)));
+	  // Remap indices in selector based on the location of index inside
+	  // selector, and vector element numbers in current vector mode.
+	  
+	  // Filling low 128bit of new selector.
+	 for (i = 0; i < d->nelt / 2; i += 1)
+	   {
+	     // value in even-indexed slot of low 128bit part of selector
+	     // vector.
+	     remapped[i] = i % 2 == 0 ? d->perm[i] - d->nelt / 2 : d->perm[i];
+	   }
+	  // Then filling the high 128bit.
+	 for (i = d->nelt / 2; i < d->nelt; i += 1)
+	   {
+	     // value in odd-indexed slot of high 128bit part of selector
+	     // vector.
+	      remapped[i] = i % 2 != 0 ? d->perm[i] - (d->nelt / 2) * 3 : d->perm[i];
+	   }
+	}
+    }
+  else if (loongarch_is_lasx_highpart_interleave_2 (d))
+    {
+      // Special highpart interleave case in V32QI vector mode. It does the
+      // same thing as the normal version above.
+      // Selector sample: E_V32QImode,
+      //	  {16, 17, 18, 19, 20, 21, 22, 23, 48, 49, 50, 51, 52, 53, 54, 55, 24,
+      //	  25, 26, 27, 28, 29, 30, 31, 56, 57, 58, 59, 60, 61, 62, 63}
+      if (!d->testing_p)
+	{
+	  // Convert op into V4DImode and do the things.
+	  op1_alt = gen_reg_rtx (d->vmode);
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  emit_move_insn (op1_alt, d->op1);
+	  emit_move_insn (op0_alt, d->op0);
+
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_target = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1, conv_op1,
+	 				  conv_op0, GEN_INT (0x13)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0, conv_op0,
+	 				  conv_op1, GEN_INT (0x01)));
+	  remapped[0] = 2;
+	  remapped[1] = 6;
+	  remapped[2] = 3;
+	  remapped[3] = 7;
+
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      rperm[i] = GEN_INT (remapped[i]);
+	    }
+
+	  sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v(4, rperm));
+	  sel = force_reg (E_V4DImode, sel);
+	  emit_insn (gen_lasx_xvshuf_d (conv_target, sel,
+					conv_op1, conv_op0));
+	}
+
+	ok = true;
+	goto expand_perm_const_2_end;
+    }
+  else if (loongarch_is_elem_duplicate (d))
+    {
+      // Brocast single element (from op0 or op1) to all slot of target
+      // register.
+      // Selector sample:E_V8SImode, { 2, 2, 2, 2, 2, 2, 2, 2 }
+      if (!d->testing_p)
+	{
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx temp_reg = gen_reg_rtx (d->vmode);
+	  rtx conv_temp = gen_rtx_SUBREG (E_V4DImode, temp_reg, 0);
+
+	  emit_move_insn (temp_reg, d->op0);
+
+	  idx = d->perm[0];
+	  // We will use xvrepl128vei.* insn to achieve the result, but we need
+	  // to make the high/low 128bit has the same contents that contain the
+	  // value that we need to broardcast, because xvrepl128vei does the
+	  // broardcast job from every 128bit of source register to
+	  // corresponded part of target register! (A deep sigh.)
+	  if (/*idx >= 0 &&*/ idx < d->nelt / 2)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op0, GEN_INT (0x0)));
+	    }
+	  else if (idx >= d->nelt / 2 && idx < d->nelt)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op0, GEN_INT (0x11)));
+	      idx -= d->nelt / 2;
+	    }
+	  else if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op1, GEN_INT (0x0)));
+	    }
+	  else if (idx >= (d->nelt + d->nelt / 2) && idx < d->nelt * 2)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (conv_temp, conv_temp,
+						  conv_op1, GEN_INT (0x11)));
+	      idx -= d->nelt / 2;
+	    }
+
+	  // Then we can finally generate this insn.
+	  switch (d->vmode)
+	    {
+	    case E_V4DImode:
+	      emit_insn (gen_lasx_xvrepl128vei_d (d->target, temp_reg, GEN_INT (idx)));
+	      break;
+	    case E_V4DFmode:
+	      emit_insn (gen_lasx_xvrepl128vei_d_f (d->target, temp_reg, GEN_INT (idx)));
+	      break;
+	    case E_V8SImode:
+	      emit_insn (gen_lasx_xvrepl128vei_w (d->target, temp_reg, GEN_INT (idx)));
+	      break;
+	    case E_V8SFmode:
+	      emit_insn (gen_lasx_xvrepl128vei_w_f (d->target, temp_reg, GEN_INT (idx)));
+	      break;
+	    case E_V16HImode:
+	      emit_insn (gen_lasx_xvrepl128vei_h (d->target, temp_reg, GEN_INT (idx)));
+	      break;
+	    case E_V32QImode:
+	      emit_insn (gen_lasx_xvrepl128vei_b (d->target, temp_reg, GEN_INT(idx)));
+	      break;
+	    default:
+	      gcc_unreachable ();
+	      break;
+	    }
+
+	  // finish func directly.
+	  ok = true;
+	  goto expand_perm_const_2_end;
+	}
+    }
+  else if (loongarch_is_op_reverse_perm (d))
+    {
+      // reverse high 128bit and low 128bit in op0.
+      // Selector sample: E_V4DFmode, { 2, 3, 0, 1 }
+      // Use xvpermi.q for doing this job.
+      if (!d->testing_p)
+	{
+	  if (d->vmode == E_V4DImode)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->op0,
+						  GEN_INT (0x01)));
+	    }
+	  else if (d->vmode == E_V4DFmode)
+	    {
+	      emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->op0,
+						  GEN_INT (0x01)));
+	    }
+	  else
+	    {
+	      gcc_unreachable ();
+	    }
+	}
+
+      ok = true;
+      goto expand_perm_const_2_end;
+    }
+  else if (loongarch_is_single_op_perm (d))
+    {
+      //Permutation that only select elements from op0.
+      if (!d->testing_p)
+	{
+	  // Prepare temp register instead of modify original op.
+	  use_alt_op = true;
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  op1_alt = gen_reg_rtx (d->vmode);
+
+	  emit_move_insn (op0_alt, d->op0);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+
+	  // Duplicate op0's low 128bit in op0, then duplicate high 128bit
+	  // in op1. After this, xvshuf.* insn's selector argument can 
+	  // access all elements we need for correct permutation result.
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op0,
+					      GEN_INT (0x00)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
+					      GEN_INT (0x11)));
+
+	  // In this case, there's no need to remap selector's indices.
+	  for (i = 0; i < d->nelt; i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	    }
+	}
+    }
+  else if (loongarch_is_divisible_perm (d))
+    {
+      // Divisible perm:
+      // Low 128bit of selector only selects elements of op0,
+      // and high 128bit of selector only selects elements of op1.
+
+      if (!d->testing_p)
+	{
+	  // Prepare temp register instead of modify original op.
+	  use_alt_op = true;
+	  op0_alt = gen_reg_rtx (d->vmode);
+	  op1_alt = gen_reg_rtx (d->vmode);
+
+	  emit_move_insn (op0_alt, d->op0);
+	  emit_move_insn (op1_alt, d->op1);
+
+	  rtx conv_op0a = gen_rtx_SUBREG (E_V4DImode, op0_alt, 0);
+	  rtx conv_op1a = gen_rtx_SUBREG (E_V4DImode, op1_alt, 0);
+	  rtx conv_op0 = gen_rtx_SUBREG (E_V4DImode, d->op0, 0);
+	  rtx conv_op1 = gen_rtx_SUBREG (E_V4DImode, d->op1, 0);
+
+	  // Reorganize op0's hi/lo 128bit and op1's hi/lo 128bit, to make sure
+	  //that selector's low 128bit can access all op0's elements, and
+	  //selector's high 128bit can access all op1's elements.
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op0a, conv_op0a, conv_op1,
+					      GEN_INT (0x02)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (conv_op1a, conv_op1a, conv_op0,
+					      GEN_INT (0x31)));
+
+	  // No need to modify indices.
+	  for (i = 0; i < d->nelt;i += 1)
+	    {
+	      remapped[i] = d->perm[i];
+	    }
+	}
+    }
+  else if (loongarch_is_triple_stride_extract (d))
+    {
+      // Selector sample: E_V4DFmode, { 1, 4, 7, 0 }
+      if (!d->testing_p)
+	{
+	  // Resolve it with brute force modification.
+	  remapped[0] = 1;
+	  remapped[1] = 2;
+	  remapped[2] = 3;
+	  remapped[3] = 0;
+	}
+    }
   else
     {
+      // When all of the detections above are failed, we will try last
+      // strategy.
+      // The for loop tries to detect following rules based on indices' value
+      // , its position inside of selector vector ,and strange behavior of xvshuf.* insn;
+      // Then we take corresponding action. (Replace with new value, or give up 
+      // whole permutation expansion.)
       for (i = 0; i < d->nelt; i += 1)
 	{
-	  idx = d->perm[i] % (2 * d->nelt);
+	  idx = d->perm[i]/* % (2 * d->nelt)*/;
 
+	  // if index is located in low 128bit of selector vector
 	  if (i < d->nelt / 2)
 	    {
+	      // Fail case 1: index tries to reach element that located in op0's
+	      // high 128bit.
 	      if (idx >= d->nelt / 2 && idx < d->nelt)
 		{
 		  goto expand_perm_const_2_end;
 		}
+	      // Fail case 2: index tries to reach element that located in
+	      // op1's high 128bit.
 	      if (idx >= (d->nelt + d->nelt / 2))
 		{
 		  goto expand_perm_const_2_end;
 		}
 
+	      // Success case: index tries to reach elements that located in
+	      // op1's low 128bit. Apply - (nelt / 2) offset to original value.
 	      if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
 		{
 		  idx -= d->nelt / 2;
 		}
-
 	    }
+	  // if index is located in high 128bit of selector vector
 	  else
 	    {
+	      // Fail case 1: index tries to reach element that located in
+	      // op1's low 128bit.
 	      if (idx >= d->nelt && idx < (d->nelt + d->nelt / 2))
 		{
 		  goto expand_perm_const_2_end;
 		}
+	      // Fail case 2: index tries to reach element that located in
+	      // op0's low 128bit.
 	      if (idx < (d->nelt / 2))
 		{
 		  goto expand_perm_const_2_end;
 		}
-
+	      // Success case: index tries to reach element that located in
+	      // op0's high 128bit.
 	      if (idx >= d->nelt / 2 && idx < d->nelt)
 		{
 		  idx -= d->nelt / 2;
 		}
 	    }
+	  // No need to process other case that we did not mentioned.
 
+	  // Assign with original or processed value.
 	  remapped[i] = idx;
 	}
     }
 
+  ok = true;
+  // If testing_p is true, compiler is trying to figure out that backend can
+  // handle this permutation, but doesn't want to generate actual insn. So if
+  // true, exit directly.
+  if (d->testing_p)
+    {
+      goto expand_perm_const_2_end;
+    }
+
+  // Convert remapped selector array to RTL array.
   for (i = 0; i < d->nelt; i += 1)
     {
-      rperm[i] = GEN_INT(remapped[i]);
+      rperm[i] = GEN_INT (remapped[i]);
     }
 
+  // Copy selector vector from memory to vector regiter for later insn gen
+  // function.
+  // if vector's element in floating point value, we cannot fit selector
+  // argument into insn gen function directly, because of the insn template
+  // definition. As a solution, generate a integral mode subreg of target,
+  // then copy selector vector(that is in integral mode) to this subreg.
   switch (d->vmode)
     {
     case E_V4DFmode:
-      sel_reg = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v(d->nelt, rperm));
-      sel_reg = force_reg (E_V4DImode, sel_reg);
-      sel_reg = gen_lowpart_SUBREG (E_V4DFmode, sel_reg);
+      sel = gen_rtx_CONST_VECTOR (E_V4DImode, gen_rtvec_v (d->nelt, rperm));
+      tmp = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+      emit_move_insn (tmp, sel);
       break;
     case E_V8SFmode:
-      sel_reg = gen_rtx_CONST_VECTOR (E_V8SImode, gen_rtvec_v(d->nelt, rperm));
-      sel_reg = force_reg (E_V8SImode, sel_reg);
-      sel_reg = gen_lowpart_SUBREG (E_V8SFmode, sel_reg);
+      sel = gen_rtx_CONST_VECTOR (E_V8SImode, gen_rtvec_v (d->nelt, rperm));
+      tmp = gen_rtx_SUBREG (E_V8SImode, d->target, 0);
+      emit_move_insn (tmp, sel);
       break;
     default:
-      sel_reg = gen_rtx_CONST_VECTOR (d->vmode, gen_rtvec_v(d->nelt, rperm));
-      sel_reg = force_reg (d->vmode, sel_reg);
+      sel = gen_rtx_CONST_VECTOR (d->vmode, gen_rtvec_v (d->nelt, rperm));
+      emit_move_insn (d->target, sel);
       break;
     }
 
   target = d->target;
-  op0 = d->op0;
-  op1 = d->one_vector_p ? d->op0 : d->op1;
+  // If temp op registers are requested in previous if branch, then use temp
+  // register intead of original one.
+  if (use_alt_op)
+    {
+      op0 = op0_alt != NULL_RTX ? op0_alt : d->op0;
+      op1 = op1_alt != NULL_RTX ? op1_alt : d->op1;
+    }
+  else
+    {
+      op0 = d->op0;
+      op1 = d->one_vector_p ? d->op0 : d->op1;
+    }
 
+  // We FINALLY can generate xvshuf.* insn.
   switch (d->vmode)
     {
     case E_V4DFmode:
-      emit_insn (gen_lasx_xvshuf_d_f (target, sel_reg, op1, op0));
+      emit_insn (gen_lasx_xvshuf_d_f (target, target, op1, op0));
       break;
     case E_V4DImode:
-      emit_insn (gen_lasx_xvshuf_d (target, sel_reg, op1, op0));
+      emit_insn (gen_lasx_xvshuf_d (target, target, op1, op0));
       break;
     case E_V8SFmode:
-      emit_insn (gen_lasx_xvshuf_w_f (target, sel_reg, op1, op0));
+      emit_insn (gen_lasx_xvshuf_w_f (target, target, op1, op0));
       break;
     case E_V8SImode:
-      emit_insn (gen_lasx_xvshuf_w (target, sel_reg, op1, op0));
+      emit_insn (gen_lasx_xvshuf_w (target, target, op1, op0));
       break;
     case E_V16HImode:
-      emit_insn (gen_lasx_xvshuf_h (target, sel_reg, op1, op0));
+      emit_insn (gen_lasx_xvshuf_h (target, target, op1, op0));
       break;
     case E_V32QImode:
-      emit_insn (gen_lasx_xvshuf_b (target, op1, op0, sel_reg));
+      emit_insn (gen_lasx_xvshuf_b (target, op1, op0, target));
       break;
     default:
+      gcc_unreachable ();
       break;
     }
 
+  // extra insn for swapping the hi/lo 128bit of target vector register.
   if (reverse_hi_lo)
     {
       switch (d->vmode)
 	{
 	case E_V4DFmode:
-	  emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v4df (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	case E_V4DImode:
-	  emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v4di (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	case E_V8SFmode:
-	  emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v8sf (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	case E_V8SImode:
-	  emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v8si (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	case E_V16HImode:
-	  emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v16hi (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	case E_V32QImode:
-	  emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target, d->target, GEN_INT(0x1)));
+	  emit_insn (gen_lasx_xvpermi_q_v32qi (d->target, d->target, d->target, GEN_INT (0x1)));
 	  break;
 	default:
 	  break;
 	}
     }
+  // extra insn required by odd/even extraction. Swapping the second and third
+  // 64bit in target vector register.
+  else if (extract_ev_od)
+    {
+      rtx converted = gen_rtx_SUBREG (E_V4DImode, d->target, 0);
+      emit_insn (gen_lasx_xvpermi_d_v4di (converted, converted, GEN_INT (0xD8)));
+    }
 
-  ok = true;
 expand_perm_const_2_end:
   return ok;
 }
 
-
-
-
 /* Implement TARGET_VECTORIZE_VEC_PERM_CONST.  */
 
 static bool
@@ -7522,12 +8797,9 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode, rtx target, rtx op0,
       if (!d.one_vector_p)
 	d.op1 = gen_raw_REG (d.vmode, LAST_VIRTUAL_REGISTER + 3);
 
-      if (ISA_HAS_LASX)
-	{
-	  ok = loongarch_expand_vec_perm_const_2 (&d);
-	  if (ok)
-	    return ok;
-	}
+      ok = loongarch_expand_vec_perm_const_2 (&d);
+      if (ok)
+	return ok;
 
       start_sequence ();
       ok = loongarch_expand_vec_perm_const_1 (&d);
@@ -7535,16 +8807,9 @@ loongarch_vectorize_vec_perm_const (machine_mode vmode, rtx target, rtx op0,
       return ok;
     }
 
-  if (ISA_HAS_LASX)
-    {
-      ok = loongarch_expand_vec_perm_const_2 (&d);
-      if (!ok)
-	ok = loongarch_expand_vec_perm_const_1 (&d);
-    }
-  else
-    {
+    ok = loongarch_expand_vec_perm_const_2 (&d);
+    if (!ok)
       ok = loongarch_expand_vec_perm_const_1 (&d);
-    }
 
   /* If we were given a two-vector permutation which just happened to
      have both input vectors equal, we folded this into a one-vector
@@ -7595,6 +8860,152 @@ loongarch_sched_reassociation_width (unsigned int opc ATTRIBUTE_UNUSED,
   return 1;
 }
 
+/* Implement extract a scalar element from vecotr register */
+
+void
+loongarch_expand_vector_extract (rtx target, rtx vec, int elt)
+{
+  machine_mode mode = GET_MODE (vec);
+  machine_mode inner_mode = GET_MODE_INNER (mode);
+  rtx tmp;
+
+  switch (mode)
+    {
+    case E_V8HImode:
+    case E_V16QImode:
+      break;
+
+    case E_V32QImode:
+      if (TARGET_LASX)
+        {
+          if (elt >= 16)
+            {
+              tmp = gen_reg_rtx (V32QImode);
+              emit_insn (gen_lasx_xvpermi_d_v32qi (tmp, vec, GEN_INT (0xe)));
+              loongarch_expand_vector_extract (target, gen_lowpart (V16QImode, tmp), elt & 15);
+            }
+          else
+            loongarch_expand_vector_extract (target, gen_lowpart (V16QImode, vec), elt & 15);
+          return;
+        }
+      break;
+
+    case E_V16HImode:
+      if (TARGET_LASX)
+        {
+          if (elt >= 8)
+            {
+              tmp = gen_reg_rtx (V16HImode);
+              emit_insn (gen_lasx_xvpermi_d_v16hi (tmp, vec, GEN_INT (0xe)));
+              loongarch_expand_vector_extract (target, gen_lowpart (V8HImode, tmp), elt & 7);
+            }
+          else
+            loongarch_expand_vector_extract (target, gen_lowpart (V8HImode, vec), elt & 7);
+	  return;
+        }
+      break;
+
+    default:
+      break;
+    }
+
+  tmp = gen_rtx_PARALLEL (VOIDmode, gen_rtvec (1, GEN_INT (elt)));
+  tmp = gen_rtx_VEC_SELECT (inner_mode, vec, tmp);
+
+  /* Let the rtl optimizers know about the zero extension performed.  */
+  if (inner_mode == QImode || inner_mode == HImode)
+    {
+      tmp = gen_rtx_ZERO_EXTEND (SImode, tmp);
+      target = gen_lowpart (SImode, target);
+    }
+  if (inner_mode == SImode || inner_mode == DImode)
+    {
+      tmp = gen_rtx_SIGN_EXTEND (inner_mode, tmp);
+    }
+
+  emit_insn (gen_rtx_SET (target, tmp));
+}
+
+/* Generate code to copy vector bits i / 2 ... i - 1 from vector SRC
+   to bits 0 ... i / 2 - 1 of vector DEST, which has the same mode.
+   The upper bits of DEST are undefined, though they shouldn't cause
+   exceptions (some bits from src or all zeros are ok).  */
+
+static void
+emit_reduc_half (rtx dest, rtx src, int i)
+{
+  rtx tem, t, d = dest;
+  switch (GET_MODE (src))
+    {
+    case E_V4SFmode:
+      tem = gen_lsx_vbsrl_w_f (dest, src, GEN_INT (i == 128 ? 8 : 4));
+      break;
+    case E_V2DFmode:
+      tem = gen_lsx_vbsrl_d_f (dest, src, GEN_INT (8));
+      break;
+    case E_V8SFmode:
+      if (i == 256)
+        tem = gen_lasx_xvpermi_d_v8sf (dest, src, GEN_INT (0xe));
+      else
+        tem = gen_lasx_xvshuf4i_w_f (dest, src,
+                                     GEN_INT (i == 128 ? 2 + (3 << 2) : 1));
+      break;
+    case E_V4DFmode:
+      if (i == 256)
+        tem = gen_lasx_xvpermi_d_v4df (dest, src, GEN_INT (0xe));
+      else
+        tem = gen_lasx_xvpermi_d_v4df (dest, src, const1_rtx);
+      break;
+    case E_V32QImode:
+    case E_V16HImode:
+    case E_V8SImode:
+    case E_V4DImode:
+      d = gen_reg_rtx (V4DImode);
+      if (i == 256)
+        tem = gen_lasx_xvpermi_d_v4di (d, gen_lowpart (V4DImode, src), GEN_INT (0xe));
+      else
+        tem = gen_lasx_xvbsrl_d (d, gen_lowpart (V4DImode, src), GEN_INT (i/16));
+      break;
+    case E_V16QImode:
+    case E_V8HImode:
+    case E_V4SImode:
+    case E_V2DImode:
+      d = gen_reg_rtx (V2DImode);
+      tem = gen_lsx_vbsrl_d (d, gen_lowpart (V2DImode, src), GEN_INT (i/16));
+      break;
+    default:
+      gcc_unreachable ();
+    }
+  emit_insn (tem);
+  if (d != dest)
+    emit_move_insn (dest, gen_lowpart (GET_MODE (dest), d));
+}
+
+/* Expand a vector reduction.  FN is the binary pattern to reduce;
+   DEST is the destination; IN is the input vector.  */
+
+void
+loongarch_expand_vector_reduc (rtx (*fn) (rtx, rtx, rtx), rtx dest, rtx in)
+{
+  rtx half, dst, vec = in;
+  machine_mode mode = GET_MODE (in);
+  int i;
+
+  for (i = GET_MODE_BITSIZE (mode);
+       i > GET_MODE_UNIT_BITSIZE (mode);
+       i >>= 1)
+    {
+      half = gen_reg_rtx (mode);
+      emit_reduc_half (half, vec, i);
+      if (i == GET_MODE_UNIT_BITSIZE (mode) * 2)
+        dst = dest;
+      else
+        dst = gen_reg_rtx (mode);
+      emit_insn (fn (dst, half, vec));
+      vec = dst;
+    }
+}
+
 /* Expand an integral vector unpack operation.  */
 
 void
@@ -7612,7 +9023,7 @@ loongarch_expand_vec_unpack (rtx operands[2], bool unsigned_p, bool high_p)
     {
       switch (imode)
 	{
-      
+
 	case E_V8SImode:
 	  if (unsigned_p)
 	    extend = gen_lasx_vext2xv_du_wu;
@@ -7620,7 +9031,7 @@ loongarch_expand_vec_unpack (rtx operands[2], bool unsigned_p, bool high_p)
 	    extend = gen_lasx_vext2xv_d_w;
 	  swap_hi_lo = gen_lasx_xvpermi_q_v8si;
 	  break;
-      
+
 	case E_V16HImode:
 	  if (unsigned_p)
 	    extend = gen_lasx_vext2xv_wu_hu;
@@ -7628,7 +9039,7 @@ loongarch_expand_vec_unpack (rtx operands[2], bool unsigned_p, bool high_p)
 	    extend = gen_lasx_vext2xv_w_h;
 	  swap_hi_lo = gen_lasx_xvpermi_q_v16hi;
 	  break;
-      
+
 	case E_V32QImode:
 	  if (unsigned_p)
 	    extend = gen_lasx_vext2xv_hu_bu;
@@ -7636,7 +9047,7 @@ loongarch_expand_vec_unpack (rtx operands[2], bool unsigned_p, bool high_p)
 	    extend = gen_lasx_vext2xv_h_b;
 	  swap_hi_lo = gen_lasx_xvpermi_q_v32qi;
 	  break;
-      
+
 	default:
 	  gcc_unreachable ();
 	  break;
@@ -7806,7 +9217,7 @@ loongarch_expand_vector_init (rtx target, rtx vals)
 		temp2 = same;
 	      else if (GET_MODE_SIZE (imode) >= UNITS_PER_WORD)
 		{
-		  if(GET_CODE (same) == MEM)
+		  if (GET_CODE (same) == MEM)
 		    {
 		      rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
 		      loongarch_emit_move (reg_tmp, same);
@@ -7817,7 +9228,7 @@ loongarch_expand_vector_init (rtx target, rtx vals)
 		}
 	      else
 		{
-		  if(GET_CODE (same) == MEM)
+		  if (GET_CODE (same) == MEM)
 		    {
 		      rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
 		      loongarch_emit_move (reg_tmp, same);
@@ -8000,7 +9411,7 @@ loongarch_expand_vector_init (rtx target, rtx vals)
 	    temp2 = same;
           else if (GET_MODE_SIZE (imode) >= UNITS_PER_WORD)
             {
-              if(GET_CODE (same) == MEM)
+              if (GET_CODE (same) == MEM)
                 {
                   rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
                   loongarch_emit_move (reg_tmp, same);
@@ -8011,7 +9422,7 @@ loongarch_expand_vector_init (rtx target, rtx vals)
             }
           else
             {
-              if(GET_CODE (same) == MEM)
+              if (GET_CODE (same) == MEM)
                 {
                   rtx reg_tmp = gen_reg_rtx (GET_MODE (same));
                   loongarch_emit_move (reg_tmp, same);
@@ -8324,7 +9735,7 @@ loongarch_expand_vec_cond_expr (machine_mode mode, machine_mode vimode,
 
 /* Expand integer vector comparison */
 bool
-loongarch_expand_int_vec_cmp(rtx operands[])
+loongarch_expand_int_vec_cmp (rtx operands[])
 {
 
   rtx_code code = GET_CODE (operands[1]);
@@ -8334,7 +9745,7 @@ loongarch_expand_int_vec_cmp(rtx operands[])
 
 /* Expand integer vector comparison */
 bool
-loongarch_expand_fp_vec_cmp(rtx operands[])
+loongarch_expand_fp_vec_cmp (rtx operands[])
 {
   rtx_code code = GET_CODE (operands[1]);
   loongarch_expand_lsx_cmp (operands[0], code, operands[2], operands[3]);
@@ -8342,14 +9753,6 @@ loongarch_expand_fp_vec_cmp(rtx operands[])
 }
 
 
-/* Implement TARGET_CASE_VALUES_THRESHOLD.  */
-
-unsigned int
-loongarch_case_values_threshold (void)
-{
-  return default_case_values_threshold ();
-}
-
 /* Implement TARGET_SPILL_CLASS.  */
 
 static reg_class_t
@@ -8359,36 +9762,6 @@ loongarch_spill_class (reg_class_t rclass ATTRIBUTE_UNUSED,
   return NO_REGS;
 }
 
-/* Implement TARGET_IRA_CHANGE_PSEUDO_ALLOCNO_CLASS.  */
-
-static reg_class_t
-loongarch_ira_change_pseudo_allocno_class (int regno,
-					   reg_class_t allocno_class,
-					   reg_class_t best_class     \
-					   ATTRIBUTE_UNUSED)
-{
-  /* LRA will allocate an FPR for an integer mode pseudo instead of spilling
-     to memory if an FPR is present in the allocno class.  It is rare that
-     we actually need to place an integer mode value in an FPR so where
-     possible limit the allocation to GR_REGS.  This will slightly pessimize
-     code that involves integer to/from float conversions as these will have
-     to reload into FPRs in LRA.  Such reloads are sometimes eliminated and
-     sometimes only partially eliminated.  We choose to take this penalty
-     in order to eliminate usage of FPRs in code that does not use floating
-     point data.
-
-     This change has a similar effect to increasing the cost of FPR->GPR
-     register moves for integer modes so that they are higher than the cost
-     of memory but changing the allocno class is more reliable.
-
-     This is also similar to forbidding integer mode values in FPRs entirely
-     but this would lead to an inconsistency in the integer to/from float
-     instructions that say integer mode values must be placed in FPRs.  */
-  if (INTEGRAL_MODE_P (PSEUDO_REGNO_MODE (regno)) && allocno_class == ALL_REGS)
-    return GR_REGS;
-  return allocno_class;
-}
-
 /* Implement TARGET_PROMOTE_FUNCTION_MODE.  */
 
 /* This function is equivalent to default_promote_function_mode_always_promote
@@ -8423,16 +9796,6 @@ loongarch_truly_noop_truncation (poly_uint64 outprec, poly_uint64 inprec)
   return !TARGET_64BIT || inprec <= 32 || outprec > 32;
 }
 
-/* Implement TARGET_CONSTANT_ALIGNMENT.  */
-
-static HOST_WIDE_INT
-loongarch_constant_alignment (const_tree exp, HOST_WIDE_INT align)
-{
-  if (TREE_CODE (exp) == STRING_CST || TREE_CODE (exp) == CONSTRUCTOR)
-    return MAX (align, BITS_PER_WORD);
-  return align;
-}
-
 /* Implement TARGET_STARTING_FRAME_OFFSET.  See loongarch_compute_frame_info
    for details about the frame layout.  */
 
@@ -8530,7 +9893,7 @@ loongarch_la464_128_load_p (rtx operands[])
   int base_reg0;
   int base_reg1;
   int dst_reg0;
-  
+
   dst_reg0 = REGNO (dst0);
 
   if (GET_CODE (XEXP (src0, 0)) == PLUS)
@@ -8756,7 +10119,11 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST
 #define TARGET_VECTORIZE_BUILTIN_VECTORIZATION_COST \
   loongarch_builtin_vectorization_cost
+#undef TARGET_VECTORIZE_ADD_STMT_COST
+#define TARGET_VECTORIZE_ADD_STMT_COST loongarch_add_stmt_cost
 
+#undef TARGET_MODE_REP_EXTENDED
+#define TARGET_MODE_REP_EXTENDED loongarch_mode_rep_extended
 
 #undef TARGET_IN_SMALL_DATA_P
 #define TARGET_IN_SMALL_DATA_P loongarch_in_small_data_p
@@ -8764,10 +10131,6 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_PREFERRED_RELOAD_CLASS
 #define TARGET_PREFERRED_RELOAD_CLASS loongarch_preferred_reload_class
 
-#undef TARGET_EXPAND_TO_RTL_HOOK
-#define TARGET_EXPAND_TO_RTL_HOOK loongarch_expand_to_rtl_hook
-#undef TARGET_ASM_FILE_START
-#define TARGET_ASM_FILE_START loongarch_file_start
 #undef TARGET_ASM_FILE_START_FILE_DIRECTIVE
 #define TARGET_ASM_FILE_START_FILE_DIRECTIVE true
 
@@ -8779,6 +10142,11 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_RETURN_IN_MEMORY
 #define TARGET_RETURN_IN_MEMORY loongarch_return_in_memory
 
+#undef TARGET_FUNCTION_VALUE
+#define TARGET_FUNCTION_VALUE loongarch_function_value
+#undef TARGET_LIBCALL_VALUE
+#define TARGET_LIBCALL_VALUE loongarch_libcall_value
+
 #undef TARGET_ASM_OUTPUT_MI_THUNK
 #define TARGET_ASM_OUTPUT_MI_THUNK loongarch_output_mi_thunk
 #undef TARGET_ASM_CAN_OUTPUT_MI_THUNK
@@ -8842,8 +10210,6 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_LEGITIMATE_CONSTANT_P
 #define TARGET_LEGITIMATE_CONSTANT_P loongarch_legitimate_constant_p
 
-#undef TARGET_ATTRIBUTE_TABLE
-#define TARGET_ATTRIBUTE_TABLE loongarch_attribute_table
 /* All our function attributes are related to how out-of-line copies should
    be compiled or called.  They don't in themselves prevent inlining.  */
 #undef TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P
@@ -8851,15 +10217,11 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 
 #undef TARGET_USE_BLOCKS_FOR_CONSTANT_P
 #define TARGET_USE_BLOCKS_FOR_CONSTANT_P hook_bool_mode_const_rtx_true
-#undef TARGET_USE_ANCHORS_FOR_SYMBOL_P
-#define TARGET_USE_ANCHORS_FOR_SYMBOL_P loongarch_use_anchors_for_symbol_p
 
 #ifdef HAVE_AS_DTPRELWORD
 #undef TARGET_ASM_OUTPUT_DWARF_DTPREL
 #define TARGET_ASM_OUTPUT_DWARF_DTPREL loongarch_output_dwarf_dtprel
 #endif
-#undef TARGET_DWARF_FRAME_REG_MODE
-#define TARGET_DWARF_FRAME_REG_MODE loongarch_dwarf_frame_reg_mode
 
 #undef TARGET_LEGITIMATE_ADDRESS_P
 #define TARGET_LEGITIMATE_ADDRESS_P loongarch_legitimate_address_p
@@ -8876,18 +10238,12 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_TRAMPOLINE_INIT
 #define TARGET_TRAMPOLINE_INIT loongarch_trampoline_init
 
-#undef TARGET_SHIFT_TRUNCATION_MASK
-#define TARGET_SHIFT_TRUNCATION_MASK loongarch_shift_truncation_mask
-
 #undef TARGET_VECTORIZE_VEC_PERM_CONST
 #define TARGET_VECTORIZE_VEC_PERM_CONST loongarch_vectorize_vec_perm_const
 
 #undef TARGET_SCHED_REASSOCIATION_WIDTH
 #define TARGET_SCHED_REASSOCIATION_WIDTH loongarch_sched_reassociation_width
 
-#undef TARGET_CASE_VALUES_THRESHOLD
-#define TARGET_CASE_VALUES_THRESHOLD loongarch_case_values_threshold
-
 #undef TARGET_ATOMIC_ASSIGN_EXPAND_FENV
 #define TARGET_ATOMIC_ASSIGN_EXPAND_FENV loongarch_atomic_assign_expand_fenv
 
@@ -8896,9 +10252,6 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 
 #undef TARGET_SPILL_CLASS
 #define TARGET_SPILL_CLASS loongarch_spill_class
-#undef TARGET_IRA_CHANGE_PSEUDO_ALLOCNO_CLASS
-#define TARGET_IRA_CHANGE_PSEUDO_ALLOCNO_CLASS \
-  loongarch_ira_change_pseudo_allocno_class
 
 #undef TARGET_HARD_REGNO_NREGS
 #define TARGET_HARD_REGNO_NREGS loongarch_hard_regno_nregs
@@ -8915,9 +10268,6 @@ loongarch_build_signbit_mask (machine_mode mode, bool vect, bool invert)
 #undef TARGET_CUSTOM_FUNCTION_DESCRIPTORS
 #define TARGET_CUSTOM_FUNCTION_DESCRIPTORS 2
 
-#undef TARGET_SECONDARY_MEMORY_NEEDED
-#define TARGET_SECONDARY_MEMORY_NEEDED loongarch_secondary_memory_needed
-
 #undef TARGET_CAN_CHANGE_MODE_CLASS
 #define TARGET_CAN_CHANGE_MODE_CLASS loongarch_can_change_mode_class
 
diff --git a/src/gcc/config/loongarch/loongarch.h b/src/gcc/config/loongarch/loongarch.h
index 656b6405a..a3f95eda9 100644
--- a/src/gcc/config/loongarch/loongarch.h
+++ b/src/gcc/config/loongarch/loongarch.h
@@ -1,5 +1,5 @@
 /* Definitions of target machine for GNU compiler.  LoongArch version.
-   Copyright (C) 2021 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Technology Co. Ltd.
    Based on MIPS and RISC-V target for GNU compiler.
 
@@ -47,12 +47,6 @@ along with GCC; see the file COPYING3.  If not see
 
 #define TARGET_LIBGCC_SDATA_SECTION ".sdata"
 
-/* Support for a compile-time default CPU, et cetera.  The rules are:
-   --with-divide is ignored if -mdivide-traps or -mdivide-breaks are
-     specified.  */
-#define OPTION_DEFAULT_SPECS \
-  {"divide", "%{!mdivide-traps:%{!mdivide-breaks:-mdivide-%(VALUE)}}"},
-
 /* Driver native functions for SPEC processing in the GCC driver.  */
 #include "loongarch-driver.h"
 
@@ -68,17 +62,6 @@ along with GCC; see the file COPYING3.  If not see
 #define NM_FLAGS "-Bn"
 #endif
 
-/* SUBTARGET_ASM_DEBUGGING_SPEC handles passing debugging options to
-   the assembler.  It may be overridden by subtargets.  */
-
-#ifndef SUBTARGET_ASM_DEBUGGING_SPEC
-#define SUBTARGET_ASM_DEBUGGING_SPEC "\
-%{g} %{g0} %{g1} %{g2} %{g3} \
-%{ggdb:-g} %{ggdb0:-g0} %{ggdb1:-g1} %{ggdb2:-g2} %{ggdb3:-g3} \
-%{gstabs:-g} %{gstabs0:-g0} %{gstabs1:-g1} %{gstabs2:-g2} %{gstabs3:-g3} \
-%{gstabs+:-g} %{gstabs+0:-g0} %{gstabs+1:-g1} %{gstabs+2:-g2} %{gstabs+3:-g3}"
-#endif
-
 /* SUBTARGET_ASM_SPEC is always passed to the assembler.  It may be
    overridden by subtargets.  */
 
@@ -127,7 +110,6 @@ along with GCC; see the file COPYING3.  If not see
 #define EXTRA_SPECS \
   {"subtarget_cc1_spec", SUBTARGET_CC1_SPEC}, \
   {"subtarget_cpp_spec", SUBTARGET_CPP_SPEC}, \
-  {"subtarget_asm_debugging_spec", SUBTARGET_ASM_DEBUGGING_SPEC}, \
   {"subtarget_asm_spec", SUBTARGET_ASM_SPEC},
 
 /* Registers may have a prefix which can be ignored when matching
@@ -146,8 +128,6 @@ along with GCC; see the file COPYING3.  If not see
 
 #define USER_LABEL_PREFIX ""
 
-/* By default, produce dwarf version 2 format debugging output in response
-   to the  '-g' and '-ggdb' option.  */
 #ifndef PREFERRED_DEBUGGING_TYPE
 #define PREFERRED_DEBUGGING_TYPE DWARF2_DEBUG
 #endif
@@ -209,18 +189,10 @@ along with GCC; see the file COPYING3.  If not see
 /* For LARCH, width of a floating point register.  */
 #define UNITS_PER_FPREG (TARGET_DOUBLE_FLOAT ? 8 : 4)
 
-/* The number of consecutive floating-point registers needed to store the
-   largest format supported by the FPU.  */
-#define MAX_FPRS_PER_FMT 1
-
-/* The number of consecutive floating-point registers needed to store the
-   smallest format supported by the FPU.  */
-#define MIN_FPRS_PER_FMT 1
-
 /* The largest size of value that can be held in floating-point
    registers and moved with a single instruction.  */
 #define UNITS_PER_HWFPVALUE \
-  (TARGET_SOFT_FLOAT ? 0 : MAX_FPRS_PER_FMT * UNITS_PER_FPREG)
+  (TARGET_SOFT_FLOAT ? 0 : UNITS_PER_FPREG)
 
 /* The largest size of value that can be held in floating-point
    registers.  */
@@ -271,7 +243,8 @@ along with GCC; see the file COPYING3.  If not see
 /* Alignment of field after `int : 0' in a structure.  */
 #define EMPTY_FIELD_BOUNDARY 32
 
-/* Every structure's size must be a multiple of this.  */
+/* Number of bits which any structure or union's size must be a multiple of.
+   Each structure or union's size is rounded up to a multiple of this.  */
 #define STRUCTURE_SIZE_BOUNDARY 8
 
 /* There is no point aligning anything to a rounder boundary than
@@ -340,8 +313,7 @@ along with GCC; see the file COPYING3.  If not see
 /* When in 64-bit mode, move insns will sign extend SImode and FCCmode
    moves.  All other references are zero extended.  */
 #define LOAD_EXTEND_OP(MODE) \
-  (TARGET_64BIT && ((MODE) == SImode || (MODE) == FCCmode) ? SIGN_EXTEND \
-							   : ZERO_EXTEND)
+  ((TARGET_64BIT && (MODE) == SImode) ? SIGN_EXTEND : UNKNOWN)
 
 /* Define this macro if it is advisable to hold scalars in registers
    in a wider mode than that declared by the program.  In such cases,
@@ -384,16 +356,7 @@ along with GCC; see the file COPYING3.  If not see
 
 #define FIRST_PSEUDO_REGISTER 74
 
-/* By default, fix the kernel registers ($26 and $27), the global
-   pointer ($28) and the stack pointer ($29).  This can change
-   depending on the command-line options.
-
-   Regarding coprocessor registers: without evidence to the contrary,
-   it's best to assume that each coprocessor register has a unique
-   use.  This can be overridden, in, e.g., loongarch_option_override or
-   TARGET_CONDITIONAL_REGISTER_USAGE should the assumption be
-   inappropriate for a particular target.  */
-
+/* zero, tp, sp and x are fixed.  */
 #define FIXED_REGISTERS							\
 { /* General-purpose registers.  */					\
   1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
@@ -460,10 +423,6 @@ along with GCC; see the file COPYING3.  If not see
 #define LSX_REG_RTX_P(X) (REG_P (X) && LSX_REG_P (REGNO (X)))
 #define LASX_REG_RTX_P(X) (REG_P (X) && LASX_REG_P (REGNO (X)))
 
-
-#define HARD_REGNO_RENAME_OK(OLD_REG, NEW_REG) \
-  loongarch_hard_regno_rename_ok (OLD_REG, NEW_REG)
-
 /* Select a register mode required for caller save of hard regno REGNO.  */
 #define HARD_REGNO_CALLER_SAVE_MODE(REGNO, NREGS, MODE) \
   loongarch_hard_regno_caller_save_mode (REGNO, NREGS, MODE)
@@ -491,7 +450,7 @@ along with GCC; see the file COPYING3.  If not see
 #define LARCH_EPILOGUE_TEMP_REGNUM (GP_TEMP_FIRST)
 
 #define CALLEE_SAVED_REG_NUMBER(REGNO) \
-  ((REGNO) >= 22 && (REGNO) <= 31 ? (REGNO) -22 : -1)
+  ((REGNO) >= 22 && (REGNO) <= 31 ? (REGNO) - 22 : -1)
 
 #define LARCH_PROLOGUE_TEMP(MODE) \
   gen_rtx_REG (MODE, LARCH_PROLOGUE_TEMP_REGNUM)
@@ -531,10 +490,10 @@ along with GCC; see the file COPYING3.  If not see
 enum reg_class
 {
   NO_REGS,	  /* no registers in set */
-  SIBCALL_REGS,	  /* SIBCALL_REGS */
-  JIRL_REGS,	  /* JIRL_REGS */
-  GR_REGS,	  /* integer registers */
+  SIBCALL_REGS,	  /* registers used by indirect sibcalls */
+  JIRL_REGS,	  /* registers used by indirect calls */
   CSR_REGS,	  /* integer registers except for $r0 and $r1 for lcsr. */
+  GR_REGS,	  /* integer registers */
   FP_REGS,	  /* floating point registers */
   FCC_REGS,	  /* status registers (fp status) */
   FRAME_REGS,	  /* arg pointer and frame pointer */
@@ -555,8 +514,8 @@ enum reg_class
   "NO_REGS",								\
   "SIBCALL_REGS",							\
   "JIRL_REGS",								\
-  "GR_REGS",								\
   "CSR_REGS",								\
+  "GR_REGS",								\
   "FP_REGS",								\
   "FCC_REGS",								\
   "FRAME_REGS",								\
@@ -577,10 +536,10 @@ enum reg_class
 #define REG_CLASS_CONTENTS						\
 {									\
   { 0x00000000, 0x00000000, 0x00000000 },	/* NO_REGS */		\
-  { 0x001ff000, 0x00000000, 0x00000000 },	/* SIBCALL_REGS */	\
+  { 0x001fd000, 0x00000000, 0x00000000 },	/* SIBCALL_REGS */	\
   { 0xff9ffff0, 0x00000000, 0x00000000 },	/* JIRL_REGS */		\
-  { 0xffffffff, 0x00000000, 0x00000000 },	/* GR_REGS */		\
   { 0xfffffffc, 0x00000000, 0x00000000 },	/* CSR_REGS */		\
+  { 0xffffffff, 0x00000000, 0x00000000 },	/* GR_REGS */		\
   { 0x00000000, 0xffffffff, 0x00000000 },	/* FP_REGS */		\
   { 0x00000000, 0x00000000, 0x000000ff },	/* FCC_REGS */		\
   { 0x00000000, 0x00000000, 0x00000300 },	/* FRAME_REGS */	\
@@ -606,7 +565,7 @@ enum reg_class
    factor or added to another register (as well as added to a
    displacement).  */
 
-#define INDEX_REG_CLASS NO_REGS
+#define INDEX_REG_CLASS GR_REGS
 
 /* We generally want to put call-clobbered registers ahead of
    call-saved ones.  (IRA expects this.)  */
@@ -678,7 +637,7 @@ enum reg_class
 
 #define CONST_HIGH_PART(VALUE) (((VALUE) + (IMM_REACH / 2)) & ~(IMM_REACH - 1))
 
-#define CONST_LOW_PART(VALUE) ((VALUE) -CONST_HIGH_PART (VALUE))
+#define CONST_LOW_PART(VALUE) ((VALUE) - CONST_HIGH_PART (VALUE))
 
 #define IMM12_INT(X) IMM12_OPERAND (INTVAL (X))
 #define IMM12_INT_UNSIGNED(X) IMM12_OPERAND_UNSIGNED (INTVAL (X))
@@ -775,20 +734,7 @@ enum reg_class
   (IN_RANGE ((N), GP_ARG_FIRST, GP_ARG_LAST) \
    || (UNITS_PER_FP_ARG && IN_RANGE ((N), FP_ARG_FIRST, FP_ARG_LAST)))
 
-/* This structure has to cope with two different argument allocation
-   schemes.  Most LoongArch ABIs view the arguments as a structure, of which
-   the first N words go in registers and the rest go on the stack.  If I
-   < N, the Ith word might go in Ith integer argument register or in a
-   floating-point register.  For these ABIs, we only need to remember
-   the offset of the current argument into the structure.
-
-   So for the standard ABIs, the first N words are allocated to integer
-   registers, and loongarch_function_arg decides on an argument-by-argument
-   basis whether that argument should really go in an integer register,
-   or in a floating-point one.  */
-
-typedef struct loongarch_args
-{
+typedef struct {
   /* Number of integer registers used so far, up to MAX_ARGS_IN_REGISTERS.  */
   unsigned int num_gprs;
 
@@ -811,9 +757,6 @@ typedef struct loongarch_args
 #define LARCH_STACK_ALIGN(LOC) \
   (TARGET_ABI_LP64 ? ROUND_UP ((LOC), 16) : ROUND_UP ((LOC), 8))
 
-/* Output assembler code to FILE to increment profiler label # LABELNO
-   for profiling a function entry.  */
-
 #define MCOUNT_NAME "_mcount"
 
 /* Emit rtl for profiling.  Output assembler code to FILE
@@ -855,13 +798,15 @@ typedef struct loongarch_args
 
 /* Addressing modes, and classification of registers for them.  */
 
-#define REGNO_OK_FOR_INDEX_P(REGNO) 0
+#define REGNO_OK_FOR_INDEX_P(REGNO) \
+  loongarch_regno_mode_ok_for_base_p (REGNO, VOIDmode, 1)
+
 #define REGNO_MODE_OK_FOR_BASE_P(REGNO, MODE) \
   loongarch_regno_mode_ok_for_base_p (REGNO, MODE, 1)
 
 /* Maximum number of registers that can appear in a valid memory address.  */
 
-#define MAX_REGS_PER_ADDRESS 1
+#define MAX_REGS_PER_ADDRESS 2
 
 /* Check for constness inline but use loongarch_legitimate_address_p
    to check whether a constant really is an address.  */
@@ -884,7 +829,6 @@ typedef struct loongarch_args
 
 #define CASE_VECTOR_MODE Pmode
 
-/* Only use short offsets if their range will not overflow.  */
 #define CASE_VECTOR_SHORTEN_MODE(MIN, MAX, BODY) Pmode
 
 /* Define this as 1 if `char' should by default be signed; else as 0.  */
@@ -1078,36 +1022,9 @@ typedef struct loongarch_args
   { "xr31",	31 + FP_REG_FIRST }					\
 }
 
-/* The LoongArch implementation uses some labels for its own purpose.  The
-   following lists what labels are created, and are all formed by the
-   pattern $L[a-z].*.  The machine independent portion of GCC creates
-   labels matching:  $L[A-Z][0-9]+ and $L[0-9]+.
-
-	LM[0-9]+	Silicon Graphics/ECOFF stabs label before each stmt.
-	$Lb[0-9]+	Begin blocks for LoongArch debug support
-	$Lc[0-9]+	Label for use in s<xx> operation.
-	$Le[0-9]+	End blocks for LoongArch debug support.  */
-
-#undef ASM_DECLARE_OBJECT_NAME
-#define ASM_DECLARE_OBJECT_NAME(STREAM, NAME, DECL) \
-  loongarch_declare_object (STREAM, NAME, "", ":\n")
-
 /* Globalizing directive for a label.  */
 #define GLOBAL_ASM_OP "\t.globl\t"
 
-/* This says how to define a global common symbol.  */
-
-#define ASM_OUTPUT_ALIGNED_DECL_COMMON loongarch_output_aligned_decl_common
-
-/* This says how to define a local common symbol (i.e., not visible to
-   linker).  */
-
-#ifndef ASM_OUTPUT_ALIGNED_LOCAL
-#define ASM_OUTPUT_ALIGNED_LOCAL(STREAM, NAME, SIZE, ALIGN) \
-  loongarch_declare_common_object (STREAM, NAME, "\n\t.lcomm\t", SIZE, ALIGN, \
-				   false)
-#endif
-
 /* This says how to output an external.  It would be possible not to
    output anything and let undefined symbol become external.  However
    the assembler uses length information on externals to allocate in
@@ -1117,16 +1034,6 @@ typedef struct loongarch_args
 #define ASM_OUTPUT_EXTERNAL(STREAM, DECL, NAME) \
   loongarch_output_external (STREAM, DECL, NAME)
 
-/* This is how to declare a function name.  The actual work of
-   emitting the label is moved to function_prologue, so that we can
-   get the line number correctly emitted before the .ent directive,
-   and after any .file directives.  Define as empty so that the function
-   is not declared before the .ent directive elsewhere.  */
-
-#undef ASM_DECLARE_FUNCTION_NAME
-#define ASM_DECLARE_FUNCTION_NAME(STREAM, NAME, DECL) \
-  loongarch_declare_function_name (STREAM, NAME, DECL)
-
 /* This is how to store into the string LABEL
    the symbol_ref name of an internal numbered label where
    PREFIX is the class of label and NUM is the number within the class.
@@ -1247,11 +1154,11 @@ typedef struct loongarch_args
 #define PTRDIFF_TYPE (POINTER_SIZE == 64 ? "long int" : "int")
 
 /* The maximum number of bytes that can be copied by one iteration of
-   a cpymemsi loop; see loongarch_block_move_loop.  */
+   a movmemsi loop; see loongarch_block_move_loop.  */
 #define LARCH_MAX_MOVE_BYTES_PER_LOOP_ITER (UNITS_PER_WORD * 4)
 
 /* The maximum number of bytes that can be copied by a straight-line
-   implementation of cpymemsi; see loongarch_block_move_straight.  We want
+   implementation of movmemsi; see loongarch_block_move_straight.  We want
    to make sure that any loop-based implementation will iterate at
    least twice.  */
 #define LARCH_MAX_MOVE_BYTES_STRAIGHT (LARCH_MAX_MOVE_BYTES_PER_LOOP_ITER * 2)
@@ -1261,11 +1168,11 @@ typedef struct loongarch_args
 */
 #define LARCH_CALL_RATIO 8
 
-/* Any loop-based implementation of cpymemsi will have at least
+/* Any loop-based implementation of movmemsi will have at least
    LARCH_MAX_MOVE_BYTES_STRAIGHT / UNITS_PER_WORD memory-to-memory
    moves, so allow individual copies of fewer elements.
 
-   When cpymemsi is not available, use a value approximating
+   When movmemsi is not available, use a value approximating
    the length of a memcpy call sequence, so that move_by_pieces
    will generate inline code if it is shorter than a function call.
    Since move_by_pieces_ninsns counts memory-to-memory moves, but
@@ -1273,7 +1180,7 @@ typedef struct loongarch_args
    value of LARCH_CALL_RATIO to take that into account.  */
 
 #define MOVE_RATIO(speed) \
-  (HAVE_cpymemsi \
+  (HAVE_movmemsi \
    ? LARCH_MAX_MOVE_BYTES_PER_LOOP_ITER / UNITS_PER_WORD \
    : CLEAR_RATIO (speed) / 2)
 
@@ -1345,54 +1252,18 @@ struct GTY (()) machine_function
   /* The current frame information, calculated by loongarch_compute_frame_info.
    */
   struct loongarch_frame_info frame;
-
-  /* True if at least one of the formal parameters to a function must be
-     written to the frame header (probably so its address can be taken).  */
-  bool does_not_use_frame_header;
-
-  /* True if none of the functions that are called by this function need
-     stack space allocated for their arguments.  */
-  bool optimize_call_stack;
-
-  /* True if one of the functions calling this function may not allocate
-     a frame header.  */
-  bool callers_may_not_allocate_frame;
 };
 #endif
 
-/* As on most targets, we want the .eh_frame section to be read-only where
-   possible.  And as on most targets, this means two things:
-
-     (a) Non-locally-binding pointers must have an indirect encoding,
-	 so that the addresses in the .eh_frame section itself become
-	 locally-binding.
-
-     (b) A shared library's .eh_frame section must encode locally-binding
-	 pointers in a relative (relocation-free) form.
-
-   However, LoongArch has traditionally not allowed directives like:
-
-	.long	x-.
-
-   in cases where "x" is in a different section, or is not defined in the
-   same assembly file.  We are therefore unable to emit the PC-relative
-   form required by (b) at assembly time.
-
-   Fortunately, the linker is able to convert absolute addresses into
-   PC-relative addresses on our behalf.  Unfortunately, only certain
-   versions of the linker know how to do this for indirect pointers,
-   and for personality data.  We must fall back on using writable
-   .eh_frame sections for shared libraries if the linker does not
-   support this feature.  */
 #define ASM_PREFERRED_EH_DATA_FORMAT(CODE, GLOBAL) \
   (((GLOBAL) ? DW_EH_PE_indirect : 0) | DW_EH_PE_absptr)
 
 /* Several named LoongArch patterns depend on Pmode.  These patterns have the
-   form <NAME>_si for Pmode == SImode and <NAME>_di for Pmode == DImode.
+   form <NAME>si for Pmode == SImode and <NAME>di for Pmode == DImode.
    Add the appropriate suffix to generator function NAME and invoke it
    with arguments ARGS.  */
 #define PMODE_INSN(NAME, ARGS) \
-  (Pmode == SImode ? NAME##_si ARGS : NAME##_di ARGS)
+  (Pmode == SImode ? NAME##si ARGS : NAME##di ARGS)
 
 /* Do emit .note.GNU-stack by default.  */
 #ifndef NEED_INDICATE_EXEC_STACK
@@ -1408,11 +1279,4 @@ struct GTY (()) machine_function
 #define UNITS_PER_FP_ARG  \
   (TARGET_HARD_FLOAT ? (TARGET_DOUBLE_FLOAT ? 8 : 4) : 0)
 
-#define LIBCALL_VALUE(MODE) \
-  loongarch_function_value (NULL_TREE, NULL_TREE, MODE)
-
-#define FUNCTION_VALUE(VALTYPE, FUNC) \
-  loongarch_function_value (VALTYPE, FUNC, VOIDmode)
-
-
 #define FUNCTION_VALUE_REGNO_P(N) ((N) == GP_RETURN || (N) == FP_RETURN)
diff --git a/src/gcc/config/loongarch/loongarch.md b/src/gcc/config/loongarch/loongarch.md
index 94f375713..7ac46e753 100644
--- a/src/gcc/config/loongarch/loongarch.md
+++ b/src/gcc/config/loongarch/loongarch.md
@@ -1,5 +1,5 @@
 ;; Machine Description for LoongArch for GNU compiler.
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Ltd.
 ;; Based on MIPS target for GNU compiler.
 
@@ -30,7 +30,12 @@
   UNSPEC_LOAD_HIGH
   UNSPEC_STORE_WORD
   UNSPEC_MOVGR2FRH
+  UNSPEC_MOVGR2FR
   UNSPEC_MOVFRH2GR
+  UNSPEC_MOVFR2GR
+  UNSPEC_MOVFCC2GR
+  UNSPEC_MOVGR2FCC
+  UNSPEC_MOVFR2FCC
 
   ;; Floating point unspecs.
   UNSPEC_FRINT
@@ -65,11 +70,6 @@
   UNSPECV_DBAR
   UNSPECV_IBAR
 
-  ;; CPUCFG
-  UNSPECV_CPUCFG
-  UNSPECV_ASRTLE_D
-  UNSPECV_ASRTGT_D
-
   ;; Privileged instructions
   UNSPECV_CSRRD
   UNSPECV_CSRWR
@@ -81,13 +81,19 @@
   UNSPECV_LDPTE
   UNSPECV_ERTN
 
-  ;; Stack checking.
+  ;; Stack checking
   UNSPECV_PROBE_STACK_RANGE
 
-  ;; Floating-point environment.
+  ;; Floating-point environment
   UNSPECV_MOVFCSR2GR
   UNSPECV_MOVGR2FCSR
 
+  ;; Others
+  UNSPECV_CPUCFG
+  UNSPECV_ASRTLE_D
+  UNSPECV_ASRTGT_D
+  UNSPECV_SYSCALL
+  UNSPECV_BREAK
 ])
 
 (define_constants
@@ -281,13 +287,11 @@
 ;; D2I	float to integer (DF to SI/DI)
 ;; D2S	double to float single
 ;; S2D	float single to double
+;; C2D  fcc to DI
 
 (define_attr "cnv_mode" "unknown,I2S,I2D,S2I,D2I,D2S,S2D"
   (const_string "unknown"))
 
-(define_attr "compression" "none,all"
-  (const_string "none"))
-
 ;; The number of individual instructions that a non-branch pattern generates
 (define_attr "insn_count" ""
   (cond [;; "Ghost" instructions occupy no space.
@@ -335,8 +339,8 @@
 	  (if_then_else (and (le (minus (match_dup 0) (pc)) (const_int 131064))
 			     (le (minus (pc) (match_dup 0)) (const_int 131068)))
 	  (const_int 4)
-	  (const_int 8))
-	  ](symbol_ref "get_attr_insn_count (insn) * 4")))
+	  (const_int 8))]
+    (symbol_ref "get_attr_insn_count (insn) * 4")))
 
 ;; The type of hardware hazard associated with this instruction.
 ;; DELAY means that the next instruction cannot read the result
@@ -356,9 +360,6 @@
 ;; modes.
 (define_mode_iterator GPR2 [SI (DI "TARGET_64BIT")])
 
-;; Likewise, but for XLEN-sized quantities.
-(define_mode_iterator X [(SI "!TARGET_64BIT") (DI "TARGET_64BIT")])
-
 ;; This mode iterator allows 16-bit and 32-bit GPR patterns and 32-bit 64-bit
 ;; FPR patterns to be generated from the same template.
 (define_mode_iterator JOIN_MODE [HI
@@ -370,6 +371,9 @@
 ;; pointer-sized quantities.  Exactly one of the two alternatives will match.
 (define_mode_iterator P [(SI "Pmode == SImode") (DI "Pmode == DImode")])
 
+;; Likewise, but for XLEN-sized quantities.
+(define_mode_iterator X [(SI "!TARGET_64BIT") (DI "TARGET_64BIT")])
+
 ;; 64-bit modes for which we provide move patterns.
 (define_mode_iterator MOVE64 [DI DF])
 
@@ -382,7 +386,7 @@
 ;; Likewise the 64-bit truncate-and-shift patterns.
 (define_mode_iterator SUBDI [QI HI SI])
 
-;; This mode iterator allows the QI HI SI and DI extension patterns to be
+;; Iterator for scalar fixed-point modes.
 (define_mode_iterator QHWD [QI HI SI (DI "TARGET_64BIT")])
 
 ;; Iterator for hardware-supported floating-point modes.
@@ -395,8 +399,8 @@
    (DI "!TARGET_64BIT && TARGET_DOUBLE_FLOAT")
    (TF "TARGET_64BIT && TARGET_DOUBLE_FLOAT")])
 
-;; In GPR templates, a string like "mul.<d>" will expand to "mul" in the
-;; 32-bit "mul.w" and "mul.d" in the 64-bit version.
+;; In GPR templates, a string like "mul.<d>" will expand to "mul.w" in the
+;; 32-bit version and "mul.d" in the 64-bit version.
 (define_mode_attr d [(SI "w") (DI "d")])
 
 ;; This attribute gives the length suffix for a load or store instruction.
@@ -404,31 +408,28 @@
 (define_mode_attr size [(QI "b") (HI "h") (SI "w") (DI "d")])
 (define_mode_attr SIZE [(QI "B") (HI "H") (SI "W") (DI "D")])
 
-;; This attributes gives the mode mask of a SHORT.
+;; This attribute gives the mode mask of a SHORT.
 (define_mode_attr mask [(QI "0x00ff") (HI "0xffff")])
 
-;; This attributes gives the size (bits) of a SHORT.
-(define_mode_attr qi_hi [(QI "7") (HI "15")])
+;; This attribute gives the size (bits) of a SHORT.
+(define_mode_attr 7_or_15 [(QI "7") (HI "15")])
 
 ;; Instruction names for stores.
 (define_mode_attr store [(QI "sb") (HI "sh") (SI "sw") (DI "sd")])
 
-;; Similarly for LoongArch indexed FPR loads and stores.
-(define_mode_attr floadx [(SF "fldx.s") (DF "fldx.d") (V2SF "fldx.d")])
-(define_mode_attr fstorex [(SF "fstx.s") (DF "fstx.d") (V2SF "fstx.d")])
-
 ;; Similarly for LoongArch indexed GPR loads and stores.
 (define_mode_attr loadx [(QI "ldx.b")
-			 (HI "ldx.h")
-			 (SI "ldx.w")
-			 (DI "ldx.d")])
+                        (HI "ldx.h")
+                        (SI "ldx.w")
+                        (DI "ldx.d")])
 (define_mode_attr storex [(QI "stx.b")
-			  (HI "stx.h")
-			  (SI "stx.w")
-			  (DI "stx.d")])
+                         (HI "stx.h")
+                         (SI "stx.w")
+                         (DI "stx.d")])
 
 ;; This attribute gives the format suffix for floating-point operations.
 (define_mode_attr fmt [(SF "s") (DF "d") (V2SF "ps")])
+(define_mode_attr ifmt [(SI "w") (DI "l")])
 
 ;; This attribute gives the upper-case mode name for one unit of a
 ;; floating-point mode or vector mode.
@@ -467,6 +468,7 @@
 ;; This code iterator allows the three bitwise instructions to be generated
 ;; from the same template.
 (define_code_iterator any_bitwise [and ior xor])
+(define_code_iterator neg_bitwise [and ior])
 
 ;; This code iterator allows addition and subtraction to be generated
 ;; from the same template.
@@ -566,6 +568,7 @@
 
 ;; The sel mnemonic to use depending on the condition test.
 (define_code_attr sel [(eq "masknez") (ne "maskeqz")])
+(define_code_attr fsel_invert [(eq "%2,%3") (ne "%3,%2")])
 (define_code_attr selinv [(eq "maskeqz") (ne "masknez")])
 
 ;;
@@ -606,29 +609,17 @@
 (define_insn "add<mode>3"
   [(set (match_operand:GPR 0 "register_operand" "=r,r")
 	(plus:GPR (match_operand:GPR 1 "register_operand" "r,r")
-		  (match_operand:GPR 2 "arith_operand" "r,Q")))]
+		  (match_operand:GPR 2 "arith_operand" "r,I")))]
   ""
   "add%i2.<d>\t%0,%1,%2";
   [(set_attr "alu_type" "add")
-   (set_attr "compression" "*,*")
    (set_attr "mode" "<MODE>")])
 
 (define_insn "*addsi3_extended"
   [(set (match_operand:DI 0 "register_operand" "=r,r")
 	(sign_extend:DI
 	     (plus:SI (match_operand:SI 1 "register_operand" "r,r")
-		      (match_operand:SI 2 "arith_operand" "r,Q"))))]
-  "TARGET_64BIT"
-  "add%i2.w\t%0,%1,%2"
-  [(set_attr "alu_type" "add")
-   (set_attr "mode" "SI")])
-
-(define_insn "*addsi3_extended2"
-  [(set (match_operand:DI 0 "register_operand" "=r,r")
-	(sign_extend:DI
-	  (subreg:SI (plus:DI (match_operand:DI 1 "register_operand" "r,r")
-			      (match_operand:DI 2 "arith_operand"    "r,Q"))
-		     0)))]
+		      (match_operand:SI 2 "arith_operand" "r,I"))))]
   "TARGET_64BIT"
   "add%i2.w\t%0,%1,%2"
   [(set_attr "alu_type" "add")
@@ -659,30 +650,18 @@
   ""
   "sub.<d>\t%0,%z1,%2"
   [(set_attr "alu_type" "sub")
-   (set_attr "compression" "*")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "*subsi3_extended"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(sign_extend:DI
-	    (minus:SI (match_operand:SI 1 "register_operand" "rJ")
-		      (match_operand:SI 2 "register_operand" "r"))))]
-  "TARGET_64BIT"
-  "sub.w\t%0,%z1,%2"
-  [(set_attr "alu_type" "sub")
-   (set_attr "mode" "DI")])
 
-(define_insn "*subsi3_extended2"
+(define_insn "*subsi3_extended"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	(sign_extend:DI
-	  (subreg:SI (minus:DI (match_operand:DI 1 "reg_or_0_operand" "rJ")
-			       (match_operand:DI 2 "register_operand" "r"))
-		     0)))]
+	     (minus:SI (match_operand:SI 1 "reg_or_0_operand" "rJ")
+		       (match_operand:SI 2 "register_operand" "r"))))]
   "TARGET_64BIT"
   "sub.w\t%0,%z1,%2"
-  [(set_attr "alu_type" "sub")
+  [(set_attr "type" "arith")
    (set_attr "mode" "SI")])
-
 
 ;;
 ;;  ....................
@@ -696,7 +675,7 @@
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(mult:ANYF (match_operand:ANYF 1 "register_operand" "f")
 		   (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmul.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fmul")
    (set_attr "mode" "<MODE>")])
@@ -729,18 +708,6 @@
   [(set_attr "type" "imul")
    (set_attr "mode" "SI")])
 
-(define_insn "*mulsi3_extended2"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(sign_extend:DI
-	  (subreg:SI (mult:DI (match_operand:DI 1 "register_operand" "r")
-			      (match_operand:DI 2 "register_operand" "r"))
-		     0)))]
-  "TARGET_64BIT"
-  "mul.w\t%0,%1,%2"
-  [(set_attr "type" "imul")
-   (set_attr "mode" "SI")])
-
-
 ;;
 ;;  ........................
 ;;
@@ -749,7 +716,6 @@
 ;;  ........................
 ;;
 
-
 (define_expand "<u>mulditi3"
   [(set (match_operand:TI 0 "register_operand")
 	(mult:TI (any_extend:TI (match_operand:DI 1 "register_operand"))
@@ -824,14 +790,13 @@
   [(set (match_operand:ANYF 0 "register_operand")
 	(div:ANYF (match_operand:ANYF 1 "reg_or_1_operand")
 		  (match_operand:ANYF 2 "register_operand")))]
-  "TARGET_HARD_FLOAT"
-{})
+  "")
 
 (define_insn "*div<mode>3"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(div:ANYF (match_operand:ANYF 1 "register_operand" "f")
 		  (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fdiv.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fdiv")
    (set_attr "mode" "<UNITMODE>")
@@ -843,42 +808,66 @@
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(div:ANYF (match_operand:ANYF 1 "const_1_operand" "")
 		  (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "frecip.<fmt>\t%0,%2"
   [(set_attr "type" "frdiv")
    (set_attr "mode" "<UNITMODE>")
    (set_attr "insn_count" "1")])
 
 ;; Integer division and modulus.
+(define_expand "<optab><mode>3"
+  [(set (match_operand:GPR 0 "register_operand")
+	(any_div:GPR (match_operand:GPR 1 "register_operand")
+		     (match_operand:GPR 2 "register_operand")))]
+ ""
+{
+  if (GET_MODE (operands[0]) == SImode)
+    {
+      rtx reg1 = gen_reg_rtx (DImode);
+      rtx reg2 = gen_reg_rtx (DImode);
 
-(define_insn "<optab><mode>3"
+      operands[1] = gen_rtx_SIGN_EXTEND (word_mode, operands[1]);
+      operands[2] = gen_rtx_SIGN_EXTEND (word_mode, operands[2]);
+
+      emit_insn (gen_rtx_SET (reg1, operands[1]));
+      emit_insn (gen_rtx_SET (reg2, operands[2]));
+
+      emit_insn (gen_<optab>di3_fake (operands[0], reg1, reg2));
+      DONE;
+    }
+})
+
+(define_insn "*<optab><mode>3"
   [(set (match_operand:GPR 0 "register_operand" "=&r")
 	(any_div:GPR (match_operand:GPR 1 "register_operand" "r")
 		     (match_operand:GPR 2 "register_operand" "r")))]
   ""
-  {
-    return loongarch_output_division ("<insn>.<d><u>\t%0,%1,%2", operands);
-  }
+{
+  return loongarch_output_division ("<insn>.<d><u>\t%0,%1,%2", operands);
+}
   [(set_attr "type" "idiv")
    (set_attr "mode" "<MODE>")])
 
+(define_insn "<optab>di3_fake"
+  [(set (match_operand:SI 0 "register_operand" "=&r")
+	(any_div:SI (match_operand:DI 1 "register_operand" "r")
+		    (match_operand:DI 2 "register_operand" "r")))]
+  ""
+{
+  return loongarch_output_division ("<insn>.w<u>\t%0,%1,%2", operands);
+}
+  [(set_attr "type" "idiv")
+   (set_attr "mode" "SI")])
 
 ;; Floating point multiply accumulate instructions.
 
 ;; a * b + c
-(define_expand "fma<mode>4"
-  [(set (match_operand:ANYF 0 "register_operand")
-	(fma:ANYF (match_operand:ANYF 1 "register_operand")
-		  (match_operand:ANYF 2 "register_operand")
-		  (match_operand:ANYF 3 "register_operand")))]
-  "TARGET_HARD_FLOAT")
-
-(define_insn "*fma<mode>4_madd4"
+(define_insn "fma<mode>4"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(fma:ANYF (match_operand:ANYF 1 "register_operand" "f")
 		  (match_operand:ANYF 2 "register_operand" "f")
 		  (match_operand:ANYF 3 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmadd.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -889,7 +878,7 @@
 	(fma:ANYF (match_operand:ANYF 1 "register_operand" "f")
 		  (match_operand:ANYF 2 "register_operand" "f")
 		  (neg:ANYF (match_operand:ANYF 3 "register_operand" "f"))))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmsub.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -906,7 +895,7 @@
 	(fma:ANYF (neg:ANYF (match_operand:ANYF 1 "register_operand" "f"))
 		  (match_operand:ANYF 2 "register_operand" "f")
 		  (match_operand:ANYF 3 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT && !HONOR_SIGNED_ZEROS (<MODE>mode)"
+  "!HONOR_SIGNED_ZEROS (<MODE>mode)"
   "fnmsub.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -921,10 +910,10 @@
 (define_insn "fnms<mode>4"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(fma:ANYF
-	  (neg:ANYF (match_operand:ANYF 1 "register_operand" "f"))
-	  (match_operand:ANYF 2 "register_operand" "f")
-	  (neg:ANYF (match_operand:ANYF 3 "register_operand" "f"))))]
-  "TARGET_HARD_FLOAT && !HONOR_SIGNED_ZEROS (<MODE>mode)"
+	    (neg:ANYF (match_operand:ANYF 1 "register_operand" "f"))
+	    (match_operand:ANYF 2 "register_operand" "f")
+	    (neg:ANYF (match_operand:ANYF 3 "register_operand" "f"))))]
+  "!HONOR_SIGNED_ZEROS (<MODE>mode)"
   "fnmadd.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -937,7 +926,7 @@
 		(neg:ANYF (match_operand:ANYF 1 "register_operand" " f"))
 		(match_operand:ANYF 2 "register_operand" " f")
 		(neg:ANYF (match_operand:ANYF 3 "register_operand" " f")))))]
-  "TARGET_HARD_FLOAT && !HONOR_SIGNED_ZEROS (<MODE>mode)"
+  "!HONOR_SIGNED_ZEROS (<MODE>mode)"
   "fmadd.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -950,7 +939,7 @@
 		(neg:ANYF (match_operand:ANYF 1 "register_operand" " f"))
 		(match_operand:ANYF 2 "register_operand" " f")
 		(match_operand:ANYF 3 "register_operand" " f"))))]
-  "TARGET_HARD_FLOAT && !HONOR_SIGNED_ZEROS (<MODE>mode)"
+  "!HONOR_SIGNED_ZEROS (<MODE>mode)"
   "fmsub.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -963,7 +952,7 @@
 		(match_operand:ANYF 1 "register_operand" " f")
 		(match_operand:ANYF 2 "register_operand" " f")
 		(match_operand:ANYF 3 "register_operand" " f"))))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fnmadd.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -976,7 +965,7 @@
 		(match_operand:ANYF 1 "register_operand" " f")
 		(match_operand:ANYF 2 "register_operand" " f")
 		(neg:ANYF (match_operand:ANYF 3 "register_operand" " f")))))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fnmsub.<fmt>\t%0,%1,%2,%3"
   [(set_attr "type" "fmadd")
    (set_attr "mode" "<UNITMODE>")])
@@ -991,7 +980,7 @@
 (define_insn "sqrt<mode>2"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(sqrt:ANYF (match_operand:ANYF 1 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fsqrt.<fmt>\t%0,%1"
   [(set_attr "type" "fsqrt")
    (set_attr "mode" "<UNITMODE>")
@@ -1011,7 +1000,7 @@
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(sqrt:ANYF (div:ANYF (match_operand:ANYF 1 "const_1_operand" "")
 			     (match_operand:ANYF 2 "register_operand" "f"))))]
-  "TARGET_HARD_FLOAT && flag_unsafe_math_optimizations"
+  "flag_unsafe_math_optimizations"
   "frsqrt.<fmt>\t%0,%2"
   [(set_attr "type" "frsqrt")
    (set_attr "mode" "<UNITMODE>")
@@ -1027,7 +1016,7 @@
 (define_insn "abs<mode>2"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(abs:ANYF (match_operand:ANYF 1 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fabs.<fmt>\t%0,%1"
   [(set_attr "type" "fabs")
    (set_attr "mode" "<UNITMODE>")])
@@ -1073,9 +1062,9 @@
 
 (define_insn "smax<mode>3"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
-       (smax:ANYF (match_operand:ANYF 1 "register_operand" "f")
-		  (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+	(smax:ANYF (match_operand:ANYF 1 "register_operand" "f")
+		   (match_operand:ANYF 2 "register_operand" "f")))]
+  ""
   "fmax.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fmove")
    (set_attr "mode" "<MODE>")])
@@ -1084,7 +1073,7 @@
   [(set (match_operand:ANYF 0 "register_operand" "=f")
        (smin:ANYF (match_operand:ANYF 1 "register_operand" "f")
 		  (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmin.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fmove")
    (set_attr "mode" "<MODE>")])
@@ -1096,7 +1085,7 @@
 		  (abs:ANYF (match_operand:ANYF 2 "register_operand" "f")))
 	      (match_dup 1)
 	      (match_dup 2)))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmaxa.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fmove")
    (set_attr "mode" "<MODE>")])
@@ -1108,7 +1097,7 @@
 		    (abs:ANYF (match_operand:ANYF 2 "register_operand" "f")))
 		(match_dup 1)
 		(match_dup 2)))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fmina.<fmt>\t%0,%1,%2"
   [(set_attr "type" "fmove")
    (set_attr "mode" "<MODE>")])
@@ -1126,7 +1115,7 @@
   ""
   "sub.<d>\t%0,%.,%1"
   [(set_attr "alu_type"	"sub")
-   (set_attr "mode"	"<MODE>")])
+   (set_attr "mode" "<MODE>")])
 
 (define_insn "one_cmpl<mode>2"
   [(set (match_operand:GPR 0 "register_operand" "=r")
@@ -1134,13 +1123,12 @@
   ""
   "nor\t%0,%.,%1"
   [(set_attr "alu_type" "not")
-   (set_attr "compression" "*")
    (set_attr "mode" "<MODE>")])
 
 (define_insn "neg<mode>2"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(neg:ANYF (match_operand:ANYF 1 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+  ""
   "fneg.<fmt>\t%0,%1"
   [(set_attr "type" "fneg")
    (set_attr "mode" "<UNITMODE>")])
@@ -1157,11 +1145,10 @@
 (define_insn "<optab><mode>3"
   [(set (match_operand:GPR 0 "register_operand" "=r,r")
 	(any_bitwise:GPR (match_operand:GPR 1 "register_operand" "r,r")
-		 (match_operand:GPR 2 "uns_arith_operand" "r,K")))]
+			 (match_operand:GPR 2 "uns_arith_operand" "r,K")))]
   ""
   "<insn>%i2\t%0,%1,%2"
   [(set_attr "type" "logical")
-   (set_attr "compression" "*,*")
    (set_attr "mode" "<MODE>")])
 
 (define_insn "and<mode>3_extended"
@@ -1181,7 +1168,7 @@
 
 (define_insn "*iorhi3"
   [(set (match_operand:HI 0 "register_operand" "=r,r")
-	(ior:HI (match_operand:HI 1 "register_operand" "r,r")
+	(ior:HI (match_operand:HI 1 "register_operand" "%r,r")
 		(match_operand:HI 2 "uns_arith_operand" "r,K")))]
   ""
   "or%i2\t%0,%1,%2"
@@ -1190,30 +1177,22 @@
 
 (define_insn "*nor<mode>3"
   [(set (match_operand:GPR 0 "register_operand" "=r")
-	(and:GPR (not:GPR (match_operand:GPR 1 "register_operand" "r"))
+	(and:GPR (not:GPR (match_operand:GPR 1 "register_operand" "%r"))
 		 (not:GPR (match_operand:GPR 2 "register_operand" "r"))))]
   ""
   "nor\t%0,%1,%2"
   [(set_attr "type" "logical")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "andn<mode>"
-  [(set (match_operand:GPR 0 "register_operand" "=r")
-	(and:GPR
-	  (not:GPR (match_operand:GPR 1 "register_operand" "r"))
-	  (match_operand:GPR 2 "register_operand" "r")))]
-  ""
-  "andn\t%0,%2,%1"
-  [(set_attr "type" "logical")])
-
-(define_insn "orn<mode>"
+(define_insn "<optab>n<mode>"
   [(set (match_operand:GPR 0 "register_operand" "=r")
-	(ior:GPR
-	  (not:GPR (match_operand:GPR 1 "register_operand" "r"))
-	  (match_operand:GPR 2 "register_operand" "r")))]
+	(neg_bitwise:GPR
+	    (not:GPR (match_operand:GPR 1 "register_operand" "r"))
+	    (match_operand:GPR 2 "register_operand" "r")))]
   ""
-  "orn\t%0,%2,%1"
-  [(set_attr "type" "logical")])
+  "<insn>n\t%0,%2,%1"
+  [(set_attr "type" "logical")
+   (set_attr "mode" "<MODE>")])
 
 
 ;;
@@ -1223,6 +1202,17 @@
 ;;
 ;;  ....................
 
+(define_insn "truncdi<mode>2"
+  [(set (match_operand:SUBDI 0 "nonimmediate_operand" "=r,m,k")
+        (truncate:SUBDI (match_operand:DI 1 "register_operand" "r,r,r")))]
+  "TARGET_64BIT"
+  "@
+    slli.w\t%0,%1,0
+    st.<size>\t%1,%0
+    stx.<size>\t%1,%0"
+  [(set_attr "move_type" "sll0,store,store")
+   (set_attr "mode" "SI")])
+
 (define_insn "truncdfsf2"
   [(set (match_operand:SF 0 "register_operand" "=f")
 	(float_truncate:SF (match_operand:DF 1 "register_operand" "f")))]
@@ -1232,58 +1222,14 @@
    (set_attr "cnv_mode"	"D2S")
    (set_attr "mode" "SF")])
 
-;; Integer truncation patterns.  Truncating SImode values to smaller
-;; modes is a no-op, as it is for most other GCC ports.  Truncating
-;; DImode values to SImode is not a no-op for TARGET_64BIT since we
-;; need to make sure that the lower 32 bits are properly sign-extended
-;; (see TARGET_TRULY_NOOP_TRUNCATION).  Truncating DImode values into modes
-;; smaller than SImode is equivalent to two separate truncations:
-;;
-;;			  A       B
-;;    DI ---> HI  ==  DI ---> SI ---> HI
-;;    DI ---> QI  ==  DI ---> SI ---> QI
-;;
-;; Step A needs a real instruction but step B does not.
-
-(define_insn "truncdi<mode>2"
-  [(set (match_operand:SUBDI 0 "nonimmediate_operand" "=r,m")
-	(truncate:SUBDI (match_operand:DI 1 "register_operand" "r,r")))]
-  "TARGET_64BIT"
-  "@
-    slli.w\t%0,%1,0
-    st.<size>\t%1,%0"
-  [(set_attr "move_type" "sll0,store")
-   (set_attr "mode" "SI")])
-
-(define_insn "truncdisi2_extended"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=ZC")
-	(truncate:SI (match_operand:DI 1 "register_operand" "r")))]
-  "TARGET_64BIT"
-  "stptr.w\t%1,%0"
-  [(set_attr "move_type" "store")
-   (set_attr "mode" "SI")])
-
-;; Combiner patterns to optimize shift/truncate combinations.
-
-(define_insn "*ashr_trunc<mode>"
-  [(set (match_operand:SUBDI 0 "register_operand" "=r")
-	(truncate:SUBDI
-	  (ashiftrt:DI (match_operand:DI 1 "register_operand" "r")
-		       (match_operand:DI 2 "const_arith_operand" ""))))]
-  "TARGET_64BIT && IN_RANGE (INTVAL (operands[2]), 32, 63)"
-  "srai.d\t%0,%1,%2"
-  [(set_attr "type" "shift")
-   (set_attr "mode" "<MODE>")])
+;;(define_insn "truncdisi2_extended"
+;;  [(set (match_operand:SI 0 "nonimmediate_operand" "=ZC")
+;;	(truncate:SI (match_operand:DI 1 "register_operand" "r")))]
+;;  "TARGET_64BIT"
+;;  "stptr.w\t%1,%0"
+;;  [(set_attr "move_type" "store")
+;;   (set_attr "mode" "SI")])
 
-(define_insn "*lshr32_trunc<mode>"
-  [(set (match_operand:SUBDI 0 "register_operand" "=r")
-	(truncate:SUBDI
-	  (lshiftrt:DI (match_operand:DI 1 "register_operand" "r")
-		       (const_int 32))))]
-  "TARGET_64BIT"
-  "srai.d\t%0,%1,32"
-  [(set_attr "type" "shift")
-   (set_attr "mode" "<MODE>")])
 
 ;;
 ;;  ....................
@@ -1291,39 +1237,58 @@
 ;;	ZERO EXTENSION
 ;;
 ;;  ....................
+(define_expand "zero_extendsidi2"
+  [(set (match_operand:DI 0 "register_operand")
+	(zero_extend:DI (match_operand:SI 1 "nonimmediate_operand")))]
+  "TARGET_64BIT")
 
-(define_insn "zero_extendsidi2"
-  [(set (match_operand:DI 0 "register_operand" "=r,r,r")
-	(zero_extend:DI (match_operand:SI 1 "nonimmediate_operand" "r,ZC,W")))]
+(define_insn_and_split "*zero_extendsidi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=r,r,r,r")
+	(zero_extend:DI (match_operand:SI 1 "nonimmediate_operand" "r,m,ZC,k")))]
   "TARGET_64BIT"
   "@
    bstrpick.d\t%0,%1,31,0
-   ldptr.w\t%0,%1\n\tlu32i.d\t%0,0
-   ld.wu\t%0,%1"
-  [(set_attr "move_type" "arith,load,load")
-   (set_attr "mode" "DI")
-   (set_attr "insn_count" "1,2,1")])
+   ld.wu\t%0,%1
+   #
+   ldx.wu\t%0,%1"
+  "&& reload_completed
+   && MEM_P (operands[1])
+   && (loongarch_14bit_shifted_offset_address_p (XEXP (operands[1], 0), SImode)
+       && !loongarch_12bit_offset_address_p (XEXP (operands[1], 0), SImode))
+   && !paradoxical_subreg_p (operands[0])"
+  [(set (match_dup 3) (match_dup 1))
+   (set (match_dup 0)
+	(ior:DI (zero_extend:DI (subreg:SI (match_dup 0) 0))
+		(match_dup 2)))]
+  {
+    operands[1] = gen_lowpart (SImode, operands[1]);
+    operands[3] = gen_lowpart (SImode, operands[0]);
+    operands[2] = const0_rtx;
+  }
+  [(set_attr "move_type" "arith,load,load,load")
+   (set_attr "mode" "DI")])
 
 (define_insn "zero_extend<SHORT:mode><GPR:mode>2"
-  [(set (match_operand:GPR 0 "register_operand" "=r,r")
+  [(set (match_operand:GPR 0 "register_operand" "=r,r,r")
 	(zero_extend:GPR
-	     (match_operand:SHORT 1 "nonimmediate_operand" "r,m")))]
+	     (match_operand:SHORT 1 "nonimmediate_operand" "r,m,k")))]
   ""
   "@
-   bstrpick.w\t%0,%1,<SHORT:qi_hi>,0
-   ld.<SHORT:size>u\t%0,%1"
-  [(set_attr "move_type" "pick_ins,load")
-   (set_attr "compression" "*,*")
+   bstrpick.w\t%0,%1,<SHORT:7_or_15>,0
+   ld.<SHORT:size>u\t%0,%1
+   ldx.<SHORT:size>u\t%0,%1"
+  [(set_attr "move_type" "pick_ins,load,load")
    (set_attr "mode" "<GPR:MODE>")])
 
 (define_insn "zero_extendqihi2"
-  [(set (match_operand:HI 0 "register_operand" "=r,r")
-	(zero_extend:HI (match_operand:QI 1 "nonimmediate_operand" "r,m")))]
+  [(set (match_operand:HI 0 "register_operand" "=r,r,r")
+	(zero_extend:HI (match_operand:QI 1 "nonimmediate_operand" "r,k,m")))]
   ""
   "@
    andi\t%0,%1,0xff
+   ldx.bu\t%0,%1
    ld.bu\t%0,%1"
-  [(set_attr "move_type" "andi,load")
+  [(set_attr "move_type" "andi,load,load")
    (set_attr "mode" "HI")])
 
 ;; Combiner patterns to optimize truncate/zero_extend combinations.
@@ -1333,7 +1298,7 @@
 	(zero_extend:GPR
 	    (truncate:SHORT (match_operand:DI 1 "register_operand" "r"))))]
   "TARGET_64BIT"
-  "bstrpick.w\t%0,%1,<SHORT:qi_hi>,0"
+  "bstrpick.w\t%0,%1,<SHORT:7_or_15>,0"
   [(set_attr "move_type" "pick_ins")
    (set_attr "mode" "<GPR:MODE>")])
 
@@ -1353,83 +1318,67 @@
 ;;
 ;;  ....................
 
-;; Extension insns.
-;; Those for integer source operand are ordered widest source type first.
-
-;; When TARGET_64BIT, all SImode integer should already be in sign-extended
-;; form (see TARGET_TRULY_NOOP_TRUNCATION and truncdisi2).  We can therefore
-;; get rid of register->register instructions if we constrain the source to
-;; be in the same register as the destination.
-;;
-;; Only the pre-reload scheduler sees the type of the register alternatives;
-;; we split them into nothing before the post-reload scheduler runs.
-;; These alternatives therefore have type "move" in order to reflect
-;; what happens if the two pre-reload operands cannot be tied, and are
-;; instead allocated two separate GPRs.
 (define_insn_and_split "extendsidi2"
-  [(set (match_operand:DI 0 "register_operand" "=r,r,r")
-	(sign_extend:DI (match_operand:SI 1 "nonimmediate_operand" "0,ZC,m")))]
+  [(set (match_operand:DI 0 "register_operand" "=r,r,r,r")
+	(sign_extend:DI
+	    (match_operand:SI 1 "nonimmediate_operand" "0,ZC,m,k")))]
   "TARGET_64BIT"
-  "@
-   #
-   ldptr.w\t%0,%1
-   ld.w\t%0,%1"
+{
+  switch (which_alternative)
+    {
+    case 0:
+      return "#";
+    case 1:
+      {
+      rtx offset = XEXP (operands[1], 0);
+      if (GET_CODE (offset) == PLUS)
+	offset = XEXP (offset, 1);
+      else
+	offset = const0_rtx;
+      if (const_arith_operand (offset, Pmode) || (offset == const0_rtx))
+	return "ld.w\t%0,%1";
+      else
+	return "ldptr.w\t%0,%1";
+      }
+    case 2:
+	return "ld.w\t%0,%1";
+    case 3:
+	return "ldx.w\t%0,%1";
+    default:
+      gcc_unreachable ();
+    }
+}
   "&& reload_completed && register_operand (operands[1], VOIDmode)"
   [(const_int 0)]
 {
   emit_note (NOTE_INSN_DELETED);
   DONE;
 }
-  [(set_attr "move_type" "move,load,load")
+  [(set_attr "move_type" "move,load,load,load")
    (set_attr "mode" "DI")])
 
 (define_insn "extend<SHORT:mode><GPR:mode>2"
-  [(set (match_operand:GPR 0 "register_operand" "=r,r")
+  [(set (match_operand:GPR 0 "register_operand" "=r,r,r")
 	(sign_extend:GPR
-	     (match_operand:SHORT 1 "nonimmediate_operand" "r,m")))]
+	     (match_operand:SHORT 1 "nonimmediate_operand" "r,m,k")))]
   ""
   "@
    ext.w.<SHORT:size>\t%0,%1
-   ld.<SHORT:size>\t%0,%1"
-  [(set_attr "move_type" "signext,load")
+   ld.<SHORT:size>\t%0,%1
+   ldx.<SHORT:size>\t%0,%1"
+  [(set_attr "move_type" "signext,load,load")
    (set_attr "mode" "<GPR:MODE>")])
 
 (define_insn "extendqihi2"
-  [(set (match_operand:HI 0 "register_operand" "=r,r")
+  [(set (match_operand:HI 0 "register_operand" "=r,r,r")
 	(sign_extend:HI
-	     (match_operand:QI 1 "nonimmediate_operand" "r,m")))]
+	     (match_operand:QI 1 "nonimmediate_operand" "r,m,k")))]
   ""
   "@
    ext.w.b\t%0,%1
-   ld.b\t%0,%1"
-  [(set_attr "move_type" "signext,load")
-   (set_attr "mode" "SI")])
-
-(define_insn "*extenddi_truncate<mode>"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(sign_extend:DI
-	    (truncate:SHORT (match_operand:DI 1 "register_operand" "r"))))]
-  "TARGET_64BIT"
-  "ext.w.<size>\t%0,%1"
-  [(set_attr "move_type" "signext")
-   (set_attr "mode" "DI")])
-
-(define_insn "*extendsi_truncate<mode>"
-  [(set (match_operand:SI 0 "register_operand" "=r")
-	(sign_extend:SI
-	    (truncate:SHORT (match_operand:DI 1 "register_operand" "r"))))]
-  "TARGET_64BIT"
-  "ext.w.<size>\t%0,%1"
-  [(set_attr "move_type" "signext")
-   (set_attr "mode" "SI")])
-
-(define_insn "*extendhi_truncateqi"
-  [(set (match_operand:HI 0 "register_operand" "=r")
-	(sign_extend:HI
-	    (truncate:QI (match_operand:DI 1 "register_operand" "r"))))]
-  "TARGET_64BIT"
-  "ext.w.b\t%0,%1"
-  [(set_attr "move_type" "signext")
+   ld.b\t%0,%1
+   ldx.b\t%0,%1"
+  [(set_attr "move_type" "signext,load,load")
    (set_attr "mode" "SI")])
 
 (define_insn "extendsfdf2"
@@ -1450,43 +1399,13 @@
 
 ;; conversion of a floating-point value to a integer
 
-(define_insn "fix_truncdfsi2"
-  [(set (match_operand:SI 0 "register_operand" "=f")
-	(fix:SI (match_operand:DF 1 "register_operand" "f")))]
-  "TARGET_DOUBLE_FLOAT"
-  "ftintrz.w.d %0,%1"
-  [(set_attr "type" "fcvt")
-   (set_attr "mode" "DF")
-   (set_attr "cnv_mode"	"D2I")])
-
-(define_insn "fix_truncsfsi2"
-  [(set (match_operand:SI 0 "register_operand" "=f")
-	(fix:SI (match_operand:SF 1 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
-  "ftintrz.w.s %0,%1"
-  [(set_attr "type" "fcvt")
-   (set_attr "mode" "SF")
-   (set_attr "cnv_mode"	"S2I")])
-
-
-(define_insn "fix_truncdfdi2"
-  [(set (match_operand:DI 0 "register_operand" "=f")
-	(fix:DI (match_operand:DF 1 "register_operand" "f")))]
-  "TARGET_DOUBLE_FLOAT"
-  "ftintrz.l.d %0,%1"
-  [(set_attr "type" "fcvt")
-   (set_attr "mode" "DF")
-   (set_attr "cnv_mode"	"D2I")])
-
-
-(define_insn "fix_truncsfdi2"
-  [(set (match_operand:DI 0 "register_operand" "=f")
-	(fix:DI (match_operand:SF 1 "register_operand" "f")))]
-  "TARGET_DOUBLE_FLOAT"
-  "ftintrz.l.s %0,%1"
+(define_insn "fix_trunc<ANYF:mode><GPR:mode>2"
+  [(set (match_operand:GPR 0 "register_operand" "=f")
+	(fix:GPR (match_operand:ANYF 1 "register_operand" "f")))]
+  ""
+  "ftintrz.<GPR:ifmt>.<ANYF:fmt>\t%0,%1"
   [(set_attr "type" "fcvt")
-   (set_attr "mode" "SF")
-   (set_attr "cnv_mode"	"S2I")])
+   (set_attr "mode" "<ANYF:MODE>")])
 
 ;; conversion of an integral (or boolean) value to a floating-point value
 
@@ -1526,7 +1445,7 @@
    (set_attr "mode" "SF")
    (set_attr "cnv_mode"	"I2S")])
 
-;; floating point value by converting to value to an unsigned integer
+;; Convert a floating-point value to an unsigned integer.
 
 (define_expand "fixuns_truncdfsi2"
   [(set (match_operand:SI 0 "register_operand")
@@ -1543,35 +1462,32 @@
 
   real_2expN (&offset, 31, DFmode);
 
-  if (reg1)		      /* Turn off complaints about unreached code.  */
-    {
-      loongarch_emit_move (reg1,
-			   const_double_from_real_value (offset, DFmode));
-      do_pending_stack_adjust ();
+  loongarch_emit_move (reg1,
+		       const_double_from_real_value (offset, DFmode));
+  do_pending_stack_adjust ();
 
-      test = gen_rtx_GE (VOIDmode, operands[1], reg1);
-      emit_jump_insn (gen_cbranchdf4 (test, operands[1], reg1, label1));
+  test = gen_rtx_GE (VOIDmode, operands[1], reg1);
+  emit_jump_insn (gen_cbranchdf4 (test, operands[1], reg1, label1));
 
-      emit_insn (gen_fix_truncdfsi2 (operands[0], operands[1]));
-      emit_jump_insn (gen_rtx_SET (pc_rtx,
-				   gen_rtx_LABEL_REF (VOIDmode, label2)));
-      emit_barrier ();
+  emit_insn (gen_fix_truncdfsi2 (operands[0], operands[1]));
+  emit_jump_insn (gen_rtx_SET (pc_rtx,
+			       gen_rtx_LABEL_REF (VOIDmode, label2)));
+  emit_barrier ();
 
-      emit_label (label1);
-      loongarch_emit_move (reg2, gen_rtx_MINUS (DFmode, operands[1], reg1));
-      loongarch_emit_move (reg3, GEN_INT (trunc_int_for_mode
-				     (BITMASK_HIGH, SImode)));
+  emit_label (label1);
+  loongarch_emit_move (reg2, gen_rtx_MINUS (DFmode, operands[1], reg1));
+  loongarch_emit_move (reg3, GEN_INT (trunc_int_for_mode
+				 (BITMASK_HIGH, SImode)));
 
-      emit_insn (gen_fix_truncdfsi2 (operands[0], reg2));
-      emit_insn (gen_iorsi3 (operands[0], operands[0], reg3));
+  emit_insn (gen_fix_truncdfsi2 (operands[0], reg2));
+  emit_insn (gen_iorsi3 (operands[0], operands[0], reg3));
 
-      emit_label (label2);
+  emit_label (label2);
 
-      /* Allow REG_NOTES to be set on last insn (labels don't have enough
-	 fields, and can't be used for REG_NOTES anyway).  */
-      emit_use (stack_pointer_rtx);
-      DONE;
-    }
+  /* Allow REG_NOTES to be set on last insn (labels don't have enough
+     fields, and can't be used for REG_NOTES anyway).  */
+  emit_use (stack_pointer_rtx);
+  DONE;
 })
 
 (define_expand "fixuns_truncdfdi2"
@@ -1705,24 +1621,24 @@
 ;;  ....................
 
 (define_expand "extzv<mode>"
-  [(set (match_operand:GPR 0 "register_operand")
-	(zero_extract:GPR (match_operand:GPR 1 "register_operand")
-			  (match_operand 2 "const_int_operand")
-			  (match_operand 3 "const_int_operand")))]
+  [(set (match_operand:X 0 "register_operand")
+	(zero_extract:X (match_operand:X 1 "register_operand")
+			(match_operand 2 "const_int_operand")
+			(match_operand 3 "const_int_operand")))]
   ""
 {
   if (!loongarch_use_ins_ext_p (operands[1], INTVAL (operands[2]),
-			   INTVAL (operands[3])))
+				INTVAL (operands[3])))
     FAIL;
 })
 
 (define_insn "*extzv<mode>"
-  [(set (match_operand:GPR 0 "register_operand" "=r")
-	(zero_extract:GPR (match_operand:GPR 1 "register_operand" "r")
-			  (match_operand 2 "const_int_operand" "")
-			  (match_operand 3 "const_int_operand" "")))]
+  [(set (match_operand:X 0 "register_operand" "=r")
+	(zero_extract:X (match_operand:X 1 "register_operand" "r")
+			(match_operand 2 "const_int_operand" "")
+			(match_operand 3 "const_int_operand" "")))]
   "loongarch_use_ins_ext_p (operands[1], INTVAL (operands[2]),
-		       INTVAL (operands[3]))"
+			    INTVAL (operands[3]))"
 {
   operands[2] = GEN_INT (INTVAL (operands[2]) + INTVAL (operands[3]) - 1);
   return "bstrpick.<d>\t%0,%1,%2,%3";
@@ -1738,7 +1654,7 @@
   ""
 {
   if (!loongarch_use_ins_ext_p (operands[0], INTVAL (operands[1]),
-			   INTVAL (operands[2])))
+				INTVAL (operands[2])))
     FAIL;
 })
 
@@ -1748,7 +1664,7 @@
 			  (match_operand:SI 2 "const_int_operand" ""))
 	(match_operand:GPR 3 "reg_or_0_operand" "rJ"))]
   "loongarch_use_ins_ext_p (operands[0], INTVAL (operands[1]),
-		       INTVAL (operands[2]))"
+			    INTVAL (operands[2]))"
 {
   operands[1] = GEN_INT (INTVAL (operands[1]) + INTVAL (operands[2]) - 1);
   return "bstrins.<d>\t%0,%z3,%1,%2";
@@ -1763,19 +1679,6 @@
 ;;
 ;;  ....................
 
-;; Allow combine to split complex const_int load sequences, using operand 2
-;; to store the intermediate results.  See move_operand for details.
-(define_split
-  [(set (match_operand:GPR 0 "register_operand")
-	(match_operand:GPR 1 "splittable_const_int_operand"))
-   (clobber (match_operand:GPR 2 "register_operand"))]
-  ""
-  [(const_int 0)]
-{
-  loongarch_move_integer (operands[2], operands[0], INTVAL (operands[1]));
-  DONE;
-})
-
 ;; 64-bit integer moves
 
 ;; Unlike most other insns, the move insns can't be split with
@@ -1792,23 +1695,23 @@
 })
 
 (define_insn "*movdi_32bit"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,ZC,r,m,*f,*f,*r,*m")
-       (match_operand:DI 1 "move_operand" "r,i,ZC,r,m,r,*J*r,*m,*f,*f"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,w,*f,*f,*r,*m")
+       (match_operand:DI 1 "move_operand" "r,i,w,r,*J*r,*m,*f,*f"))]
   "!TARGET_64BIT
    && (register_operand (operands[0], DImode)
        || reg_or_0_operand (operands[1], DImode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store,load,store,mgtf,fpload,mftg,fpstore")
+  [(set_attr "move_type" "move,const,load,store,mgtf,fpload,mftg,fpstore")
    (set_attr "mode" "DI")])
 
 (define_insn "*movdi_64bit"
-  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,ZC,r,m,*f,*f,*r,*m")
-	(match_operand:DI 1 "move_operand" "r,Yd,ZC,rJ,m,rJ,*r*J,*m,*f,*f"))]
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=r,r,r,w,*f,*f,*r,*m")
+	(match_operand:DI 1 "move_operand" "r,Yd,w,rJ,*r*J,*m,*f,*f"))]
   "TARGET_64BIT
    && (register_operand (operands[0], DImode)
        || reg_or_0_operand (operands[1], DImode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store,load,store,mgtf,fpload,mftg,fpstore")
+  [(set_attr "move_type" "move,const,load,store,mgtf,fpload,mftg,fpstore")
    (set_attr "mode" "DI")])
 
 ;; 32-bit Integer moves
@@ -1823,13 +1726,12 @@
 })
 
 (define_insn "*movsi_internal"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,r,r,ZC,r,m,*f,*f,*r,*m,*r,*z")
-	(match_operand:SI 1 "move_operand" "r,Yd,ZC,rJ,m,rJ,*r*J,*m,*f,*f,*z,*r"))]
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=r,r,r,w,*f,*f,*r,*m,*r,*z")
+	(match_operand:SI 1 "move_operand" "r,Yd,w,rJ,*r*J,*m,*f,*f,*z,*r"))]
   "(register_operand (operands[0], SImode)
        || reg_or_0_operand (operands[1], SImode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store,load,store,mgtf,fpload,mftg,fpstore,mftg,mgtf")
-   (set_attr "compression" "all,*,*,*,*,*,*,*,*,*,*,*")
+  [(set_attr "move_type" "move,const,load,store,mgtf,fpload,mftg,fpstore,mftg,mgtf")
    (set_attr "mode" "SI")])
 
 ;; 16-bit Integer moves
@@ -1849,13 +1751,12 @@
 })
 
 (define_insn "*movhi_internal"
-  [(set (match_operand:HI 0 "nonimmediate_operand" "=r,r,r,r,m")
-	(match_operand:HI 1 "move_operand" "r,Yd,I,m,rJ"))]
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=r,r,r,r,m,r,k")
+	(match_operand:HI 1 "move_operand" "r,Yd,I,m,rJ,k,rJ"))]
   "(register_operand (operands[0], HImode)
        || reg_or_0_operand (operands[1], HImode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,const,load,store")
-   (set_attr "compression" "all,all,*,*,*")
+  [(set_attr "move_type" "move,const,const,load,store,load,store")
    (set_attr "mode" "HI")])
 
 ;; 8-bit Integer moves
@@ -1875,13 +1776,12 @@
 })
 
 (define_insn "*movqi_internal"
-  [(set (match_operand:QI 0 "nonimmediate_operand" "=r,r,r,m")
-	(match_operand:QI 1 "move_operand" "r,I,m,rJ"))]
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=r,r,r,m,r,k")
+	(match_operand:QI 1 "move_operand" "r,I,m,rJ,k,rJ"))]
   "(register_operand (operands[0], QImode)
        || reg_or_0_operand (operands[1], QImode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "move,const,load,store")
-   (set_attr "compression" "all,*,*,*")
+  [(set_attr "move_type" "move,const,load,store,load,store")
    (set_attr "mode" "QI")])
 
 ;; 32-bit floating point moves
@@ -1896,13 +1796,13 @@
 })
 
 (define_insn "*movsf_hardfloat"
-  [(set (match_operand:SF 0 "nonimmediate_operand" "=f,f,f,m,m,*f,*r,*r,*r,*m")
-	(match_operand:SF 1 "move_operand" "f,G,m,f,G,*r,*f,*G*r,*m,*r"))]
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=f,f,f,m,f,k,m,*f,*r,*r,*r,*m")
+	(match_operand:SF 1 "move_operand" "f,G,m,f,k,f,G,*r,*f,*G*r,*m,*r"))]
   "TARGET_HARD_FLOAT
    && (register_operand (operands[0], SFmode)
        || reg_or_0_operand (operands[1], SFmode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "fmove,mgtf,fpload,fpstore,store,mgtf,mftg,move,load,store")
+  [(set_attr "move_type" "fmove,mgtf,fpload,fpstore,fpload,fpstore,store,mgtf,mftg,move,load,store")
    (set_attr "mode" "SF")])
 
 (define_insn "*movsf_softfloat"
@@ -1927,13 +1827,13 @@
 })
 
 (define_insn "*movdf_hardfloat"
-  [(set (match_operand:DF 0 "nonimmediate_operand" "=f,f,f,m,m,*f,*r,*r,*r,*m")
-	(match_operand:DF 1 "move_operand" "f,G,m,f,G,*r,*f,*r*G,*m,*r"))]
+  [(set (match_operand:DF 0 "nonimmediate_operand" "=f,f,f,m,f,k,m,*f,*r,*r,*r,*m")
+	(match_operand:DF 1 "move_operand" "f,G,m,f,k,f,G,*r,*f,*r*G,*m,*r"))]
   "TARGET_DOUBLE_FLOAT
    && (register_operand (operands[0], DFmode)
        || reg_or_0_operand (operands[1], DFmode))"
   { return loongarch_output_move (operands[0], operands[1]); }
-  [(set_attr "move_type" "fmove,mgtf,fpload,fpstore,store,mgtf,mftg,move,load,store")
+  [(set_attr "move_type" "fmove,mgtf,fpload,fpstore,fpload,fpstore,store,mgtf,mftg,move,load,store")
    (set_attr "mode" "DF")])
 
 (define_insn "*movdf_softfloat"
@@ -1996,7 +1896,7 @@
 (define_split
   [(set (match_operand:MOVE64 0 "nonimmediate_operand")
 	(match_operand:MOVE64 1 "move_operand"))]
-  "reload_completed && loongarch_split_move_insn_p (operands[0], operands[1], insn)"
+  "reload_completed && loongarch_split_move_insn_p (operands[0], operands[1])"
   [(const_int 0)]
 {
   loongarch_split_move_insn (operands[0], operands[1], curr_insn);
@@ -2006,7 +1906,7 @@
 (define_split
   [(set (match_operand:MOVE128 0 "nonimmediate_operand")
 	(match_operand:MOVE128 1 "move_operand"))]
-  "reload_completed && loongarch_split_move_insn_p (operands[0], operands[1], insn)"
+  "reload_completed && loongarch_split_move_insn_p (operands[0], operands[1])"
   [(const_int 0)]
 {
   loongarch_split_move_insn (operands[0], operands[1], curr_insn);
@@ -2048,6 +1948,14 @@
   DONE;
 })
 
+;; Clear one FCC register
+
+(define_insn "movfcc"
+  [(set (match_operand:FCC 0 "register_operand" "=z")
+	(const_int 0))]
+  ""
+  "movgr2cf\t%0,$r0")
+
 ;; Conditional move instructions.
 
 (define_insn "*sel<code><GPR:mode>_using_<GPR2:mode>"
@@ -2058,26 +1966,26 @@
 	 (match_operand:GPR 2 "reg_or_0_operand" "r,J")
 	 (match_operand:GPR 3 "reg_or_0_operand" "J,r")))]
   "register_operand (operands[2], <GPR:MODE>mode)
-       != register_operand (operands[3], <GPR:MODE>mode)"
+   != register_operand (operands[3], <GPR:MODE>mode)"
   "@
    <sel>\t%0,%2,%1
    <selinv>\t%0,%3,%1"
   [(set_attr "type" "condmove")
    (set_attr "mode" "<GPR:MODE>")])
 
-;; sel.fmt copies the 3rd argument when the 1st is non-zero and the 2nd
+;; fsel copies the 3rd argument when the 1st is non-zero and the 2nd
 ;; argument if the 1st is zero.  This means operand 2 and 3 are
 ;; inverted in the instruction.
 
 (define_insn "*sel<mode>"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(if_then_else:ANYF
-	 (ne:FCC (match_operand:FCC 1 "register_operand" "z")
+	 (equality_op:FCC (match_operand:FCC 1 "register_operand" "z")
 		 (const_int 0))
 	 (match_operand:ANYF 2 "reg_or_0_operand" "f")
 	 (match_operand:ANYF 3 "reg_or_0_operand" "f")))]
   "TARGET_HARD_FLOAT"
-  "fsel\t%0,%3,%2,%1"
+  "fsel\t%0,<fsel_invert>,%1"
   [(set_attr "type" "condmove")
    (set_attr "mode" "<ANYF:MODE>")])
 
@@ -2090,11 +1998,10 @@
 			  (match_operand:GPR 3 "reg_or_0_operand")])))]
   "TARGET_COND_MOVE_INT"
 {
-  if (!INTEGRAL_MODE_P (GET_MODE (XEXP (operands[1], 0))))
-    FAIL;
-
-  loongarch_expand_conditional_move (operands);
-  DONE;
+  if(loongarch_expand_conditional_move_la464 (operands))
+	DONE;
+   else
+	FAIL;
 })
 
 (define_expand "mov<mode>cc"
@@ -2104,11 +2011,11 @@
 			   (match_operand:ANYF 3 "reg_or_0_operand")])))]
   "TARGET_COND_MOVE_FLOAT"
 {
-  if (!FLOAT_MODE_P (GET_MODE (XEXP (operands[1], 0))))
-    FAIL;
 
-  loongarch_expand_conditional_move (operands);
-  DONE;
+  if(loongarch_expand_conditional_move_la464 (operands))
+	DONE;
+   else
+	FAIL;
 })
 
 (define_insn "lu32i_d"
@@ -2128,95 +2035,21 @@
 	  (and:DI (match_operand:DI 1 "register_operand" "r")
 		  (match_operand 2 "lu52i_mask_operand"))
 	  (match_operand 3 "const_lu52i_operand" "v")))]
-    "TARGET_64BIT"
-    "lu52i.d\t%0,%1,%X3>>52"
-    [(set_attr "type" "arith")
-     (set_attr "mode" "DI")])
+  "TARGET_64BIT"
+  "lu52i.d\t%0,%1,%X3>>52"
+  [(set_attr "type" "arith")
+   (set_attr "mode" "DI")])
 
 ;; Convert floating-point numbers to integers
 (define_insn "frint_<fmt>"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
 	(unspec:ANYF [(match_operand:ANYF 1 "register_operand" "f")]
 		      UNSPEC_FRINT))]
-  "TARGET_HARD_FLOAT"
+  ""
   "frint.<fmt>\t%0,%1"
   [(set_attr "type" "fcvt")
    (set_attr "mode" "<MODE>")])
 
-;; LoongArch supports loading and storing a floating point register from
-;; the sum of two general-purpose registers.  We use two versions for each of
-;; these four instructions: one where the two general-purpose registers are
-;; SImode, and one where they are DImode.  This is because general-purpose
-;; registers will be in SImode when they hold 32-bit values, but,
-;; since the 32-bit values are always sign extended, the f{ld/st}x.{s/d}
-;; instructions will still work correctly.
-
-;; ??? Perhaps it would be better to support these instructions by
-;; modifying TARGET_LEGITIMATE_ADDRESS_P and friends.  However, since
-;; these instructions can only be used to load and store floating
-;; point registers, that would probably cause trouble in reload.
-
-(define_insn "*<ANYF:floadx>_<P:mode>"
-  [(set (match_operand:ANYF 0 "register_operand" "=f")
-	(mem:ANYF (plus:P (match_operand:P 1 "register_operand" "r")
-			  (match_operand:P 2 "register_operand" "r"))))]
-  "TARGET_HARD_FLOAT"
-  "<ANYF:floadx>\t%0,%1,%2"
-  [(set_attr "type" "fpidxload")
-   (set_attr "mode" "<ANYF:UNITMODE>")])
-
-(define_insn "*<ANYF:fstorex>_<P:mode>"
-  [(set (mem:ANYF (plus:P (match_operand:P 1 "register_operand" "r")
-			  (match_operand:P 2 "register_operand" "r")))
-	(match_operand:ANYF 0 "register_operand" "f"))]
-  "TARGET_HARD_FLOAT"
-  "<ANYF:fstorex>\t%0,%1,%2"
-  [(set_attr "type" "fpidxstore")
-   (set_attr "mode" "<ANYF:UNITMODE>")])
-
-;; loading and storing a integer register from the sum of two general-purpose
-;; registers.
-
-(define_insn "*<GPR:loadx>_<P:mode>"
-  [(set (match_operand:GPR 0 "register_operand" "=r")
-	(mem:GPR
-	    (plus:P (match_operand:P 1 "register_operand" "r")
-		    (match_operand:P 2 "register_operand" "r"))))]
-  ""
-  "<GPR:loadx>\t%0,%1,%2"
-  [(set_attr "type" "load")
-   (set_attr "mode" "<GPR:MODE>")])
-
-(define_insn "*<GPR:storex>_<P:mode>"
-  [(set (mem:GPR (plus:P (match_operand:P 1 "register_operand" "r")
-			 (match_operand:P 2 "register_operand" "r")))
-	(match_operand:GPR 0 "register_operand" "r"))]
-  ""
-  "<GPR:storex>\t%0,%1,%2"
-  [(set_attr "type" "store")
-   (set_attr "mode" "<GPR:MODE>")])
-
-;; SHORT mode sign_extend.
-(define_insn "*extend_<SHORT:loadx>_<GPR:mode>"
-  [(set (match_operand:GPR 0 "register_operand" "=r")
-	(sign_extend:GPR
-	  (mem:SHORT
-	    (plus:P (match_operand:P 1 "register_operand" "r")
-		    (match_operand:P 2 "register_operand" "r")))))]
-  ""
-  "<SHORT:loadx>\t%0,%1,%2"
-  [(set_attr "type" "load")
-   (set_attr "mode" "<GPR:MODE>")])
-
-(define_insn "*extend_<SHORT:storex>"
-  [(set (mem:SHORT (plus:P (match_operand:P 1 "register_operand" "r")
-			   (match_operand:P 2 "register_operand" "r")))
-	(match_operand:SHORT 0 "register_operand" "r"))]
-  ""
-  "<SHORT:storex>\t%0,%1,%2"
-  [(set_attr "type" "store")
-   (set_attr "mode" "SI")])
-
 ;; Load the low word of operand 0 with operand 1.
 (define_insn "load_low<mode>"
   [(set (match_operand:SPLITF 0 "register_operand" "=f,f")
@@ -2262,7 +2095,7 @@
 
 ;; Thread-Local Storage
 
-(define_insn "got_load_tls_gd<mode>"
+(define_insn "got_load_tls_gd<P:mode>"
   [(set (match_operand:P 0 "register_operand" "=r")
 	(unspec:P
 	    [(match_operand:P 1 "symbolic_operand" "")]
@@ -2272,7 +2105,7 @@
   [(set_attr "got" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "got_load_tls_ld<mode>"
+(define_insn "got_load_tls_ld<P:mode>"
   [(set (match_operand:P 0 "register_operand" "=r")
 	(unspec:P
 	    [(match_operand:P 1 "symbolic_operand" "")]
@@ -2282,7 +2115,7 @@
   [(set_attr "got" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "got_load_tls_le<mode>"
+(define_insn "got_load_tls_le<P:mode>"
   [(set (match_operand:P 0 "register_operand" "=r")
 	(unspec:P
 	    [(match_operand:P 1 "symbolic_operand" "")]
@@ -2292,7 +2125,7 @@
   [(set_attr "got" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "got_load_tls_ie<mode>"
+(define_insn "got_load_tls_ie<P:mode>"
   [(set (match_operand:P 0 "register_operand" "=r")
 	(unspec:P
 	    [(match_operand:P 1 "symbolic_operand" "")]
@@ -2314,6 +2147,21 @@
   [(set_attr "move_type" "mgtf")
    (set_attr "mode" "<HALFMODE>")])
 
+(define_insn "movsgr2fr<mode>"
+  [(set (match_operand:ANYF 0 "register_operand" "=f")
+	(unspec:ANYF [(match_operand:SI 1 "register_operand" "r")]
+			UNSPEC_MOVGR2FR))]
+  "TARGET_DOUBLE_FLOAT"
+  "movgr2fr.w\t%0,%1"
+  )
+(define_insn "movdgr2fr<mode>"
+  [(set (match_operand:ANYF 0 "register_operand" "=f")
+	(unspec:ANYF [(match_operand:DI 1 "register_operand" "r")]
+			UNSPEC_MOVGR2FR))]
+  "TARGET_DOUBLE_FLOAT"
+  "movgr2fr.d\t%0,%1"
+  )
+
 ;; Move high word of operand 1 to operand 0 using movfrh2gr.s.
 (define_insn "movfrh2gr<mode>"
   [(set (match_operand:<HALFMODE> 0 "register_operand" "=r")
@@ -2324,6 +2172,45 @@
   [(set_attr "move_type" "mftg")
    (set_attr "mode" "<HALFMODE>")])
 
+(define_insn "movsfr2gr<mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(unspec:GPR [(match_operand:SF 1 "register_operand" "f")]
+			    UNSPEC_MOVFR2GR))]
+  "TARGET_DOUBLE_FLOAT"
+  "movfr2gr.s\t%0,%1"
+  )
+(define_insn "movdfr2gr<mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(unspec:GPR [(match_operand:DF 1 "register_operand" "f")]
+			    UNSPEC_MOVFR2GR))]
+  "TARGET_DOUBLE_FLOAT"
+  "movfr2gr.d\t%0,%1"
+  )
+
+(define_insn "movfr2fcc<mode>"
+  [(set (match_operand:FCC 0 "register_operand" "=z")
+       (unspec:FCC [(match_operand:ANYF 1 "register_operand" "f")]
+                           UNSPEC_MOVFR2FCC))]
+  "TARGET_HARD_FLOAT"
+  "movfr2cf\t%0,%1"
+  [(set_attr "mode" "<MODE>")])
+
+(define_insn "movgr2fcc<mode>"
+  [(set (match_operand:FCC 0 "register_operand" "=z")
+       (unspec:FCC [(match_operand:GPR 1 "register_operand" "r")]
+                           UNSPEC_MOVGR2FCC))]
+  "TARGET_HARD_FLOAT"
+  "movgr2cf\t%0,%1"
+  [(set_attr "mode" "<MODE>")])
+
+(define_insn "movfcc2gr<mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+       (unspec:GPR [(match_operand:FCC 1 "register_operand" "z")]
+                           UNSPEC_MOVFCC2GR))]
+  "TARGET_HARD_FLOAT"
+  "movcf2gr\t%0,%1"
+  [ (set_attr "mode" "<MODE>")])
+
 
 ;; Expand in-line code to clear the instruction cache between operand[0] and
 ;; operand[1].
@@ -2331,19 +2218,24 @@
   [(match_operand 0 "pmode_register_operand")
    (match_operand 1 "pmode_register_operand")]
   ""
-  "
 {
-  emit_insn (gen_ibar (const0_rtx));
+  emit_insn (gen_loongarch_ibar (const0_rtx));
   DONE;
-}")
+})
 
-(define_insn "ibar"
-  [(unspec_volatile:SI [(match_operand 0 "const_uimm15_operand")] UNSPECV_IBAR)]
+(define_insn "loongarch_ibar"
+  [(unspec_volatile:SI
+      [(match_operand 0 "const_uimm15_operand")]
+       UNSPECV_IBAR)
+   (clobber (mem:BLK (scratch)))]
   ""
   "ibar\t%0")
 
-(define_insn "dbar"
-  [(unspec_volatile:SI [(match_operand 0 "const_uimm15_operand")] UNSPECV_DBAR)]
+(define_insn "loongarch_dbar"
+  [(unspec_volatile:SI
+      [(match_operand 0 "const_uimm15_operand")]
+       UNSPECV_DBAR)
+   (clobber (mem:BLK (scratch)))]
   ""
   "dbar\t%0")
 
@@ -2351,7 +2243,7 @@
 
 ;; Privileged state instruction
 
-(define_insn "cpucfg"
+(define_insn "loongarch_cpucfg"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(unspec_volatile:SI [(match_operand:SI 1 "register_operand" "r")]
 			     UNSPECV_CPUCFG))]
@@ -2360,98 +2252,122 @@
   [(set_attr "type" "load")
    (set_attr "mode" "SI")])
 
-(define_insn "asrtle_d"
-	[(unspec_volatile:DI [(match_operand:DI 0 "register_operand" "r")
-			      (match_operand:DI 1 "register_operand" "r")]
-			      UNSPECV_ASRTLE_D)]
+(define_insn "loongarch_syscall"
+  [(unspec_volatile:SI
+      [(match_operand 0 "const_uimm15_operand")]
+       UNSPECV_SYSCALL)
+   (clobber (mem:BLK (scratch)))]
+  ""
+  "syscall\t%0")
+
+(define_insn "loongarch_break"
+  [(unspec_volatile:SI
+      [(match_operand 0 "const_uimm15_operand")]
+       UNSPECV_BREAK)
+   (clobber (mem:BLK (scratch)))]
+  ""
+  "break\t%0")
+
+(define_insn "loongarch_asrtle_d"
+  [(unspec_volatile:DI [(match_operand:DI 0 "register_operand" "r")
+			(match_operand:DI 1 "register_operand" "r")]
+		       UNSPECV_ASRTLE_D)]
   "TARGET_64BIT"
   "asrtle.d\t%0,%1"
   [(set_attr "type" "load")
    (set_attr "mode" "DI")])
 
-(define_insn "asrtgt_d"
-	[(unspec_volatile:DI [(match_operand:DI 0 "register_operand" "r")
-			      (match_operand:DI 1 "register_operand" "r")]
-			      UNSPECV_ASRTGT_D)]
+(define_insn "loongarch_asrtgt_d"
+  [(unspec_volatile:DI [(match_operand:DI 0 "register_operand" "r")
+			(match_operand:DI 1 "register_operand" "r")]
+		       UNSPECV_ASRTGT_D)]
   "TARGET_64BIT"
   "asrtgt.d\t%0,%1"
   [(set_attr "type" "load")
    (set_attr "mode" "DI")])
 
-(define_insn "<p>csrrd"
+(define_insn "loongarch_csrrd_<d>"
   [(set (match_operand:GPR 0 "register_operand" "=r")
 	(unspec_volatile:GPR [(match_operand  1 "const_uimm14_operand")]
-			     UNSPECV_CSRRD))]
+			     UNSPECV_CSRRD))
+   (clobber (mem:BLK (scratch)))]
   ""
   "csrrd\t%0,%1"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "<p>csrwr"
+(define_insn "loongarch_csrwr_<d>"
   [(set (match_operand:GPR 0 "register_operand" "=r")
 	  (unspec_volatile:GPR
-	  [(match_operand:GPR 1 "register_operand" "0")
-	   (match_operand 2 "const_uimm14_operand")]
-	  UNSPECV_CSRWR))]
+	    [(match_operand:GPR 1 "register_operand" "0")
+	     (match_operand 2 "const_uimm14_operand")]
+	    UNSPECV_CSRWR))
+   (clobber (mem:BLK (scratch)))]
   ""
   "csrwr\t%0,%2"
   [(set_attr "type" "store")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "<p>csrxchg"
+(define_insn "loongarch_csrxchg_<d>"
   [(set (match_operand:GPR 0 "register_operand" "=r")
 	  (unspec_volatile:GPR
-	  [(match_operand:GPR 1 "register_operand" "0")
-	   (match_operand:GPR 2 "register_operand" "q")
-	   (match_operand 3 "const_uimm14_operand")]
-	  UNSPECV_CSRXCHG))]
+	    [(match_operand:GPR 1 "register_operand" "0")
+	     (match_operand:GPR 2 "register_operand" "q")
+	     (match_operand 3 "const_uimm14_operand")]
+	    UNSPECV_CSRXCHG))
+   (clobber (mem:BLK (scratch)))]
   ""
   "csrxchg\t%0,%2,%3"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "iocsrrd_<size>"
+(define_insn "loongarch_iocsrrd_<size>"
   [(set (match_operand:QHWD 0 "register_operand" "=r")
 	(unspec_volatile:QHWD [(match_operand:SI 1 "register_operand" "r")]
-			      UNSPECV_IOCSRRD))]
+			      UNSPECV_IOCSRRD))
+   (clobber (mem:BLK (scratch)))]
   ""
   "iocsrrd.<size>\t%0,%1"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "iocsrwr_<size>"
+(define_insn "loongarch_iocsrwr_<size>"
   [(unspec_volatile:QHWD [(match_operand:QHWD 0 "register_operand" "r")
 			  (match_operand:SI 1 "register_operand" "r")]
-			UNSPECV_IOCSRWR)]
+			 UNSPECV_IOCSRWR)
+   (clobber (mem:BLK (scratch)))]
   ""
   "iocsrwr.<size>\t%0,%1"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "<p>cacop"
+(define_insn "loongarch_cacop_<d>"
   [(unspec_volatile:X [(match_operand 0 "const_uimm5_operand")
-			 (match_operand:X 1 "register_operand" "r")
-			 (match_operand 2 "const_imm12_operand")]
-			 UNSPECV_CACOP)]
+		       (match_operand:X 1 "register_operand" "r")
+		       (match_operand 2 "const_imm12_operand")]
+		      UNSPECV_CACOP)
+   (clobber (mem:BLK (scratch)))]
   ""
   "cacop\t%0,%1,%2"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "<p>lddir"
+(define_insn "loongarch_lddir_<d>"
   [(unspec_volatile:X [(match_operand:X 0 "register_operand" "r")
-			 (match_operand:X 1 "register_operand" "r")
-			 (match_operand 2 "const_uimm5_operand")]
-			 UNSPECV_LDDIR)]
+		       (match_operand:X 1 "register_operand" "r")
+		       (match_operand 2 "const_uimm5_operand")]
+		      UNSPECV_LDDIR)
+   (clobber (mem:BLK (scratch)))]
   ""
   "lddir\t%0,%1,%2"
   [(set_attr "type" "load")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "<p>ldpte"
+(define_insn "loongarch_ldpte_<d>"
   [(unspec_volatile:X [(match_operand:X 0 "register_operand" "r")
 			 (match_operand 1 "const_uimm5_operand")]
-			 UNSPECV_LDPTE)]
+			 UNSPECV_LDPTE)
+   (clobber (mem:BLK (scratch)))]
   ""
   "ldpte\t%0,%1"
   [(set_attr "type" "load")
@@ -2464,7 +2380,7 @@
 ;; Argument 2 is the length.
 ;; Argument 3 is the alignment.
 
-(define_expand "cpymemsi"
+(define_expand "movmemsi"
   [(parallel [(set (match_operand:BLK 0 "general_operand")
 		   (match_operand:BLK 1 "general_operand"))
 	      (use (match_operand:SI 2 ""))
@@ -2497,7 +2413,6 @@
   return "<insn>%i2.<d>\t%0,%1,%2";
 }
   [(set_attr "type" "shift")
-   (set_attr "compression" "none")
    (set_attr "mode" "<MODE>")])
 
 (define_insn "*<optab>si3_extend"
@@ -2531,19 +2446,7 @@
 ;;
 ;; (immediate_operand >> const_immalsl_operand) == 0xffffffff
 
-(define_insn "zero_extend_ashift1"
-  [(set (match_operand:DI 0 "register_operand" "=r")
-	(and:DI (ashift:DI (subreg:DI (match_operand:SI 1 "register_operand" "r") 0)
-			   (match_operand 2 "const_immalsl_operand" ""))
-		(match_operand 3 "immediate_operand" "")))]
-  "TARGET_64BIT
-   && ((INTVAL (operands[3]) >> INTVAL (operands[2])) == 0xffffffff)"
-  "bstrpick.d\t%0,%1,31,0\n\talsl.d\t%0,%0,$r0,%2"
-  [(set_attr "type" "arith")
-   (set_attr "mode" "DI")
-   (set_attr "insn_count" "2")])
-
-(define_insn "zero_extend_ashift2"
+(define_insn "zero_extend_ashift"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	(and:DI (ashift:DI (match_operand:DI 1 "register_operand" "r")
 			   (match_operand 2 "const_immalsl_operand" ""))
@@ -2555,20 +2458,7 @@
    (set_attr "mode" "DI")
    (set_attr "insn_count" "2")])
 
-(define_insn "alsl_paired1"
-  [(set (match_operand:DI 0 "register_operand" "=&r")
-	(plus:DI (and:DI (ashift:DI (subreg:DI (match_operand:SI 1 "register_operand" "r") 0)
-				    (match_operand 2 "const_immalsl_operand" ""))
-			 (match_operand 3 "immediate_operand" ""))
-		 (match_operand:DI 4 "register_operand" "r")))]
-  "TARGET_64BIT
-   && ((INTVAL (operands[3]) >> INTVAL (operands[2])) == 0xffffffff)"
-  "bstrpick.d\t%0,%1,31,0\n\talsl.d\t%0,%0,%4,%2"
-  [(set_attr "type" "arith")
-  (set_attr "mode" "DI")
-  (set_attr "insn_count" "2")])
-
-(define_insn "alsl_paired2"
+(define_insn "bstrpick_alsl_paired"
   [(set (match_operand:DI 0 "register_operand" "=&r")
 	(plus:DI (match_operand:DI 1 "register_operand" "r")
 		 (and:DI (ashift:DI (match_operand:DI 2 "register_operand" "r")
@@ -2676,7 +2566,7 @@
 	  (match_operator 1 "equality_operator"
 	    [(match_operand:FCC 2 "register_operand" "z")
 	    (const_int 0)])
-	    (pc)
+	  (pc)
 	  (label_ref (match_operand 0 "" ""))))]
   "TARGET_HARD_FLOAT"
 {
@@ -2692,8 +2582,8 @@
   [(set (pc)
 	(if_then_else
 	 (match_operator 1 "order_operator"
-			 [(match_operand:GPR 2 "register_operand" "r,r")
-			  (match_operand:GPR 3 "reg_or_0_operand" "J,r")])
+			 [(match_operand:X 2 "register_operand" "r,r")
+			  (match_operand:X 3 "reg_or_0_operand" "J,r")])
 	 (label_ref (match_operand 0 "" ""))
 	 (pc)))]
   ""
@@ -2705,8 +2595,8 @@
   [(set (pc)
 	(if_then_else
 	 (match_operator 1 "order_operator"
-			 [(match_operand:GPR 2 "register_operand" "r,r")
-			  (match_operand:GPR 3 "reg_or_0_operand" "J,r")])
+			 [(match_operand:X 2 "register_operand" "r,r")
+			  (match_operand:X 3 "reg_or_0_operand" "J,r")])
 	 (pc)
 	 (label_ref (match_operand 0 "" ""))))]
   ""
@@ -2720,8 +2610,8 @@
   [(set (pc)
 	(if_then_else
 	 (match_operator 1 "equality_operator"
-			 [(match_operand:GPR 2 "register_operand" "r")
-			  (match_operand:GPR 3 "reg_or_0_operand" "rJ")])
+			 [(match_operand:X 2 "register_operand" "r")
+			  (match_operand:X 3 "reg_or_0_operand" "rJ")])
 	 (label_ref (match_operand 0 "" ""))
 	 (pc)))]
   ""
@@ -2734,8 +2624,8 @@
   [(set (pc)
 	(if_then_else
 	 (match_operator 1 "equality_operator"
-			 [(match_operand:GPR 2 "register_operand" "r")
-			  (match_operand:GPR 3 "reg_or_0_operand" "rJ")])
+			 [(match_operand:X 2 "register_operand" "r")
+			  (match_operand:X 3 "reg_or_0_operand" "rJ")])
 	 (pc)
 	 (label_ref (match_operand 0 "" ""))))]
   ""
@@ -2747,8 +2637,8 @@
 (define_expand "cbranch<mode>4"
   [(set (pc)
 	(if_then_else (match_operator 0 "comparison_operator"
-		      [(match_operand:GPR 1 "register_operand")
-			(match_operand:GPR 2 "nonmemory_operand")])
+			[(match_operand:GPR 1 "register_operand")
+			 (match_operand:GPR 2 "nonmemory_operand")])
 		      (label_ref (match_operand 3 ""))
 		      (pc)))]
   ""
@@ -2761,7 +2651,7 @@
   [(set (pc)
 	(if_then_else (match_operator 0 "comparison_operator"
 			[(match_operand:ANYF 1 "register_operand")
-			(match_operand:ANYF 2 "register_operand")])
+			 (match_operand:ANYF 2 "register_operand")])
 		      (label_ref (match_operand 3 ""))
 		      (pc)))]
   ""
@@ -2799,63 +2689,63 @@
   DONE;
 })
 
-(define_insn "*seq_zero_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(eq:GPR2 (match_operand:GPR 1 "register_operand" "r")
+(define_insn "*seq_zero_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(eq:GPR (match_operand:X 1 "register_operand" "r")
 		 (const_int 0)))]
   ""
   "sltui\t%0,%1,1"
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
 
-(define_insn "*sne_zero_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(ne:GPR2 (match_operand:GPR 1 "register_operand" "r")
+(define_insn "*sne_zero_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(ne:GPR (match_operand:X 1 "register_operand" "r")
 		 (const_int 0)))]
   ""
   "sltu\t%0,%.,%1"
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
-(define_insn "*sgt<u>_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(any_gt:GPR2 (match_operand:GPR 1 "register_operand" "r")
-		     (match_operand:GPR 2 "reg_or_0_operand" "rJ")))]
+(define_insn "*sgt<u>_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(any_gt:GPR (match_operand:X 1 "register_operand" "r")
+		     (match_operand:X 2 "reg_or_0_operand" "rJ")))]
   ""
   "slt<u>\t%0,%z2,%1"
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
-(define_insn "*sge<u>_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(any_ge:GPR2 (match_operand:GPR 1 "register_operand" "r")
+(define_insn "*sge<u>_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(any_ge:GPR (match_operand:X 1 "register_operand" "r")
 		     (const_int 1)))]
   ""
   "slt<u>i\t%0,%.,%1"
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
-(define_insn "*slt<u>_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(any_lt:GPR2 (match_operand:GPR 1 "register_operand" "r")
-		     (match_operand:GPR 2 "arith_operand" "rI")))]
+(define_insn "*slt<u>_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(any_lt:GPR (match_operand:X 1 "register_operand" "r")
+		     (match_operand:X 2 "arith_operand" "rI")))]
   ""
   "slt<u>%i2\t%0,%1,%2";
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
-(define_insn "*sle<u>_<GPR:mode><GPR2:mode>"
-  [(set (match_operand:GPR2 0 "register_operand" "=r")
-	(any_le:GPR2 (match_operand:GPR 1 "register_operand" "r")
-		     (match_operand:GPR 2 "sle_operand" "")))]
+(define_insn "*sle<u>_<X:mode><GPR:mode>"
+  [(set (match_operand:GPR 0 "register_operand" "=r")
+	(any_le:GPR (match_operand:X 1 "register_operand" "r")
+		     (match_operand:X 2 "sle_operand" "")))]
   ""
 {
   operands[2] = GEN_INT (INTVAL (operands[2]) + 1);
   return "slt<u>i\t%0,%1,%2";
 }
   [(set_attr "type" "slt")
-   (set_attr "mode" "<GPR:MODE>")])
+   (set_attr "mode" "<X:MODE>")])
 
 
 ;;
@@ -2868,8 +2758,8 @@
 (define_insn "s<code>_<ANYF:mode>_using_FCCmode"
   [(set (match_operand:FCC 0 "register_operand" "=z")
 	(fcond:FCC (match_operand:ANYF 1 "register_operand" "f")
-		    (match_operand:ANYF 2 "register_operand" "f")))]
-  "TARGET_HARD_FLOAT"
+		   (match_operand:ANYF 2 "register_operand" "f")))]
+  ""
   "fcmp.<fcond>.<fmt>\t%Z0%1,%2"
   [(set_attr "type" "fcmp")
    (set_attr "mode" "FCC")])
@@ -2915,12 +2805,10 @@
   DONE;
 })
 
-(define_insn "indirect_jump_<mode>"
+(define_insn "indirect_jump<mode>"
   [(set (pc) (match_operand:P 0 "register_operand" "r"))]
   ""
-  {
-    return "jr\t%0";
-  }
+  "jr\t%0"
   [(set_attr "type" "jump")
    (set_attr "mode" "none")])
 
@@ -2931,22 +2819,20 @@
   ""
 {
   if (flag_pic)
-      operands[0] = expand_simple_binop (Pmode, PLUS, operands[0],
-					 gen_rtx_LABEL_REF (Pmode,
-							    operands[1]),
-					 NULL_RTX, 0, OPTAB_DIRECT);
+    operands[0] = expand_simple_binop (Pmode, PLUS, operands[0],
+				       gen_rtx_LABEL_REF (Pmode,
+							  operands[1]),
+				       NULL_RTX, 0, OPTAB_DIRECT);
   emit_jump_insn (PMODE_INSN (gen_tablejump, (operands[0], operands[1])));
   DONE;
 })
 
-(define_insn "tablejump_<mode>"
+(define_insn "tablejump<mode>"
   [(set (pc)
 	(match_operand:P 0 "register_operand" "r"))
    (use (label_ref (match_operand 1 "" "")))]
   ""
-  {
-    return "jr\t%0";
-  }
+  "jr\t%0"
   [(set_attr "type" "jump")
    (set_attr "mode" "none")])
 
@@ -2979,7 +2865,7 @@
   [(set_attr "type" "ghost")
    (set_attr "mode" "none")])
 
-(define_insn "probe_stack_range_<P:mode>"
+(define_insn "probe_stack_range<P:mode>"
   [(set (match_operand:P 0 "register_operand" "=r")
 	(unspec_volatile:P [(match_operand:P 1 "register_operand" "0")
 			    (match_operand:P 2 "register_operand" "r")
@@ -3026,12 +2912,12 @@
 (define_insn "*<optab>"
   [(any_return)]
   ""
-  {
-    operands[0] = gen_rtx_REG (Pmode, RETURN_ADDR_REGNUM);
-    return "jr\t%0";
-  }
-  [(set_attr "type"	"jump")
-   (set_attr "mode"	"none")])
+{
+  operands[0] = gen_rtx_REG (Pmode, RETURN_ADDR_REGNUM);
+  return "jr\t%0";
+}
+  [(set_attr "type" "jump")
+   (set_attr "mode" "none")])
 
 ;; Normal return.
 
@@ -3039,11 +2925,9 @@
   [(any_return)
    (use (match_operand 0 "pmode_register_operand" ""))]
   ""
-  {
-    return "jr\t%0";
-  }
-  [(set_attr "type"	"jump")
-   (set_attr "mode"	"none")])
+  "jr\t%0"
+  [(set_attr "type" "jump")
+   (set_attr "mode" "none")])
 
 ;; Exception return.
 (define_insn "loongarch_ertn"
@@ -3051,8 +2935,8 @@
    (unspec_volatile [(const_int 0)] UNSPECV_ERTN)]
   ""
   "ertn"
-  [(set_attr "type"	"trap")
-   (set_attr "mode"	"none")])
+  [(set_attr "type" "trap")
+   (set_attr "mode" "none")])
 
 ;; This is used in compiling the unwind routines.
 (define_expand "eh_return"
@@ -3153,10 +3037,8 @@
 	return "pcaddu18i\t$r12,(%%plt(%0)+0x20000)>>18\n\t"
 	       "jirl\t$r0,$r12,%%plt(%0)+4-((%%plt(%0)+(4+0x20000))>>18<<18)";
       else
-	{
-	  sorry ("cmodel extreme and tiny static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
     }
@@ -3186,7 +3068,7 @@
     {
       /*  Handle return values created by loongarch_return_fpr_single.  */
       if (GET_CODE (operands[0]) == PARALLEL && XVECLEN (operands[0], 0) == 1)
-      operands[0] = XEXP (XVECEXP (operands[0], 0, 0), 0);
+	operands[0] = XEXP (XVECEXP (operands[0], 0, 0), 0);
 
       emit_call_insn (gen_sibcall_value_internal (operands[0], target,
 						  operands[2]));
@@ -3231,10 +3113,8 @@
 	return "pcaddu18i\t$r12,(%%plt(%1)+0x20000)>>18\n\t"
 	       "jirl\t$r0,$r12,%%plt(%1)+4-((%%plt(%1)+(4+0x20000))>>18<<18)";
       else
-	{
-	  sorry ("loongarch cmodel extreme and tiny-static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
   }
@@ -3281,10 +3161,8 @@
 	return "pcaddu18i\t$r12,(%%plt(%1)+0x20000)>>18\n\t"
 	       "jirl\t$r0,$r12,%%plt(%1)+4-((%%plt(%1)+(4+0x20000))>>18<<18)";
       else
-	{
-	  sorry ("loongarch cmodel extreme and tiny-static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
   }
@@ -3341,10 +3219,8 @@
       else if (TARGET_CMODEL_NORMAL || TARGET_CMODEL_TINY)
 	return "bl\t%%plt(%0)";
       else
-	{
-	  sorry ("cmodel extreme and tiny-static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
     }
@@ -3360,7 +3236,7 @@
   ""
 {
   rtx target = loongarch_legitimize_call_address (XEXP (operands[1], 0));
- /*  Handle return values created by loongarch_pass_fpr_pair.  */
+  /* Handle return values created by loongarch_pass_fpr_pair.  */
   if (GET_CODE (operands[0]) == PARALLEL && XVECLEN (operands[0], 0) == 2)
     {
       rtx arg1 = XEXP (XVECEXP (operands[0], 0, 0), 0);
@@ -3371,9 +3247,9 @@
     }
    else
     {
-      /*  Handle return values created by loongarch_return_fpr_single.  */
+      /* Handle return values created by loongarch_return_fpr_single.  */
       if (GET_CODE (operands[0]) == PARALLEL && XVECLEN (operands[0], 0) == 1)
-      operands[0] = XEXP (XVECEXP (operands[0], 0, 0), 0);
+	    operands[0] = XEXP (XVECEXP (operands[0], 0, 0), 0);
 
       emit_call_insn (gen_call_value_internal (operands[0], target,
 					       operands[2]));
@@ -3419,10 +3295,8 @@
       else if (TARGET_CMODEL_NORMAL || TARGET_CMODEL_TINY)
 	return "bl\t%%plt(%1)";
       else
-	{
-	  sorry ("loongarch cmodel extreme and tiny-static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
     }
@@ -3471,10 +3345,8 @@
       else if (TARGET_CMODEL_NORMAL || TARGET_CMODEL_TINY)
 	return "bl\t%%plt(%1)";
       else
-	{
-	  sorry ("loongarch cmodel extreme and tiny-static not support plt");
-	  return "";  /* GCC complains about may fall through.  */
-	}
+	/* Code model "extreme" and "tiny-static" do not support plt.  */
+	gcc_unreachable ();
     default:
       gcc_unreachable ();
     }
@@ -3517,14 +3389,14 @@
   [(const_int 0)]
   ""
   "nop"
-  [(set_attr "type"	"nop")
-   (set_attr "mode"	"none")])
+  [(set_attr "type" "nop")
+   (set_attr "mode" "none")])
 
 ;; __builtin_loongarch_movfcsr2gr: move the FCSR into operand 0.
 (define_insn "loongarch_movfcsr2gr"
   [(set (match_operand:SI 0 "register_operand" "=r")
-    (unspec_volatile:SI [(match_operand 1 "const_uimm5_operand")]
-    UNSPECV_MOVFCSR2GR))]
+	(unspec_volatile:SI [(match_operand 1 "const_uimm5_operand")]
+			     UNSPECV_MOVFCSR2GR))]
   "TARGET_HARD_FLOAT"
   "movfcsr2gr\t%0,$r%1")
 
@@ -3532,14 +3404,14 @@
 (define_insn "loongarch_movgr2fcsr"
   [(unspec_volatile [(match_operand 0 "const_uimm5_operand")
 		     (match_operand:SI 1 "register_operand" "r")]
-	  UNSPECV_MOVGR2FCSR)]
+		     UNSPECV_MOVGR2FCSR)]
   "TARGET_HARD_FLOAT"
   "movgr2fcsr\t$r%0,%1")
 
 (define_insn "fclass_<fmt>"
   [(set (match_operand:ANYF 0 "register_operand" "=f")
-       (unspec:ANYF [(match_operand:ANYF 1 "register_operand" "f")]
-		UNSPEC_FCLASS))]
+	(unspec:ANYF [(match_operand:ANYF 1 "register_operand" "f")]
+		      UNSPEC_FCLASS))]
   "TARGET_HARD_FLOAT"
   "fclass.<fmt>\t%0,%1"
   [(set_attr "type" "unknown")
@@ -3548,57 +3420,51 @@
 (define_insn "bytepick_w"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(unspec:SI [(match_operand:SI 1 "register_operand" "r")
-		   (match_operand:SI 2 "register_operand" "r")
-		   (match_operand:SI 3 "const_0_to_3_operand" "n")]
-	      UNSPEC_BYTEPICK_W))]
+		    (match_operand:SI 2 "register_operand" "r")
+		    (match_operand:SI 3 "const_0_to_3_operand" "n")]
+		    UNSPEC_BYTEPICK_W))]
   ""
   "bytepick.w\t%0,%1,%2,%z3"
-  [(set_attr "mode"    "SI")])
+  [(set_attr "mode" "SI")])
 
 (define_insn "bytepick_d"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	(unspec:DI [(match_operand:DI 1 "register_operand" "r")
-		   (match_operand:DI 2 "register_operand" "r")
-		   (match_operand:DI 3 "const_0_to_7_operand" "n")]
-	      UNSPEC_BYTEPICK_D))]
+		    (match_operand:DI 2 "register_operand" "r")
+		    (match_operand:DI 3 "const_0_to_7_operand" "n")]
+		    UNSPEC_BYTEPICK_D))]
   ""
   "bytepick.d\t%0,%1,%2,%z3"
-  [(set_attr "mode"    "DI")])
+  [(set_attr "mode" "DI")])
 
 (define_insn "bitrev_4b"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(unspec:SI [(match_operand:SI 1 "register_operand" "r")]
-	    UNSPEC_BITREV_4B))]
+		    UNSPEC_BITREV_4B))]
   ""
   "bitrev.4b\t%0,%1"
-  [(set_attr "type"    "unknown")
-   (set_attr "mode"    "SI")])
+  [(set_attr "type" "unknown")
+   (set_attr "mode" "SI")])
 
 (define_insn "bitrev_8b"
   [(set (match_operand:DI 0 "register_operand" "=r")
 	(unspec:DI [(match_operand:DI 1 "register_operand" "r")]
-	    UNSPEC_BITREV_8B))]
+		    UNSPEC_BITREV_8B))]
   ""
   "bitrev.8b\t%0,%1"
-  [(set_attr "type"    "unknown")
-   (set_attr "mode"    "DI")])
+  [(set_attr "type" "unknown")
+   (set_attr "mode" "DI")])
 
 (define_insn "stack_tie<mode>"
   [(set (mem:BLK (scratch))
-	(unspec:BLK [(match_operand:GPR 0 "register_operand" "r")
-		     (match_operand:GPR 1 "register_operand" "r")]
-		    UNSPEC_TIE))]
+	(unspec:BLK [(match_operand:X 0 "register_operand" "r")
+		     (match_operand:X 1 "register_operand" "r")]
+		     UNSPEC_TIE))]
   ""
   ""
-  [(set_attr "length" "0")]
-)
+  [(set_attr "length" "0")
+   (set_attr "type" "ghost")])
 
-(define_insn "gpr_restore_return"
-  [(return)
-   (use (match_operand 0 "pmode_register_operand" ""))
-   (const_int 0)]
-  ""
-  "")
 
 (define_split
   [(match_operand 0 "small_data_pattern")]
@@ -3609,33 +3475,24 @@
 
 ;; Match paired HI/SI/SF/DFmode load/stores.
 (define_insn "*join2_load_store<JOIN_MODE:mode>"
-  [(set (match_operand:JOIN_MODE 0 "nonimmediate_operand" "=r,f,m,m,r,ZC")
+  [(set (match_operand:JOIN_MODE 0 "nonimmediate_operand"
+  "=&r,f,m,m,&r,ZC")
 	(match_operand:JOIN_MODE 1 "nonimmediate_operand" "m,m,r,f,ZC,r"))
-   (set (match_operand:JOIN_MODE 2 "nonimmediate_operand" "=r,f,m,m,r,ZC")
+   (set (match_operand:JOIN_MODE 2 "nonimmediate_operand"
+   "=r,f,m,m,r,ZC")
 	(match_operand:JOIN_MODE 3 "nonimmediate_operand" "m,m,r,f,ZC,r"))]
   "reload_completed"
   {
-    bool load_p = (which_alternative == 0 || which_alternative == 1);
-    /* Reg-renaming pass reuses base register if it is dead after bonded loads.
-       Hardware does not bond those loads, even when they are consecutive.
-       However, order of the loads need to be checked for correctness.  */
-    if (!load_p || !reg_overlap_mentioned_p (operands[0], operands[1]))
-      {
-	output_asm_insn (loongarch_output_move (operands[0], operands[1]),
-			 operands);
-	output_asm_insn (loongarch_output_move (operands[2], operands[3]),
-			 &operands[2]);
-      }
-    else
-      {
-	output_asm_insn (loongarch_output_move (operands[2], operands[3]),
-			 &operands[2]);
-	output_asm_insn (loongarch_output_move (operands[0], operands[1]),
-			 operands);
-      }
+    /* The load destination does not overlap the source.  */
+    gcc_assert (!reg_overlap_mentioned_p (operands[0], operands[1]));
+    output_asm_insn (loongarch_output_move (operands[0], operands[1]),
+		     operands);
+    output_asm_insn (loongarch_output_move (operands[2], operands[3]),
+		     &operands[2]);
     return "";
   }
-  [(set_attr "move_type" "load,fpload,store,fpstore,load,store")
+  [(set_attr "move_type"
+  "load,fpload,store,fpstore,load,store")
    (set_attr "insn_count" "2,2,2,2,2,2")])
 
 ;; 2 HI/SI/SF/DF loads are bonded.
@@ -3666,25 +3523,16 @@
 
 ;; Match paired HImode loads.
 (define_insn "*join2_loadhi"
-  [(set (match_operand:SI 0 "register_operand" "=r")
+  [(set (match_operand:SI 0 "register_operand" "=&r")
 	(any_extend:SI (match_operand:HI 1 "non_volatile_mem_operand" "m")))
    (set (match_operand:SI 2 "register_operand" "=r")
 	(any_extend:SI (match_operand:HI 3 "non_volatile_mem_operand" "m")))]
   "reload_completed"
   {
-    /* Reg-renaming pass reuses base register if it is dead after bonded loads.
-       Hardware does not bond those loads, even when they are consecutive.
-       However, order of the loads need to be checked for correctness.  */
-    if (!reg_overlap_mentioned_p (operands[0], operands[1]))
-      {
-	output_asm_insn ("ld.h<u>\t%0,%1", operands);
-	output_asm_insn ("ld.h<u>\t%2,%3", operands);
-      }
-    else
-      {
-	output_asm_insn ("ld.h<u>\t%2,%3", operands);
-	output_asm_insn ("ld.h<u>\t%0,%1", operands);
-      }
+    /* The load destination does not overlap the source.  */
+    gcc_assert (!reg_overlap_mentioned_p (operands[0], operands[1]));
+    output_asm_insn ("ld.h<u>\t%0,%1", operands);
+    output_asm_insn ("ld.h<u>\t%2,%3", operands);
 
     return "";
   }
@@ -3709,7 +3557,7 @@
 
 (define_mode_iterator QHSD [QI HI SI DI])
 
-(define_insn "crc_w_<size>_w"
+(define_insn "loongarch_crc_w_<size>_w"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(unspec:SI [(match_operand:QHSD 1 "register_operand" "r")
 		   (match_operand:SI 2 "register_operand" "r")]
@@ -3719,7 +3567,7 @@
   [(set_attr "type" "unknown")
    (set_attr "mode" "<MODE>")])
 
-(define_insn "crcc_w_<size>_w"
+(define_insn "loongarch_crcc_w_<size>_w"
   [(set (match_operand:SI 0 "register_operand" "=r")
 	(unspec:SI [(match_operand:QHSD 1 "register_operand" "r")
 		   (match_operand:SI 2 "register_operand" "r")]
diff --git a/src/gcc/config/loongarch/loongarch.opt b/src/gcc/config/loongarch/loongarch.opt
index 4e0e44127..562acc8ad 100644
--- a/src/gcc/config/loongarch/loongarch.opt
+++ b/src/gcc/config/loongarch/loongarch.opt
@@ -8,7 +8,7 @@
 ; Generated by "genstr" from the template "loongarch.opt.in"
 ; and definitions from "loongarch-strings".
 ;
-; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;
 ; This file is part of GCC.
 ;
@@ -154,6 +154,10 @@ mvecarg
 Target Report Var(TARGET_VECARG) Init(1)
 Target pass vect arg uses vector register.
 
+mmemvec-cost=
+Target RejectNegative Joined UInteger Var(loongarch_vector_access_cost) IntegerRange(1, 5)
+mmemvec-cost=COST      Set the cost of vector memory access instructions.
+
 mcheck-zero-division
 Target Mask(CHECK_ZERO_DIV)
 Trap on integer divide by zero.
diff --git a/src/gcc/config/loongarch/lsx.md b/src/gcc/config/loongarch/lsx.md
index ef658a6a5..914ca270e 100644
--- a/src/gcc/config/loongarch/lsx.md
+++ b/src/gcc/config/loongarch/lsx.md
@@ -746,7 +746,7 @@
   [(set (match_operand:LSX 0 "nonimmediate_operand")
 	(match_operand:LSX 1 "move_operand"))]
   "reload_completed && ISA_HAS_LSX
-   && loongarch_split_move_insn_p (operands[0], operands[1], insn)"
+   && loongarch_split_move_insn_p (operands[0], operands[1])"
   [(const_int 0)]
 {
   loongarch_split_move_insn (operands[0], operands[1], curr_insn);
@@ -3192,3 +3192,101 @@
 }
   [(set_attr "type" "simd_fcmp")
    (set_attr "mode" "FCC")])
+
+;; Vector reduction operation
+(define_expand "reduc_plus_scal_v2di"
+  [(match_operand:DI 0 "register_operand")
+   (match_operand:V2DI 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (V2DImode);
+  emit_insn (gen_lsx_vhaddw_q_d (tmp, operands[1], operands[1]));
+  emit_insn (gen_vec_extractv2didi (operands[0], tmp, const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_plus_scal_v4si"
+  [(match_operand:SI 0 "register_operand")
+   (match_operand:V4SI 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (V2DImode);
+  rtx tmp1 = gen_reg_rtx (V2DImode);
+  emit_insn (gen_lsx_vhaddw_d_w (tmp, operands[1], operands[1]));
+  emit_insn (gen_lsx_vhaddw_q_d (tmp1, tmp, tmp));
+  emit_insn (gen_vec_extractv4sisi (operands[0], gen_lowpart(V4SImode,tmp1), const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_plus_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:FLSX 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_add<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_<optab>_scal_<mode>"
+  [(any_bitwise:<UNITMODE>
+     (match_operand:<UNITMODE> 0 "register_operand")
+     (match_operand:ILSX 1 "register_operand"))]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_<optab><mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_smax_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:LSX 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_smax<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_smin_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:LSX 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_smin<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_umax_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:ILSX 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_umax<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
+
+(define_expand "reduc_umin_scal_<mode>"
+  [(match_operand:<UNITMODE> 0 "register_operand")
+   (match_operand:ILSX 1 "register_operand")]
+  "ISA_HAS_LSX"
+{
+  rtx tmp = gen_reg_rtx (<MODE>mode);
+  loongarch_expand_vector_reduc (gen_umin<mode>3, tmp, operands[1]);
+  emit_insn (gen_vec_extract<mode><unitmode> (operands[0], tmp,
+        const0_rtx));
+  DONE;
+})
diff --git a/src/gcc/config/loongarch/predicates.md b/src/gcc/config/loongarch/predicates.md
index 8003a3ec8..46c781a58 100644
--- a/src/gcc/config/loongarch/predicates.md
+++ b/src/gcc/config/loongarch/predicates.md
@@ -1,5 +1,5 @@
 ;; Predicate definitions for LoongArch target.
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Co. Ltd.
 ;; Based on MIPS target for GNU compiler.
 ;;
@@ -71,9 +71,6 @@
   (and (match_code "const_int")
        (match_test "UIMM6_OPERAND (INTVAL (op))")))
 
-(define_predicate "const_uimm7_operand"
-  (and (match_code "const_int")
-       (match_test "IN_RANGE (INTVAL (op), 0, 127)")))
 
 (define_predicate "const_uimm8_operand"
   (and (match_code "const_int")
@@ -87,10 +84,6 @@
   (and (match_code "const_int")
        (match_test "IN_RANGE (INTVAL (op), 0, 32767)")))
 
-(define_predicate "const_imm5_operand"
-  (and (match_code "const_int")
-       (match_test "IN_RANGE (INTVAL (op), -16, 15)")))
-
 (define_predicate "const_imm10_operand"
   (and (match_code "const_int")
        (match_test "IMM10_OPERAND (INTVAL (op))")))
@@ -103,10 +96,6 @@
   (and (match_code "const_int")
        (match_test "IMM13_OPERAND (INTVAL (op))")))
 
-(define_predicate "reg_imm10_operand"
-  (ior (match_operand 0 "const_imm10_operand")
-       (match_operand 0 "register_operand")))
-
 (define_predicate "aq8b_operand"
   (and (match_code "const_int")
        (match_test "loongarch_signed_immediate_p (INTVAL (op), 8, 0)")))
@@ -139,6 +128,7 @@
   (and (match_code "const_int")
        (match_test "loongarch_signed_immediate_p (INTVAL (op), 10, 3)")))
 
+
 (define_predicate "aq12b_operand"
   (and (match_code "const_int")
        (match_test "loongarch_signed_immediate_p (INTVAL (op), 12, 0)")))
@@ -237,154 +227,20 @@
   (and (match_code "const_int")
        (match_test "IN_RANGE (INTVAL (op), 0, 7)")))
 
-(define_predicate "qi_mask_operand"
-  (and (match_code "const_int")
-       (match_test "UINTVAL (op) == 0xff")))
-
-(define_predicate "hi_mask_operand"
-  (and (match_code "const_int")
-       (match_test "UINTVAL (op) == 0xffff")))
-
 (define_predicate "lu52i_mask_operand"
   (and (match_code "const_int")
        (match_test "UINTVAL (op) == 0xfffffffffffff")))
 
-(define_predicate "si_mask_operand"
-  (and (match_code "const_int")
-       (match_test "UINTVAL (op) == 0xffffffff")))
-
-(define_predicate "and_load_operand"
-  (ior (match_operand 0 "qi_mask_operand")
-       (match_operand 0 "hi_mask_operand")
-       (match_operand 0 "si_mask_operand")))
-
 (define_predicate "low_bitmask_operand"
   (and (match_code "const_int")
        (match_test "low_bitmask_len (mode, INTVAL (op)) > 12")))
 
-(define_predicate "and_reg_operand"
-  (ior (match_operand 0 "register_operand")
-       (match_operand 0 "const_uns_arith_operand")
-       (match_operand 0 "low_bitmask_operand")
-       (match_operand 0 "si_mask_operand")))
-
-(define_predicate "and_operand"
-  (ior (match_operand 0 "and_load_operand")
-       (match_operand 0 "and_reg_operand")))
-
-(define_predicate "d_operand"
-  (and (match_code "reg")
-       (match_test "GP_REG_P (REGNO (op))")))
-
-(define_predicate "db4_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op) + 1, 4, 0)")))
-
-(define_predicate "db7_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op) + 1, 7, 0)")))
-
-(define_predicate "db8_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op) + 1, 8, 0)")))
-
-(define_predicate "ib3_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op) - 1, 3, 0)")))
-
-(define_predicate "sb4_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_signed_immediate_p (INTVAL (op), 4, 0)")))
-
-(define_predicate "sb5_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_signed_immediate_p (INTVAL (op), 5, 0)")))
-
-(define_predicate "sb8_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_signed_immediate_p (INTVAL (op), 8, 0)")))
-
-(define_predicate "sd8_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_signed_immediate_p (INTVAL (op), 8, 3)")))
-
-(define_predicate "ub4_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 4, 0)")))
-
-(define_predicate "ub8_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 8, 0)")))
-
-(define_predicate "uh4_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 4, 1)")))
-
-(define_predicate "uw4_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 4, 2)")))
-
-(define_predicate "uw5_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 5, 2)")))
-
-(define_predicate "uw6_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 6, 2)")))
-
-(define_predicate "uw8_operand"
-  (and (match_code "const_int")
-       (match_test "loongarch_unsigned_immediate_p (INTVAL (op), 8, 2)")))
-
-(define_predicate "addiur2_operand"
-  (and (match_code "const_int")
-	(ior (match_test "INTVAL (op) == -1")
-	     (match_test "INTVAL (op) == 1")
-	     (match_test "INTVAL (op) == 4")
-	     (match_test "INTVAL (op) == 8")
-	     (match_test "INTVAL (op) == 12")
-	     (match_test "INTVAL (op) == 16")
-	     (match_test "INTVAL (op) == 20")
-	     (match_test "INTVAL (op) == 24"))))
-
-(define_predicate "addiusp_operand"
-  (and (match_code "const_int")
-       (ior (match_test "(IN_RANGE (INTVAL (op), 2, 257))")
-	    (match_test "(IN_RANGE (INTVAL (op), -258, -3))"))))
-
-(define_predicate "andi16_operand"
-  (and (match_code "const_int")
-	(ior (match_test "IN_RANGE (INTVAL (op), 1, 4)")
-	     (match_test "IN_RANGE (INTVAL (op), 7, 8)")
-	     (match_test "IN_RANGE (INTVAL (op), 15, 16)")
-	     (match_test "IN_RANGE (INTVAL (op), 31, 32)")
-	     (match_test "IN_RANGE (INTVAL (op), 63, 64)")
-	     (match_test "INTVAL (op) == 255")
-	     (match_test "INTVAL (op) == 32768")
-	     (match_test "INTVAL (op) == 65535"))))
-
-(define_predicate "movep_src_register"
-  (and (match_code "reg")
-       (ior (match_test ("IN_RANGE (REGNO (op), 2, 3)"))
-	    (match_test ("IN_RANGE (REGNO (op), 16, 20)")))))
-
-(define_predicate "movep_src_operand"
-  (ior (match_operand 0 "const_0_operand")
-       (match_operand 0 "movep_src_register")))
-
-(define_predicate "fcc_reload_operand"
-  (and (match_code "reg,subreg")
-       (match_test "FCC_REG_P (true_regnum (op))")))
-
-(define_predicate "muldiv_target_operand"
-		(match_operand 0 "register_operand"))
-
 (define_predicate "const_call_insn_operand"
   (match_code "const,symbol_ref,label_ref")
 {
   enum loongarch_symbol_type symbol_type;
 
-  if (!loongarch_symbolic_constant_p (op, SYMBOL_CONTEXT_CALL, &symbol_type))
+  if (!loongarch_symbolic_constant_p (op, &symbol_type))
     return false;
 
   switch (symbol_type)
@@ -409,7 +265,7 @@
 (define_predicate "is_const_call_local_symbol"
   (and (match_operand 0 "const_call_insn_operand")
        (ior (match_test "loongarch_global_symbol_p (op) == 0")
-       (match_test "loongarch_symbol_binds_local_p (op) != 0"))
+	    (match_test "loongarch_symbol_binds_local_p (op) != 0"))
        (match_test "CONSTANT_P (op)")))
 
 (define_predicate "is_const_call_weak_symbol"
@@ -491,47 +347,22 @@
     case CONST:
     case SYMBOL_REF:
     case LABEL_REF:
-      return (loongarch_symbolic_constant_p (op, SYMBOL_CONTEXT_LEA,
-					     &symbol_type));
+      return (loongarch_symbolic_constant_p (op, &symbol_type));
     default:
       return true;
     }
 })
 
-(define_predicate "consttable_operand"
-  (match_test "CONSTANT_P (op)"))
-
 (define_predicate "symbolic_operand"
   (match_code "const,symbol_ref,label_ref")
 {
   enum loongarch_symbol_type type;
-  return loongarch_symbolic_constant_p (op, SYMBOL_CONTEXT_LEA, &type);
+  return loongarch_symbolic_constant_p (op, &type);
 })
 
-(define_predicate "got_disp_operand"
-  (match_code "const,symbol_ref,label_ref")
-{
-  enum loongarch_symbol_type type;
-  return (loongarch_symbolic_constant_p (op, SYMBOL_CONTEXT_LEA, &type)
-	  && type == SYMBOL_GOT_DISP);
-})
-
-(define_predicate "symbol_ref_operand"
-  (match_code "symbol_ref"))
-
-(define_predicate "stack_operand"
-  (and (match_code "mem")
-       (match_test "loongarch_stack_address_p (XEXP (op, 0), GET_MODE (op))")))
-
 (define_predicate "equality_operator"
   (match_code "eq,ne"))
 
-(define_predicate "extend_operator"
-  (match_code "zero_extend,sign_extend"))
-
-(define_predicate "trap_comparison_operator"
-  (match_code "eq,ne,lt,ltu,ge,geu"))
-
 (define_predicate "order_operator"
   (match_code "lt,ltu,le,leu,ge,geu,gt,gtu"))
 
@@ -544,10 +375,6 @@
   (and (match_code "set,parallel,unspec,unspec_volatile,prefetch")
        (match_test "loongarch_small_data_pattern_p (op)")))
 
-(define_predicate "mem_noofs_operand"
-  (and (match_code "mem")
-       (match_code "reg" "0")))
-
 ;; Return 1 if the operand is in non-volatile memory.
 (define_predicate "non_volatile_mem_operand"
   (and (match_operand 0 "memory_operand")
diff --git a/src/gcc/config/loongarch/sync.md b/src/gcc/config/loongarch/sync.md
index 11e902c80..e32093935 100644
--- a/src/gcc/config/loongarch/sync.md
+++ b/src/gcc/config/loongarch/sync.md
@@ -1,5 +1,5 @@
 ;; Machine description for LoongArch atomic operations.
-;; Copyright (C) 2021-2022 Free Software Foundation, Inc.
+;; Copyright (C) 2020-2022 Free Software Foundation, Inc.
 ;; Contributed by Loongson Co. Ltd.
 ;; Based on MIPS and RISC-V target for GNU compiler.
 
diff --git a/src/gcc/rtlanal.c b/src/gcc/rtlanal.c
index dabc5403f..c8dac7188 100644
--- a/src/gcc/rtlanal.c
+++ b/src/gcc/rtlanal.c
@@ -5655,7 +5655,12 @@ canonicalize_condition (rtx_insn *insn, rtx cond, int reverse,
 	    code = GET_CODE (x);
 	  if (reverse_code)
 	    {
-	      code = reversed_comparison_code (x, prev);
+		    if (GET_MODE_CLASS (GET_MODE(SET_DEST (set))) == MODE_CC
+			&& GET_MODE_CLASS (GET_MODE(XEXP(x, 0))) == MODE_FLOAT
+			&& GET_MODE_CLASS (GET_MODE(XEXP(x, 1))) == MODE_FLOAT)
+                      code = reverse_condition_maybe_unordered(code);
+		    else
+	                  code = reversed_comparison_code (x, prev);
 	      if (code == UNKNOWN)
 		return 0;
 	      reverse_code = 0;
diff --git a/src/gcc/testsuite/g++.target/loongarch/loongarch.exp b/src/gcc/testsuite/g++.target/loongarch/loongarch.exp
new file mode 100644
index 000000000..4fd1194a5
--- /dev/null
+++ b/src/gcc/testsuite/g++.target/loongarch/loongarch.exp
@@ -0,0 +1,34 @@
+# Copyright (C) 2019-2022 Free Software Foundation, Inc.
+
+# This program is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3 of the License, or
+# (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+# GCC testsuite that uses the `dg.exp' driver.
+
+# Exit immediately if this isn't a LoongArch target.
+if ![istarget loongarch*-*-*] then {
+  return
+}
+
+# Load support procs.
+load_lib g++-dg.exp
+
+# Initialize `dg'.
+dg-init
+
+# Main loop.
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*.C]] "" ""
+
+# All done.
+dg-finish
diff --git a/src/gcc/testsuite/g++.target/loongarch/pr106096.C b/src/gcc/testsuite/g++.target/loongarch/pr106096.C
new file mode 100644
index 000000000..5edbda220
--- /dev/null
+++ b/src/gcc/testsuite/g++.target/loongarch/pr106096.C
@@ -0,0 +1,75 @@
+/* PR target/106096
+   Reduced from gimple-range-path.cc.  It was miscompiled with -O2 and
+   caused ICE (segfault) building stage 2 libgcc.  */
+
+/* { dg-do run } */
+/* { dg-options "-O2" } */
+enum E
+{
+  TS_TYPED
+} a;
+int b, c;
+char d, e;
+
+__attribute__ ((cold, noipa, noinline)) void
+cold (int *, E, char *, int, char *)
+{
+  __builtin_trap ();
+}
+
+int *
+contains_struct_check (E x)
+{
+  if (a)
+    cold (&b, x, &d, c, &e);
+  return &b;
+}
+
+struct vrange
+{
+  virtual void set_varying (int *);
+};
+
+struct int_range : vrange
+{
+  int *m_ranges[510];
+};
+
+__attribute__ ((noipa, noinline)) void
+vrange::set_varying (int *)
+{
+}
+
+struct Value_Range
+{
+  Value_Range (int *);
+  int_range m_irange;
+};
+
+__attribute__ ((noipa, noinline)) Value_Range::Value_Range (int *) {}
+
+struct path_range_query
+{
+  void ssa_range_in_phi (vrange &);
+  bool m_resolve;
+};
+
+__attribute__ ((noipa, noinline)) void
+path_range_query::ssa_range_in_phi (vrange &r)
+{
+  if (m_resolve)
+    {
+      Value_Range (contains_struct_check (TS_TYPED));
+      return;
+    }
+  r.set_varying (contains_struct_check (TS_TYPED));
+}
+
+int
+main ()
+{
+  path_range_query prq{ 0 };
+  vrange vr;
+  prq.ssa_range_in_phi (vr);
+  return 0;
+}
diff --git a/src/gcc/testsuite/gcc.target/loongarch/cmov_ff.c b/src/gcc/testsuite/gcc.target/loongarch/cmov_ff.c
new file mode 100644
index 000000000..f8a910afd
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/cmov_ff.c
@@ -0,0 +1,17 @@
+/* Test asm const. */
+/* { dg-do compile } */
+/* { dg-options "-O2 -S" } */
+/* { dg-final { scan-assembler-times "main:.*fcmp.*fsel.*" 1 } } */
+#include <stdio.h>
+
+extern void foo_ff(float*, float*, float*, float*);
+
+int main(void)
+{
+	float a,b;
+	float c,d,out;
+	foo_ff(&a, &b, &c, &d);
+	out = a>b?c:d;
+	printf("%f\n", out);
+}
+
diff --git a/src/gcc/testsuite/gcc.target/loongarch/cmov_fi.c b/src/gcc/testsuite/gcc.target/loongarch/cmov_fi.c
new file mode 100644
index 000000000..634c40367
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/cmov_fi.c
@@ -0,0 +1,17 @@
+/* Test asm const. */
+/* { dg-do compile } */
+/* { dg-options "-O2 -S" } */
+/* { dg-final { scan-assembler-times "main:.*movgr2fr.*movgr2fr.*fsel.*movfr2gr.*" 1 } } */
+#include <stdio.h>
+
+extern void foo_fi(float*, float*, int*, int*);
+
+int main(void)
+{
+	float a,b;
+	int c,d,out;
+	foo_fi(&a, &b, &c, &d);
+	out = a>b?c:d;
+	printf("%f\n", out);
+}
+
diff --git a/src/gcc/testsuite/gcc.target/loongarch/cmov_if.c b/src/gcc/testsuite/gcc.target/loongarch/cmov_if.c
new file mode 100644
index 000000000..110d693ed
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/cmov_if.c
@@ -0,0 +1,17 @@
+/* Test asm const. */
+/* { dg-do compile } */
+/* { dg-options "-O2 -S" } */
+/* { dg-final { scan-assembler-times "main:.*movgr2fr.*movfr2cf.*fsel.*" 1 } } */
+#include <stdio.h>
+
+extern void foo_if(int*, int*, float*, float*);
+
+int main(void)
+{
+	int a,b;
+	float c,d,out;
+	foo_if(&a, &b, &c, &d);
+	out = a==b?c:d;
+	printf("%f\n", out);
+}
+
diff --git a/src/gcc/testsuite/gcc.target/loongarch/cmov_ii.c b/src/gcc/testsuite/gcc.target/loongarch/cmov_ii.c
new file mode 100644
index 000000000..66f656fd6
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/cmov_ii.c
@@ -0,0 +1,17 @@
+/* Test asm const. */
+/* { dg-do compile } */
+/* { dg-options "-O2 -S" } */
+/* { dg-final { scan-assembler-times "main:.*xor.*masknez.*maskeqz.*or.*" 1 } } */
+#include <stdio.h>
+
+extern void foo_ii(int*, int*, int*, int*);
+
+int main(void)
+{
+	int a,b;
+	int c,d,out;
+	foo_ii(&a, &b, &c, &d);
+	out = a==b?c:d;
+	printf("%d\n", out);
+}
+
diff --git a/src/gcc/testsuite/gcc.target/loongarch/larch-builtin.c b/src/gcc/testsuite/gcc.target/loongarch/larch-builtin.c
new file mode 100644
index 000000000..ca7ddb140
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/larch-builtin.c
@@ -0,0 +1,265 @@
+/* Test for LoongArch intrinsics. */
+
+/* { dg-do compile } */
+
+/* { dg-final { scan-assembler-times "test_rdtime_d:.*rdtime\\.d.*\\.size	test_rdtime_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_rdtimeh_w:.*rdtimeh\\.w.*\\.size	test_rdtimeh_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_rdtimel_w:.*rdtimel\\.w.*\\.size	test_rdtimel_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_movfcsr2gr:.*movfcsr2gr.*\\.size	test_movfcsr2gr" 1 } } */
+/* { dg-final { scan-assembler-times "test_movgr2fcsr:.*movgr2fcsr.*\\.size	test_movgr2fcsr" 1 } } */
+/* { dg-final { scan-assembler-times "test_cacop_d:.*cacop.*\\.size	test_cacop_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_cpucfg:.*cpucfg.*\\.size	test_cpucfg" 1 } } */
+/* { dg-final { scan-assembler-times "test_asrtle_d:.*asrtle\\.d.*\\.size	test_asrtle_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_asrtgt_d:.*asrtgt\\.d.*\\.size	test_asrtgt_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_lddir_d:.*lddir.*\\.size	test_lddir_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_ldpte_d:.*ldpte.*\\.size	test_ldpte_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_crc_w_b_w:.*crc\\.w\\.b\\.w.*\\.size	test_crc_w_b_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crc_w_h_w:.*crc\\.w\\.h\\.w.*\\.size	test_crc_w_h_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crc_w_w_w:.*crc\\.w\\.w\\.w.*\\.size	test_crc_w_w_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crc_w_d_w:.*crc\\.w\\.d\\.w.*\\.size	test_crc_w_d_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crcc_w_b_w:.*crcc\\.w\\.b\\.w.*\\.size	test_crcc_w_b_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crcc_w_h_w:.*crcc\\.w\\.h\\.w.*\\.size	test_crcc_w_h_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crcc_w_w_w:.*crcc\\.w\\.w\\.w.*\\.size	test_crcc_w_w_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_crcc_w_d_w:.*crcc\\.w\\.d\\.w.*\\.size	test_crcc_w_d_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrrd_w:.*csrrd.*\\.size	test_csrrd_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrwr_w:.*csrwr.*\\.size	test_csrwr_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrxchg_w:.*csrxchg.*\\.size	test_csrxchg_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrrd_d:.*csrrd.*\\.size	test_csrrd_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrwr_d:.*csrwr.*\\.size	test_csrwr_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_csrxchg_d:.*csrxchg.*\\.size	test_csrxchg_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrrd_b:.*iocsrrd\\.b.*\\.size	test_iocsrrd_b" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrrd_h:.*iocsrrd\\.h.*\\.size	test_iocsrrd_h" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrrd_w:.*iocsrrd\\.w.*\\.size	test_iocsrrd_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrrd_d:.*iocsrrd\\.d.*\\.size	test_iocsrrd_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrwr_b:.*iocsrwr\\.b.*\\.size	test_iocsrwr_b" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrwr_h:.*iocsrwr\\.h.*\\.size	test_iocsrwr_h" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrwr_w:.*iocsrwr\\.w.*\\.size	test_iocsrwr_w" 1 } } */
+/* { dg-final { scan-assembler-times "test_iocsrwr_d:.*iocsrwr\\.d.*\\.size	test_iocsrwr_d" 1 } } */
+/* { dg-final { scan-assembler-times "test_dbar:.*dbar.*\\.size	test_dbar" 1 } } */
+/* { dg-final { scan-assembler-times "test_ibar:.*ibar.*\\.size	test_ibar" 1 } } */
+/* { dg-final { scan-assembler-times "test_syscall:.*syscall.*\\.size	test_syscall" 1 } } */
+/* { dg-final { scan-assembler-times "test_break:.*break.*\\.size	test_break" 1 } } */
+
+#include<larchintrin.h>
+
+__drdtime_t
+test_rdtime_d ()
+{
+  return __rdtime_d ();
+}
+
+__rdtime_t
+test_rdtimeh_w ()
+{
+  return __rdtimeh_w ();
+}
+
+__rdtime_t
+test_rdtimel_w ()
+{
+  return __rdtimel_w ();
+}
+
+unsigned int
+test_movfcsr2gr ()
+{
+  return __movfcsr2gr (1);
+}
+
+void
+test_movgr2fcsr (unsigned int _1)
+{
+  __movgr2fcsr (1, _1);
+}
+
+void
+test_cacop_d (unsigned long int _1)
+{
+  __cacop_d (1, _1, 1);
+}
+
+unsigned int
+test_cpucfg (unsigned int _1)
+{
+  return __cpucfg (_1);
+}
+
+void
+test_asrtle_d (long int _1, long int _2)
+{
+  __asrtle_d (_1, _2);
+}
+
+void
+test_asrtgt_d (long int _1, long int _2)
+{
+  __asrtgt_d (_1, _2);
+}
+
+long int
+test_lddir_d (long int _1)
+{
+  return __lddir_d (_1, 1);
+}
+
+void
+test_ldpte_d (long int _1)
+{
+  __ldpte_d (_1, 1);
+}
+
+int
+test_crc_w_b_w (char _1, int _2)
+{
+  return __crc_w_b_w (_1, _2);
+}
+
+int
+test_crc_w_h_w (short _1, int _2)
+{
+  return __crc_w_h_w (_1, _2);
+}
+
+int
+test_crc_w_w_w (int _1, int _2)
+{
+  return __crc_w_w_w (_1, _2);
+}
+
+int
+test_crc_w_d_w (long int _1, int _2)
+{
+  return __crc_w_d_w (_1, _2);
+}
+
+int
+test_crcc_w_b_w (char _1, int _2)
+{
+  return __crcc_w_b_w (_1, _2);
+}
+
+int
+test_crcc_w_h_w (short _1, int _2)
+{
+  return __crcc_w_h_w (_1, _2);
+}
+
+int
+test_crcc_w_w_w (int _1, int _2)
+{
+  return __crcc_w_w_w (_1, _2);
+}
+
+int
+test_crcc_w_d_w (long int _1, int _2)
+{
+  return __crcc_w_d_w (_1, _2);
+}
+
+unsigned int
+test_csrrd_w ()
+{
+  return __csrrd_w (1);
+}
+
+unsigned int
+test_csrwr_w (unsigned int _1)
+{
+  return __csrwr_w (_1, 1);
+}
+
+unsigned int
+test_csrxchg_w (unsigned int _1, unsigned int _2)
+{
+  return __csrxchg_w (_1, _2, 1);
+}
+
+unsigned long int
+test_csrrd_d ()
+{
+  return __csrrd_d (1);
+}
+
+unsigned long int
+test_csrwr_d (unsigned long int _1)
+{
+  return __csrwr_d (_1, 1);
+}
+
+unsigned long int
+test_csrxchg_d (unsigned long int _1, unsigned long int _2)
+{
+  return __csrxchg_d (_1, _2, 1);
+}
+
+unsigned char
+test_iocsrrd_b (unsigned int _1)
+{
+  return __iocsrrd_b (_1);
+}
+
+unsigned char
+test_iocsrrd_h (unsigned int _1)
+{
+  return __iocsrrd_h (_1);
+}
+
+unsigned int
+test_iocsrrd_w (unsigned int _1)
+{
+  return __iocsrrd_w (_1);
+}
+
+unsigned long int
+test_iocsrrd_d (unsigned int _1)
+{
+  return __iocsrrd_d (_1);
+}
+
+void
+test_iocsrwr_b (unsigned char _1, unsigned int _2)
+{
+  __iocsrwr_b (_1, _2);
+}
+
+void
+test_iocsrwr_h (unsigned short _1, unsigned int _2)
+{
+  __iocsrwr_h (_1, _2);
+}
+
+void
+test_iocsrwr_w (unsigned int _1, unsigned int _2)
+{
+  __iocsrwr_w (_1, _2);
+}
+
+void
+test_iocsrwr_d (unsigned long int _1, unsigned int _2)
+{
+  __iocsrwr_d (_1, _2);
+}
+
+void
+test_dbar ()
+{
+  __dbar (1);
+}
+
+void
+test_ibar ()
+{
+  __ibar (1);
+}
+
+void
+test_syscall ()
+{
+  __syscall (1);
+}
+
+void
+test_break ()
+{
+  __break (1);
+}
diff --git a/src/gcc/testsuite/gcc.target/loongarch/loongarch.exp b/src/gcc/testsuite/gcc.target/loongarch/loongarch.exp
index be9543d38..bebc00047 100644
--- a/src/gcc/testsuite/gcc.target/loongarch/loongarch.exp
+++ b/src/gcc/testsuite/gcc.target/loongarch/loongarch.exp
@@ -1,4 +1,4 @@
-# Copyright (C) 2017-2018 Free Software Foundation, Inc.
+# Copyright (C) 2020-2022 Free Software Foundation, Inc.
 
 # This program is free software; you can redistribute it and/or modify
 # it under the terms of the GNU General Public License as published by
diff --git a/src/gcc/testsuite/gcc.target/loongarch/prolog-opt.c b/src/gcc/testsuite/gcc.target/loongarch/prolog-opt.c
new file mode 100644
index 000000000..676ce80bb
--- /dev/null
+++ b/src/gcc/testsuite/gcc.target/loongarch/prolog-opt.c
@@ -0,0 +1,14 @@
+/* Test that LoongArch backend stack drop operation optimized.  */
+
+/* { dg-do compile } */
+/* { dg-options "-O2 -mabi=lp64d" } */
+/* { dg-final { scan-assembler "addi.d\t\\\$r3,\\\$r3,-16" } } */
+
+extern int printf (char *, ...);
+
+int main()
+{
+  char buf[1024 * 12];
+  printf ("%p\n", buf);
+  return 0;
+}
diff --git a/src/gcc/testsuite/lib/target-supports.exp b/src/gcc/testsuite/lib/target-supports.exp
index 5686bca2e..3a8f8beaa 100644
--- a/src/gcc/testsuite/lib/target-supports.exp
+++ b/src/gcc/testsuite/lib/target-supports.exp
@@ -276,13 +276,15 @@ proc check_configured_with { pattern } {
 proc check_weak_available { } {
     global target_cpu
 
-    # All mips targets should support it
+    # All loongarch targets should support it
 
-    if { [ string first "mips" $target_cpu ] >= 0 } {
+    if { [ string first "loongarch" $target_cpu ] >= 0 } {
         return 1
     }
 
-    if { [ string first "loongarch" $target_cpu ] >= 0 } {
+    # All mips targets should support it
+
+    if { [ string first "mips" $target_cpu ] >= 0 } {
         return 1
     }
 
diff --git a/src/libgcc/config/loongarch/crtfastmath.c b/src/libgcc/config/loongarch/crtfastmath.c
index d73347488..5f7b298ac 100644
--- a/src/libgcc/config/loongarch/crtfastmath.c
+++ b/src/libgcc/config/loongarch/crtfastmath.c
@@ -1,4 +1,4 @@
-/* Copyright (C) 2010-2018 Free Software Foundation, Inc.
+/* Copyright (C) 2020-2022 Free Software Foundation, Inc.
    Contributed by Loongson Ltd.
    Based on MIPS target for GNU compiler.
 
diff --git a/src/libgcc/config/loongarch/linux-unwind.h b/src/libgcc/config/loongarch/linux-unwind.h
index c20a972c9..30603e44f 100644
--- a/src/libgcc/config/loongarch/linux-unwind.h
+++ b/src/libgcc/config/loongarch/linux-unwind.h
@@ -1,5 +1,5 @@
 /* DWARF2 EH unwinding support for LoongArch Linux.
-   Copyright (C) 2004-2018 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
 
 This file is part of GCC.
 
diff --git a/src/libgcc/config/loongarch/sfp-machine.h b/src/libgcc/config/loongarch/sfp-machine.h
index e13e16043..420f94274 100644
--- a/src/libgcc/config/loongarch/sfp-machine.h
+++ b/src/libgcc/config/loongarch/sfp-machine.h
@@ -1,5 +1,5 @@
 /* softfp machine description for LoongArch.
-   Copyright (C) 2009-2018 Free Software Foundation, Inc.
+   Copyright (C) 2020-2022 Free Software Foundation, Inc.
 
 This file is part of GCC.
 
